# Comparing `tmp/pesummary-0.9.1.tar.gz` & `tmp/pesummary-1.0.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/pesummary-0.9.1.tar", last modified: Fri Sep  4 10:04:10 2020, max compression
+gzip compressed data, was "pesummary-1.0.0.tar", last modified: Sun Apr 23 23:46:58 2023, max compression
```

## Comparing `pesummary-0.9.1.tar` & `pesummary-1.0.0.tar`

### file list

```diff
@@ -1,203 +1,447 @@
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:10.000000 pesummary-0.9.1/
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:07.000000 pesummary-0.9.1/pesummary.egg-info/
--rw-r--r--   0 Charlie    (503) staff       (20)     2237 2020-09-04 10:04:06.000000 pesummary-0.9.1/pesummary.egg-info/PKG-INFO
--rw-r--r--   0 Charlie    (503) staff       (20)     5487 2020-09-04 10:04:06.000000 pesummary-0.9.1/pesummary.egg-info/SOURCES.txt
--rw-r--r--   0 Charlie    (503) staff       (20)      886 2020-09-04 10:04:06.000000 pesummary-0.9.1/pesummary.egg-info/entry_points.txt
--rw-r--r--   0 Charlie    (503) staff       (20)      172 2020-09-04 10:04:06.000000 pesummary-0.9.1/pesummary.egg-info/requires.txt
--rw-r--r--   0 Charlie    (503) staff       (20)       10 2020-09-04 10:04:06.000000 pesummary-0.9.1/pesummary.egg-info/top_level.txt
--rw-r--r--   0 Charlie    (503) staff       (20)        1 2020-09-04 10:04:06.000000 pesummary-0.9.1/pesummary.egg-info/dependency_links.txt
--rw-r--r--   0 Charlie    (503) staff       (20)     2237 2020-09-04 10:04:10.000000 pesummary-0.9.1/PKG-INFO
--rw-r--r--   0 Charlie    (503) staff       (20)     1091 2020-04-21 10:57:35.000000 pesummary-0.9.1/LICENSE.md
--rw-r--r--   0 Charlie    (503) staff       (20)      280 2020-04-21 10:57:35.000000 pesummary-0.9.1/pyproject.toml
--rw-r--r--   0 Charlie    (503) staff       (20)      326 2020-07-09 16:07:41.000000 pesummary-0.9.1/MANIFEST.in
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:10.000000 pesummary-0.9.1/pesummary/
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/core/
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/core/file/
--rw-r--r--   0 Charlie    (503) staff       (20)     1617 2020-05-28 08:30:23.000000 pesummary-0.9.1/pesummary/core/file/injection.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/core/file/formats/
--rw-r--r--   0 Charlie    (503) staff       (20)    10777 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/core/file/formats/bilby.py
--rw-r--r--   0 Charlie    (503) staff       (20)     2389 2020-05-28 08:17:27.000000 pesummary-0.9.1/pesummary/core/file/formats/dat.py
--rw-r--r--   0 Charlie    (503) staff       (20)      763 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/file/formats/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     6369 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/file/formats/hdf5.py
--rw-r--r--   0 Charlie    (503) staff       (20)     5321 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/core/file/formats/default.py
--rw-r--r--   0 Charlie    (503) staff       (20)    23435 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/file/formats/base_read.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4841 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/file/formats/json.py
--rw-r--r--   0 Charlie    (503) staff       (20)    31362 2020-07-24 22:42:22.000000 pesummary-0.9.1/pesummary/core/file/formats/pesummary.py
--rw-r--r--   0 Charlie    (503) staff       (20)      763 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/file/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3785 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/file/mcmc.py
--rw-r--r--   0 Charlie    (503) staff       (20)    23268 2020-07-24 22:42:22.000000 pesummary-0.9.1/pesummary/core/file/meta_file.py
--rw-r--r--   0 Charlie    (503) staff       (20)     8872 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/file/read.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/core/css/
--rw-r--r--   0 Charlie    (503) staff       (20)     1137 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/css/navbar.css
--rw-r--r--   0 Charlie    (503) staff       (20)     1601 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/css/image_styles.css
--rw-r--r--   0 Charlie    (503) staff       (20)      612 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/css/table.css
--rw-r--r--   0 Charlie    (503) staff       (20)      178 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/css/font.css
--rw-r--r--   0 Charlie    (503) staff       (20)     1806 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/css/side_bar.css
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/core/js/
--rw-r--r--   0 Charlie    (503) staff       (20)     1318 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/js/html_to_json.js
--rw-r--r--   0 Charlie    (503) staff       (20)     1313 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/js/side_bar.js
--rw-r--r--   0 Charlie    (503) staff       (20)     2361 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/js/multi_dropbar.js
--rw-r--r--   0 Charlie    (503) staff       (20)     4139 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/js/grab.js
--rw-r--r--   0 Charlie    (503) staff       (20)     7471 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/js/multiple_posteriors.js
--rw-r--r--   0 Charlie    (503) staff       (20)     2272 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/js/html_to_csv.js
--rw-r--r--   0 Charlie    (503) staff       (20)      705 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/js/html_to_shell.js
--rw-r--r--   0 Charlie    (503) staff       (20)     6956 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/js/combine_corner.js
--rw-r--r--   0 Charlie    (503) staff       (20)     2378 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/js/search.js
--rw-r--r--   0 Charlie    (503) staff       (20)     1427 2020-05-08 09:31:08.000000 pesummary-0.9.1/pesummary/core/js/modal.js
--rw-r--r--   0 Charlie    (503) staff       (20)    19303 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/command_line.py
--rw-r--r--   0 Charlie    (503) staff       (20)      763 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3570 2020-07-09 20:17:02.000000 pesummary-0.9.1/pesummary/core/parser.py
--rw-r--r--   0 Charlie    (503) staff       (20)    67541 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/inputs.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/core/notebook/
--rw-r--r--   0 Charlie    (503) staff       (20)      788 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/notebook/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     7084 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/notebook/notebook.py
--rw-r--r--   0 Charlie    (503) staff       (20)     2648 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/finish.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/core/plots/
--rw-r--r--   0 Charlie    (503) staff       (20)    23746 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/plots/plot.py
--rw-r--r--   0 Charlie    (503) staff       (20)     9110 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/plots/bounded_1d_kde.py
--rw-r--r--   0 Charlie    (503) staff       (20)      860 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/plots/latex_labels.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3243 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/plots/population.py
--rw-r--r--   0 Charlie    (503) staff       (20)    12339 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/plots/kde.py
--rw-r--r--   0 Charlie    (503) staff       (20)     2747 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/plots/figure.py
--rw-r--r--   0 Charlie    (503) staff       (20)      763 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/plots/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     8340 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/plots/interactive.py
--rw-r--r--   0 Charlie    (503) staff       (20)    16660 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/plots/publication.py
--rw-r--r--   0 Charlie    (503) staff       (20)    38248 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/plots/main.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:09.000000 pesummary-0.9.1/pesummary/core/webpage/
--rw-r--r--   0 Charlie    (503) staff       (20)    47001 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/webpage/webpage.py
--rw-r--r--   0 Charlie    (503) staff       (20)     7887 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/core/webpage/tables.py
--rw-r--r--   0 Charlie    (503) staff       (20)    50335 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/core/webpage/main.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3528 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/core/webpage/base.py
--rw-r--r--   0 Charlie    (503) staff       (20)      497 2020-09-04 10:04:10.000000 pesummary-0.9.1/pesummary/_version.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:09.000000 pesummary-0.9.1/pesummary/io/
--rw-r--r--   0 Charlie    (503) staff       (20)     1974 2020-05-27 15:59:15.000000 pesummary-0.9.1/pesummary/io/write.py
--rw-r--r--   0 Charlie    (503) staff       (20)      812 2020-05-27 15:59:15.000000 pesummary-0.9.1/pesummary/io/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1657 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/io/read.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:10.000000 pesummary-0.9.1/pesummary/tests/
--rw-r--r--   0 Charlie    (503) staff       (20)    20046 2020-07-24 22:42:22.000000 pesummary-0.9.1/pesummary/tests/meta_file_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)       69 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/main_input.ini
--rw-r--r--   0 Charlie    (503) staff       (20)     3379 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/psd_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)    36442 2020-09-03 21:20:15.000000 pesummary-0.9.1/pesummary/tests/executable_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)    12035 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/summaryplots_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)    46362 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/read_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1879 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/lalinference.sh
--rw-r--r--   0 Charlie    (503) staff       (20)    26954 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/tests/utils_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1076 2020-07-24 22:42:22.000000 pesummary-0.9.1/pesummary/tests/executables.sh
--rw-r--r--   0 Charlie    (503) staff       (20)     6393 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/write_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3248 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/calibration_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3027 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/injection_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)    12916 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/main_injection.xml
--rw-r--r--   0 Charlie    (503) staff       (20)    21242 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/input_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)      763 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)       94 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/example_config.ini
--rw-r--r--   0 Charlie    (503) staff       (20)     1170 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/bilby.sh
--rw-r--r--   0 Charlie    (503) staff       (20)     1516 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/existing_file.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1823 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/finish_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)    29575 2020-09-03 21:20:15.000000 pesummary-0.9.1/pesummary/tests/conversion_test.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:10.000000 pesummary-0.9.1/pesummary/tests/files/
--rw-r--r--   0 Charlie    (503) staff       (20)       19 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/files/example.ini
--rw-r--r--   0 Charlie    (503) staff       (20)     3576 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/files/config_lalinference.ini
--rw-r--r--   0 Charlie    (503) staff       (20)      511 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/files/config_bilby.ini
--rw-r--r--   0 Charlie    (503) staff       (20)    96127 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/files/psd_file.txt
--rw-r--r--   0 Charlie    (503) staff       (20)   843570 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/files/calibration_envelope.txt
--rw-r--r--   0 Charlie    (503) staff       (20)     2321 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/tests/bounded_kde_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)     9092 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/webpage_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)       33 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/main_input_core.ini
--rw-r--r--   0 Charlie    (503) staff       (20)     3094 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/ligo_skymap_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1913 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/cosmology_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1859 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/tests/notebook_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1152 2020-07-24 22:42:22.000000 pesummary-0.9.1/pesummary/tests/imports.sh
--rw-r--r--   0 Charlie    (503) staff       (20)    20778 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/tests/plot_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)    15163 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/base.py
--rw-r--r--   0 Charlie    (503) staff       (20)    18447 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/tests/workflow_test.py
--rw-r--r--   0 Charlie    (503) staff       (20)      949 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/__init__.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:10.000000 pesummary-0.9.1/pesummary/utils/
--rw-r--r--   0 Charlie    (503) staff       (20)    52433 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/utils/samples_dict.py
--rw-r--r--   0 Charlie    (503) staff       (20)      862 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/utils/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4423 2020-05-27 15:59:15.000000 pesummary-0.9.1/pesummary/utils/tqdm.py
--rw-r--r--   0 Charlie    (503) staff       (20)    30778 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/utils/utils.py
--rw-r--r--   0 Charlie    (503) staff       (20)      396 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/utils/exceptions.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1938 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/utils/dict.py
--rw-r--r--   0 Charlie    (503) staff       (20)     7255 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/utils/decorators.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/cli/
--rw-r--r--   0 Charlie    (503) staff       (20)     9132 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/cli/summarytest.py
--rw-r--r--   0 Charlie    (503) staff       (20)    11114 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/cli/summaryrecreate.py
--rw-r--r--   0 Charlie    (503) staff       (20)    19121 2020-07-09 20:17:02.000000 pesummary-0.9.1/pesummary/cli/summarymodify.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1033 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/cli/summaryversion.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1426 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/cli/summarycombine.py
--rw-r--r--   0 Charlie    (503) staff       (20)     7753 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/cli/summaryclassification.py
--rw-r--r--   0 Charlie    (503) staff       (20)        0 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/cli/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     8382 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/cli/summarypageslw.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3418 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/cli/summaryclean.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4782 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/cli/summarydetchar.py
--rw-r--r--   0 Charlie    (503) staff       (20)     9291 2020-07-24 22:42:22.000000 pesummary-0.9.1/pesummary/cli/summaryjscompare.py
--rw-r--r--   0 Charlie    (503) staff       (20)    13229 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/cli/summarypipe.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1818 2020-05-28 08:17:27.000000 pesummary-0.9.1/pesummary/cli/summarygracedb.py
--rw-r--r--   0 Charlie    (503) staff       (20)     9551 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/cli/summarypages.py
--rw-r--r--   0 Charlie    (503) staff       (20)    14253 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/cli/summaryplots.py
--rw-r--r--   0 Charlie    (503) staff       (20)    14610 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/cli/summarypublication.py
--rw-r--r--   0 Charlie    (503) staff       (20)     6529 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/cli/summarycompare.py
--rw-r--r--   0 Charlie    (503) staff       (20)    11751 2020-04-23 16:33:50.000000 pesummary-0.9.1/pesummary/cli/summaryreview.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:09.000000 pesummary-0.9.1/pesummary/gw/
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:09.000000 pesummary-0.9.1/pesummary/gw/file/
--rw-r--r--   0 Charlie    (503) staff       (20)     3818 2020-05-28 08:17:27.000000 pesummary-0.9.1/pesummary/gw/file/injection.py
--rw-r--r--   0 Charlie    (503) staff       (20)     7914 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/file/evolve.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:09.000000 pesummary-0.9.1/pesummary/gw/file/formats/
--rw-r--r--   0 Charlie    (503) staff       (20)     9717 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/gw/file/formats/bilby.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4027 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/file/formats/GWTC1.py
--rw-r--r--   0 Charlie    (503) staff       (20)      763 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/gw/file/formats/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     6428 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/gw/file/formats/default.py
--rw-r--r--   0 Charlie    (503) staff       (20)    23443 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/gw/file/formats/base_read.py
--rw-r--r--   0 Charlie    (503) staff       (20)    20427 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/file/formats/lalinference.py
--rw-r--r--   0 Charlie    (503) staff       (20)    14459 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/file/formats/pesummary.py
--rw-r--r--   0 Charlie    (503) staff       (20)   107544 2020-09-04 10:02:45.000000 pesummary-0.9.1/pesummary/gw/file/conversions.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1404 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/file/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     5652 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/file/skymap.py
--rw-r--r--   0 Charlie    (503) staff       (20)    31005 2020-05-27 15:59:15.000000 pesummary-0.9.1/pesummary/gw/file/nrutils.py
--rw-r--r--   0 Charlie    (503) staff       (20)     8751 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/file/meta_file.py
--rw-r--r--   0 Charlie    (503) staff       (20)     6596 2020-07-24 22:42:22.000000 pesummary-0.9.1/pesummary/gw/file/psd.py
--rw-r--r--   0 Charlie    (503) staff       (20)    18997 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/gw/file/standard_names.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3170 2020-05-28 08:17:27.000000 pesummary-0.9.1/pesummary/gw/file/calibration.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3620 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/file/read.py
--rw-r--r--   0 Charlie    (503) staff       (20)    10361 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/command_line.py
--rw-r--r--   0 Charlie    (503) staff       (20)     6925 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/pepredicates.py
--rw-r--r--   0 Charlie    (503) staff       (20)      763 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/gw/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1533 2020-05-28 08:17:27.000000 pesummary-0.9.1/pesummary/gw/parser.py
--rw-r--r--   0 Charlie    (503) staff       (20)    44123 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/gw/inputs.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4892 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/p_astro.py
--rw-r--r--   0 Charlie    (503) staff       (20)     2700 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/gracedb.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:09.000000 pesummary-0.9.1/pesummary/gw/notebook/
--rw-r--r--   0 Charlie    (503) staff       (20)     7755 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/gw/notebook/public.py
--rw-r--r--   0 Charlie    (503) staff       (20)      805 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/gw/notebook/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1842 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/gw/notebook/notebook.py
--rw-r--r--   0 Charlie    (503) staff       (20)     2729 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/cosmology.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4985 2020-07-09 20:17:02.000000 pesummary-0.9.1/pesummary/gw/finish.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:09.000000 pesummary-0.9.1/pesummary/gw/plots/
--rw-r--r--   0 Charlie    (503) staff       (20)     3678 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/plots/public.py
--rw-r--r--   0 Charlie    (503) staff       (20)    51829 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/gw/plots/plot.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1960 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/gw/plots/cmap.py
--rw-r--r--   0 Charlie    (503) staff       (20)     5108 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/gw/plots/latex_labels.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3209 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/gw/plots/bounded_2d_kde.py
--rw-r--r--   0 Charlie    (503) staff       (20)      815 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/gw/plots/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4470 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/plots/detchar.py
--rw-r--r--   0 Charlie    (503) staff       (20)     8714 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/gw/plots/cylon.csv
--rw-r--r--   0 Charlie    (503) staff       (20)    17399 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/gw/plots/publication.py
--rw-r--r--   0 Charlie    (503) staff       (20)    33090 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/gw/plots/violin.py
--rw-r--r--   0 Charlie    (503) staff       (20)    41714 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/gw/plots/main.py
--rw-r--r--   0 Charlie    (503) staff       (20)     3318 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/gw/plots/bounds.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:09.000000 pesummary-0.9.1/pesummary/gw/webpage/
--rw-r--r--   0 Charlie    (503) staff       (20)     7799 2020-07-09 16:07:41.000000 pesummary-0.9.1/pesummary/gw/webpage/public.py
--rw-r--r--   0 Charlie    (503) staff       (20)        0 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/gw/webpage/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)    36187 2020-08-13 16:12:34.000000 pesummary-0.9.1/pesummary/gw/webpage/main.py
--rw-r--r--   0 Charlie    (503) staff       (20)      278 2020-09-04 10:04:06.000000 pesummary-0.9.1/pesummary/.version
--rw-r--r--   0 Charlie    (503) staff       (20)     7042 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/_version_helper.py
-drwxr-xr-x   0 Charlie    (503) staff       (20)        0 2020-09-04 10:04:08.000000 pesummary-0.9.1/pesummary/conf/
--rw-r--r--   0 Charlie    (503) staff       (20)     3003 2020-07-24 22:42:22.000000 pesummary-0.9.1/pesummary/conf/configuration.py
--rw-r--r--   0 Charlie    (503) staff       (20)      353 2020-09-03 18:30:01.000000 pesummary-0.9.1/pesummary/conf/matplotlib_rcparams.sty
--rw-r--r--   0 Charlie    (503) staff       (20)       52 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/conf/__init__.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4676 2020-04-21 10:57:35.000000 pesummary-0.9.1/pesummary/conf/caption.py
--rw-r--r--   0 Charlie    (503) staff       (20)     1477 2020-07-24 22:42:22.000000 pesummary-0.9.1/README.md
--rw-r--r--   0 Charlie    (503) staff       (20)     5455 2020-09-03 18:30:01.000000 pesummary-0.9.1/setup.py
--rw-r--r--   0 Charlie    (503) staff       (20)     4863 2020-07-09 16:07:41.000000 pesummary-0.9.1/CONTRIBUTING.md
--rw-r--r--   0 Charlie    (503) staff       (20)      440 2020-09-04 10:04:10.000000 pesummary-0.9.1/setup.cfg
--rw-r--r--   0 Charlie    (503) staff       (20)    68611 2020-05-27 15:59:15.000000 pesummary-0.9.1/versioneer.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.956660 pesummary-1.0.0/
+-rw-r--r--   0 charlie    (501) staff       (20)      318 2022-09-20 20:30:50.000000 pesummary-1.0.0/.AUTHORS
+-rw-r--r--   0 charlie    (501) staff       (20)      239 2023-04-23 23:35:41.000000 pesummary-1.0.0/.flake8
+-rw-r--r--   0 charlie    (501) staff       (20)      119 2022-09-20 20:30:50.000000 pesummary-1.0.0/.gitattributes
+-rw-r--r--   0 charlie    (501) staff       (20)      646 2022-09-20 20:30:50.000000 pesummary-1.0.0/.gitignore
+-rw-r--r--   0 charlie    (501) staff       (20)    12639 2023-04-23 23:35:45.000000 pesummary-1.0.0/.gitlab-ci.yml
+-rw-r--r--   0 charlie    (501) staff       (20)      123 2022-09-20 20:30:50.000000 pesummary-1.0.0/.pre-commit-config.yaml
+-rw-r--r--   0 charlie    (501) staff       (20)    41646 2023-04-23 23:35:41.000000 pesummary-1.0.0/CHANGELOG.md
+-rw-r--r--   0 charlie    (501) staff       (20)     4818 2022-09-20 20:30:50.000000 pesummary-1.0.0/CONTRIBUTING.md
+-rw-r--r--   0 charlie    (501) staff       (20)     1116 2023-04-23 23:35:41.000000 pesummary-1.0.0/LICENSE.md
+-rw-r--r--   0 charlie    (501) staff       (20)      308 2022-09-20 20:30:50.000000 pesummary-1.0.0/MANIFEST.in
+-rw-r--r--   0 charlie    (501) staff       (20)     2550 2023-04-23 23:46:58.956483 pesummary-1.0.0/PKG-INFO
+-rw-r--r--   0 charlie    (501) staff       (20)     1481 2022-09-20 20:30:50.000000 pesummary-1.0.0/README.md
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.886496 pesummary-1.0.0/containers/
+-rw-r--r--   0 charlie    (501) staff       (20)      977 2022-09-20 20:30:50.000000 pesummary-1.0.0/containers/Dockerfile-template
+-rw-r--r--   0 charlie    (501) staff       (20)     1231 2023-04-23 23:35:41.000000 pesummary-1.0.0/containers/write_dockerfile.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.887886 pesummary-1.0.0/docs/
+-rw-r--r--   0 charlie    (501) staff       (20)       46 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/.gitattributes
+-rw-r--r--   0 charlie    (501) staff       (20)     1633 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/citing_pesummary.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.888109 pesummary-1.0.0/docs/conf/
+-rw-r--r--   0 charlie    (501) staff       (20)      693 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/conf/configuration.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     9118 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/conf.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.890174 pesummary-1.0.0/docs/core/
+-rw-r--r--   0 charlie    (501) staff       (20)     1339 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/Array.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2686 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/MCMCSamplesDict.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     5211 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/MultiAnalysisSamplesDict.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      391 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/ProbabilityDict.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      395 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/ProbabilityDict2D.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     3906 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/SamplesDict.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2993 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/bounded_kdes.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.891649 pesummary-1.0.0/docs/core/cli/
+-rw-r--r--   0 charlie    (501) staff       (20)      778 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summaryclean.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      636 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summarycombine.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      643 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summarycombine_posteriors.rst
+-rw-r--r--   0 charlie    (501) staff       (20)    15166 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summarycompare.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      366 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summaryextract.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      351 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summarymodify.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     3312 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/core/cli/summarypages.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      529 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summarypageslw.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2815 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/core/cli/summarypublication.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      451 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summarysplit.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      239 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/cli/summaryversion.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.892047 pesummary-1.0.0/docs/core/examples/
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/examples/bounded_kde.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/examples/bounded_kde_uniform.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/examples/bounded_kdeplot.png
+-rw-r--r--   0 charlie    (501) staff       (20)     3564 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/file_formats.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1225 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/index.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1079 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/pdf.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     4858 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/core/pesummary_file.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     4791 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/read.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      411 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/core/seaborn.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.892175 pesummary-1.0.0/docs/core/tutorials/
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.892979 pesummary-1.0.0/docs/core/tutorials/examples/
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/tutorials/examples/Histogram.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/tutorials/examples/MultiAnalysisCorner.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/tutorials/examples/MultiAnalysisHistogram.png
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/tutorials/examples/MultiAnalysisReverseTriangle.png
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/tutorials/examples/MultiAnalysisTriangle.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/tutorials/examples/violin.png
+-rw-r--r--   0 charlie    (501) staff       (20)     2839 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/core/tutorials/plotting_from_metafile.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1746 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/core/write.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1967 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/front.html.in
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.895194 pesummary-1.0.0/docs/gw/
+-rw-r--r--   0 charlie    (501) staff       (20)     3666 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/Conversion.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1929 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/SamplesDict.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     3249 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/calibration.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     5891 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/classification.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.896318 pesummary-1.0.0/docs/gw/cli/
+-rw-r--r--   0 charlie    (501) staff       (20)      186 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/cli/executable_descriptions.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1353 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/cli/summaryclassification.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1996 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/cli/summarydetchar.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      616 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/cli/summarygracedb.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      566 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/cli/summarypipe.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1433 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/cli/summaryrecreate.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      682 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/cli/summaryreview.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2637 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/cli/summarytgr.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.896903 pesummary-1.0.0/docs/gw/examples/
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/examples/GW190412_violin.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/examples/bounded_violin.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/examples/gaussian_violin.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/examples/split_violin.png
+-rw-r--r--   0 charlie    (501) staff       (20)     1317 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/fetch.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1674 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/file_formats.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     3319 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/index.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      238 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/parameters.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     3462 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/pesummary_file.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2948 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/psd.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1791 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/read.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     4759 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/remnant_fits.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2595 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/strain.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.898085 pesummary-1.0.0/docs/gw/summarypages/
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.898827 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/
+-rw-r--r--   0 charlie    (501) staff       (20)      692 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/classification.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1128 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/corner.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.899276 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/examples/
+-rw-r--r--   0 charlie    (501) staff       (20)      966 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/examples/.gitattributes
+-rw-r--r--   0 charlie    (501) staff       (20)      132 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/examples/corner.mp4
+-rw-r--r--   0 charlie    (501) staff       (20)      132 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/examples/interactive_corner_source.html
+-rw-r--r--   0 charlie    (501) staff       (20)      646 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/home.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      976 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/interactive.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1785 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/IMRPhenomPv3HM/mass_1.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      391 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/about.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.899716 pesummary-1.0.0/docs/gw/summarypages/comparison/
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.899877 pesummary-1.0.0/docs/gw/summarypages/comparison/examples/
+-rw-r--r--   0 charlie    (501) staff       (20)   639556 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/comparison/examples/interactive_ridgeline.html
+-rw-r--r--   0 charlie    (501) staff       (20)      615 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/comparison/home.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      912 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/comparison/interactive.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2030 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/comparison/mass_1.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      549 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/downloads.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.901215 pesummary-1.0.0/docs/gw/summarypages/examples/
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/examples/watermark.png
+-rw-r--r--   0 charlie    (501) staff       (20)      980 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/home.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      356 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/logging.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      504 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/publication.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1166 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/version.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      502 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/summarypages/watermark.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     4038 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tgr_file.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.902753 pesummary-1.0.0/docs/gw/tutorials/
+-rw-r--r--   0 charlie    (501) staff       (20)     1707 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/tutorials/GWTC1_plots.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      967 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/tutorials/comparing_public_data_files.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     9519 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/conversions_and_condor.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.905398 pesummary-1.0.0/docs/gw/tutorials/examples/
+-rw-r--r--   0 charlie    (501) staff       (20)      528 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/.gitattributes
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/2d_contour_plot_luminosity_distance_and_chirp_mass.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/2d_contour_plot_mass_1_and_mass_2.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/2d_contour_plot_theta_jn_and_luminosity_distance.png
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/GW150914.png
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/GW190814.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/Histogram.png
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/MultiAnalysisCorner.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/MultiAnalysisHistogram.png
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/MultiAnalysisReverseTriangle.png
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/MultiAnalysisTriangle.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/PSD.png
+-rw-r--r--   0 charlie    (501) staff       (20)      131 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/comparison_for_GW150914.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/event_scatter_plot_total_mass_and_redshift.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/spin_disk.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/uncertainty_waveform_td.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/violin.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/violin_plot_mass_ratio.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/waveform_fd.png
+-rw-r--r--   0 charlie    (501) staff       (20)      130 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/examples/waveform_td.png
+-rw-r--r--   0 charlie    (501) staff       (20)     3997 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/tutorials/interaction_with_ligo_skymap.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      801 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/make_your_own_page_from_metafile.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      811 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/tutorials/plot_waveform_on_strain_data.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     3473 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/plotting_from_metafile.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      709 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/tutorials/population_scatter_plot_GWTC-1.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      814 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/tutorials/public_pages.rst
+-rw-r--r--   0 charlie    (501) staff       (20)      690 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/tutorials/release_notebook.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1614 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/gw/tutorials/waveforms.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     4667 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/gw/violin.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     1891 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/index.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2845 2023-04-23 23:35:41.000000 pesummary-1.0.0/docs/installation.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.905525 pesummary-1.0.0/docs/io/
+-rw-r--r--   0 charlie    (501) staff       (20)     3014 2023-04-23 23:35:45.000000 pesummary-1.0.0/docs/io/read.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2351 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/virtual_environment.rst
+-rw-r--r--   0 charlie    (501) staff       (20)     2209 2022-09-20 20:30:50.000000 pesummary-1.0.0/docs/what_is_pesummary.rst
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.906275 pesummary-1.0.0/examples/
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.906928 pesummary-1.0.0/examples/core/
+-rw-r--r--   0 charlie    (501) staff       (20)     1127 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/core/bounded_kdeplot.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1002 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/core/extract_information_from_pesummary_file.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3120 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/core/multiple_analysis_custom_read.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1490 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/core/pesummary_data_structure.txt
+-rw-r--r--   0 charlie    (501) staff       (20)     2495 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/core/single_analysis_custom_read.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1711 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/custom_plot.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2174 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/extract_information_without_pesummary.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.909283 pesummary-1.0.0/examples/gw/
+-rw-r--r--   0 charlie    (501) staff       (20)     1097 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/GWTC-1_population_scatterplot.sh
+-rw-r--r--   0 charlie    (501) staff       (20)     2624 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/GWTC1_plots.sh
+-rw-r--r--   0 charlie    (501) staff       (20)     1599 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/compare_GW150914.py
+-rw-r--r--   0 charlie    (501) staff       (20)      941 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/extract_information_from_pesummary_file.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2709 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/gw/extract_information_from_pesummary_file_without_pesummary.py
+-rw-r--r--   0 charlie    (501) staff       (20)      895 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/fetch_open_data.py
+-rw-r--r--   0 charlie    (501) staff       (20)      681 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/gw/fetch_open_strain.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1699 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/latex.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1351 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/gw/ligo_skymap.py
+-rwxr-xr-x   0 charlie    (501) staff       (20)      523 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/make_public_release.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1390 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/gw/making_a_waveform_in_frequency_domain.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1281 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/gw/making_a_waveform_in_time_domain.py
+-rw-r--r--   0 charlie    (501) staff       (20)      748 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/gw/making_a_waveform_in_time_domain_with_uncertainty.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2690 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/pesummary_data_structure.txt
+-rw-r--r--   0 charlie    (501) staff       (20)     1034 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/plot_waveform_on_strain_data_GW150914.py
+-rw-r--r--   0 charlie    (501) staff       (20)      775 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/gw/plot_waveform_on_strain_data_GW190814.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1641 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/gw/prior_conditioning.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1504 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/gw/violin_plots.py
+-rw-r--r--   0 charlie    (501) staff       (20)      108 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/pesummary_core.ini
+-rw-r--r--   0 charlie    (501) staff       (20)      282 2022-09-20 20:30:50.000000 pesummary-1.0.0/examples/pesummary_gw.ini
+-rw-r--r--   0 charlie    (501) staff       (20)     1431 2023-04-23 23:35:41.000000 pesummary-1.0.0/examples/run_with_python.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.909901 pesummary-1.0.0/pesummary/
+-rw-r--r--   0 charlie    (501) staff       (20)      303 2023-04-23 23:46:58.000000 pesummary-1.0.0/pesummary/.version
+-rw-r--r--   0 charlie    (501) staff       (20)     1997 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)      160 2023-04-23 23:46:58.000000 pesummary-1.0.0/pesummary/_version.py
+-rw-r--r--   0 charlie    (501) staff       (20)     5674 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/_version_helper.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.914301 pesummary-1.0.0/pesummary/cli/
+-rw-r--r--   0 charlie    (501) staff       (20)       52 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/cli/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7226 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/cli/summaryclassification.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2508 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summaryclean.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1257 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarycombine.py
+-rw-r--r--   0 charlie    (501) staff       (20)     8223 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarycombine_posteriors.py
+-rw-r--r--   0 charlie    (501) staff       (20)    11646 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarycompare.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3778 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarydetchar.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2563 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summaryextract.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1434 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarygracedb.py
+-rw-r--r--   0 charlie    (501) staff       (20)     9668 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summaryjscompare.py
+-rw-r--r--   0 charlie    (501) staff       (20)    24057 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarymodify.py
+-rw-r--r--   0 charlie    (501) staff       (20)    10894 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarypages.py
+-rw-r--r--   0 charlie    (501) staff       (20)     4097 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarypageslw.py
+-rw-r--r--   0 charlie    (501) staff       (20)    31078 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarypipe.py
+-rw-r--r--   0 charlie    (501) staff       (20)    13421 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summaryplots.py
+-rw-r--r--   0 charlie    (501) staff       (20)    13547 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarypublication.py
+-rw-r--r--   0 charlie    (501) staff       (20)    10990 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summaryrecreate.py
+-rw-r--r--   0 charlie    (501) staff       (20)    31148 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summaryreview.py
+-rw-r--r--   0 charlie    (501) staff       (20)     6758 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarysplit.py
+-rw-r--r--   0 charlie    (501) staff       (20)    15327 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarytest.py
+-rw-r--r--   0 charlie    (501) staff       (20)     9472 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/cli/summarytgr.py
+-rw-r--r--   0 charlie    (501) staff       (20)      386 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/cli/summaryversion.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.915021 pesummary-1.0.0/pesummary/conf/
+-rw-r--r--   0 charlie    (501) staff       (20)       52 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/conf/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     5198 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/conf/caption.py
+-rw-r--r--   0 charlie    (501) staff       (20)     4146 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/conf/configuration.py
+-rw-r--r--   0 charlie    (501) staff       (20)      422 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/conf/matplotlib_rcparams.sty
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.915727 pesummary-1.0.0/pesummary/core/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/__init__.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.917600 pesummary-1.0.0/pesummary/core/cli/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/cli/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)    10430 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/cli/actions.py
+-rw-r--r--   0 charlie    (501) staff       (20)    83845 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/core/cli/inputs.py
+-rw-r--r--   0 charlie    (501) staff       (20)    25326 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/cli/parser.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.919046 pesummary-1.0.0/pesummary/core/css/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/css/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)      178 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/css/font.css
+-rw-r--r--   0 charlie    (501) staff       (20)     1601 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/css/image_styles.css
+-rw-r--r--   0 charlie    (501) staff       (20)     1137 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/css/navbar.css
+-rw-r--r--   0 charlie    (501) staff       (20)     1806 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/css/side_bar.css
+-rw-r--r--   0 charlie    (501) staff       (20)      612 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/css/table.css
+-rw-r--r--   0 charlie    (501) staff       (20)      879 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/css/toggle.css
+-rw-r--r--   0 charlie    (501) staff       (20)      411 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/css/watermark.css
+-rw-r--r--   0 charlie    (501) staff       (20)     5928 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/fetch.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.920067 pesummary-1.0.0/pesummary/core/file/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/file/__init__.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.922362 pesummary-1.0.0/pesummary/core/file/formats/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/file/formats/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)    39308 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/base_read.py
+-rw-r--r--   0 charlie    (501) staff       (20)    14406 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/bilby.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2252 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/file/formats/csv.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2717 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/dat.py
+-rw-r--r--   0 charlie    (501) staff       (20)    10261 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/default.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7086 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/hdf5.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3282 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/ini.py
+-rw-r--r--   0 charlie    (501) staff       (20)     6841 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/json.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3776 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/file/formats/numpy.py
+-rw-r--r--   0 charlie    (501) staff       (20)    27079 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/pesummary.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1288 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/pickle.py
+-rw-r--r--   0 charlie    (501) staff       (20)     6490 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/formats/sql.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1619 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/injection.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3756 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/file/mcmc.py
+-rw-r--r--   0 charlie    (501) staff       (20)    26372 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/meta_file.py
+-rw-r--r--   0 charlie    (501) staff       (20)     9039 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/file/read.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1942 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/finish.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.924171 pesummary-1.0.0/pesummary/core/js/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     6956 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/combine_corner.js
+-rw-r--r--   0 charlie    (501) staff       (20)     1007 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/expert.js
+-rw-r--r--   0 charlie    (501) staff       (20)     4825 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/grab.js
+-rw-r--r--   0 charlie    (501) staff       (20)     2272 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/html_to_csv.js
+-rw-r--r--   0 charlie    (501) staff       (20)     1318 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/html_to_json.js
+-rw-r--r--   0 charlie    (501) staff       (20)      705 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/html_to_shell.js
+-rw-r--r--   0 charlie    (501) staff       (20)     1427 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/modal.js
+-rw-r--r--   0 charlie    (501) staff       (20)     2361 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/multi_dropbar.js
+-rw-r--r--   0 charlie    (501) staff       (20)     7471 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/multiple_posteriors.js
+-rw-r--r--   0 charlie    (501) staff       (20)     2378 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/search.js
+-rw-r--r--   0 charlie    (501) staff       (20)     1313 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/js/side_bar.js
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.924442 pesummary-1.0.0/pesummary/core/notebook/
+-rw-r--r--   0 charlie    (501) staff       (20)      134 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/notebook/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     6430 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/notebook/notebook.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.926424 pesummary-1.0.0/pesummary/core/plots/
+-rw-r--r--   0 charlie    (501) staff       (20)      507 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     8888 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/plots/bounded_1d_kde.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3624 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/bounded_2d_kde.py
+-rw-r--r--   0 charlie    (501) staff       (20)     8503 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/plots/corner.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2094 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/figure.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7638 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/plots/interactive.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1841 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/interpolate.py
+-rw-r--r--   0 charlie    (501) staff       (20)      206 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/latex_labels.py
+-rw-r--r--   0 charlie    (501) staff       (20)    52496 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/plots/main.py
+-rw-r--r--   0 charlie    (501) staff       (20)    31427 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/core/plots/plot.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2589 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/population.py
+-rw-r--r--   0 charlie    (501) staff       (20)    30277 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/publication.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.927016 pesummary-1.0.0/pesummary/core/plots/seaborn/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/seaborn/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)    15430 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/plots/seaborn/kde.py
+-rw-r--r--   0 charlie    (501) staff       (20)    33102 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/core/plots/seaborn/violin.py
+-rw-r--r--   0 charlie    (501) staff       (20)      748 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/reweight.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.928200 pesummary-1.0.0/pesummary/core/webpage/
+-rw-r--r--   0 charlie    (501) staff       (20)     3038 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/core/webpage/base.py
+-rw-r--r--   0 charlie    (501) staff       (20)      825 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/core/webpage/copyright.txt
+-rw-r--r--   0 charlie    (501) staff       (20)    75272 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/core/webpage/main.py
+-rw-r--r--   0 charlie    (501) staff       (20)     8151 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/core/webpage/tables.py
+-rw-r--r--   0 charlie    (501) staff       (20)    53226 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/core/webpage/webpage.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.930173 pesummary-1.0.0/pesummary/gw/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)    20564 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/gw/classification.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.930803 pesummary-1.0.0/pesummary/gw/cli/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/cli/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)    62745 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/gw/cli/inputs.py
+-rw-r--r--   0 charlie    (501) staff       (20)    21564 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/cli/parser.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.933361 pesummary-1.0.0/pesummary/gw/conversions/
+-rw-r--r--   0 charlie    (501) staff       (20)    94137 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/conversions/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2299 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/conversions/angles.py
+-rw-r--r--   0 charlie    (501) staff       (20)     5839 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/conversions/cosmology.py
+-rw-r--r--   0 charlie    (501) staff       (20)    14219 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/conversions/evolve.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3743 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/conversions/mass.py
+-rw-r--r--   0 charlie    (501) staff       (20)    31392 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/conversions/nrutils.py
+-rw-r--r--   0 charlie    (501) staff       (20)    27930 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/conversions/remnant.py
+-rw-r--r--   0 charlie    (501) staff       (20)    38522 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/conversions/snr.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7953 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/conversions/spins.py
+-rw-r--r--   0 charlie    (501) staff       (20)    22067 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/conversions/tgr.py
+-rw-r--r--   0 charlie    (501) staff       (20)    13450 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/conversions/tidal.py
+-rw-r--r--   0 charlie    (501) staff       (20)      866 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/conversions/time.py
+-rw-r--r--   0 charlie    (501) staff       (20)      408 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/conversions/utils.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1991 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/cosmology.py
+-rw-r--r--   0 charlie    (501) staff       (20)     5841 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/fetch.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.935343 pesummary-1.0.0/pesummary/gw/file/
+-rw-r--r--   0 charlie    (501) staff       (20)      750 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/file/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7153 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/calibration.py
+-rw-r--r--   0 charlie    (501) staff       (20)      472 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/file/conversions.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.936679 pesummary-1.0.0/pesummary/gw/file/formats/
+-rw-r--r--   0 charlie    (501) staff       (20)     3495 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/formats/GWTC1.py
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/file/formats/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)    25811 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/formats/base_read.py
+-rw-r--r--   0 charlie    (501) staff       (20)    10739 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/formats/bilby.py
+-rw-r--r--   0 charlie    (501) staff       (20)    10090 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/formats/default.py
+-rw-r--r--   0 charlie    (501) staff       (20)    20402 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/formats/lalinference.py
+-rw-r--r--   0 charlie    (501) staff       (20)    14349 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/formats/pesummary.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1971 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/file/formats/princeton.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1672 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/formats/xml.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2095 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/injection.py
+-rw-r--r--   0 charlie    (501) staff       (20)    14137 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/meta_file.py
+-rw-r--r--   0 charlie    (501) staff       (20)    10521 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/psd.py
+-rw-r--r--   0 charlie    (501) staff       (20)     4126 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/file/read.py
+-rw-r--r--   0 charlie    (501) staff       (20)     4998 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/file/skymap.py
+-rw-r--r--   0 charlie    (501) staff       (20)    26901 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/file/standard_names.py
+-rw-r--r--   0 charlie    (501) staff       (20)     8572 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/file/strain.py
+-rw-r--r--   0 charlie    (501) staff       (20)     4556 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/finish.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2048 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/gracedb.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.937099 pesummary-1.0.0/pesummary/gw/notebook/
+-rw-r--r--   0 charlie    (501) staff       (20)      151 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/notebook/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1188 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/notebook/notebook.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7067 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/notebook/public.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.939569 pesummary-1.0.0/pesummary/gw/plots/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-22 09:35:49.000000 pesummary-1.0.0/pesummary/gw/plots/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2666 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/plots/bounds.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2244 2022-09-22 09:35:49.000000 pesummary-1.0.0/pesummary/gw/plots/cmap.py
+-rw-r--r--   0 charlie    (501) staff       (20)     8714 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/plots/cylon.csv
+-rw-r--r--   0 charlie    (501) staff       (20)     9876 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/gw/plots/detchar.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7431 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/plots/latex_labels.py
+-rw-r--r--   0 charlie    (501) staff       (20)    46366 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/gw/plots/main.py
+-rw-r--r--   0 charlie    (501) staff       (20)    54314 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/gw/plots/plot.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3204 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/plots/public.py
+-rw-r--r--   0 charlie    (501) staff       (20)    17321 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/plots/publication.py
+-rw-r--r--   0 charlie    (501) staff       (20)     6012 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/plots/tgr.py
+-rw-r--r--   0 charlie    (501) staff       (20)     5131 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/pycbc.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3553 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/reweight.py
+-rw-r--r--   0 charlie    (501) staff       (20)    15728 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/waveform.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.940313 pesummary-1.0.0/pesummary/gw/webpage/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/webpage/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)    35253 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/webpage/main.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7405 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/gw/webpage/public.py
+-rw-r--r--   0 charlie    (501) staff       (20)    12756 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/gw/webpage/tgr.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.940842 pesummary-1.0.0/pesummary/io/
+-rw-r--r--   0 charlie    (501) staff       (20)     1244 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/io/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3562 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/io/read.py
+-rw-r--r--   0 charlie    (501) staff       (20)     4510 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/io/write.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.949012 pesummary-1.0.0/pesummary/tests/
+-rw-r--r--   0 charlie    (501) staff       (20)      109 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)      329 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/_bilby.sh
+-rw-r--r--   0 charlie    (501) staff       (20)     1949 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/action_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    17771 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/base.py
+-rw-r--r--   0 charlie    (501) staff       (20)      514 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/bilby.sh
+-rw-r--r--   0 charlie    (501) staff       (20)      656 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/bilby_mcmc.sh
+-rw-r--r--   0 charlie    (501) staff       (20)      933 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/bilby_pipe.sh
+-rw-r--r--   0 charlie    (501) staff       (20)     1917 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/bounded_kde_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2682 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/calibration_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7459 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/tests/classification_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    47589 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/conversion_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3026 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/cosmology_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)       94 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/example_config.ini
+-rw-r--r--   0 charlie    (501) staff       (20)    89588 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/executable_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1076 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/executables.sh
+-rw-r--r--   0 charlie    (501) staff       (20)     1626 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/existing_file.py
+-rw-r--r--   0 charlie    (501) staff       (20)     4231 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/fetch_test.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.952145 pesummary-1.0.0/pesummary/tests/files/
+-rw-r--r--   0 charlie    (501) staff       (20)       55 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/files/.gitattributes
+-rw-r--r--   0 charlie    (501) staff       (20)   843570 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/files/calibration_envelope.txt
+-rw-r--r--   0 charlie    (501) staff       (20)      545 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/files/config_bilby.ini
+-rw-r--r--   0 charlie    (501) staff       (20)     3576 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/files/config_lalinference.ini
+-rw-r--r--   0 charlie    (501) staff       (20)       19 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/files/example.ini
+-rw-r--r--   0 charlie    (501) staff       (20)   264597 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/files/lal_pdf_for_summarytgr.dat.gz
+-rw-r--r--   0 charlie    (501) staff       (20)    96127 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/files/psd_file.txt
+-rw-r--r--   0 charlie    (501) staff       (20)     1250 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/finish_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1152 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/imports.sh
+-rw-r--r--   0 charlie    (501) staff       (20)     2456 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/injection_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    29221 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/input_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1875 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/lalinference.sh
+-rw-r--r--   0 charlie    (501) staff       (20)     2461 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/ligo_skymap_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    12707 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/main_injection.xml
+-rw-r--r--   0 charlie    (501) staff       (20)       69 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/main_input.ini
+-rw-r--r--   0 charlie    (501) staff       (20)       33 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/main_input_core.ini
+-rw-r--r--   0 charlie    (501) staff       (20)    19872 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/meta_file_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     1487 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/notebook_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    20218 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/plot_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3994 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/psd_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)      667 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/pycbc.sh
+-rw-r--r--   0 charlie    (501) staff       (20)     4930 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/pycbc_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    75493 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/read_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3482 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/reweight_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3242 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/strain_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    11818 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/summaryplots_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    49822 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/utils_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     8479 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/tests/waveform_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    11475 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/webpage_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)    21449 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/workflow_test.py
+-rw-r--r--   0 charlie    (501) staff       (20)     7496 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/tests/write_test.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.955922 pesummary-1.0.0/pesummary/utils/
+-rw-r--r--   0 charlie    (501) staff       (20)      240 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/utils/__init__.py
+-rw-r--r--   0 charlie    (501) staff       (20)    11023 2023-04-23 23:35:41.000000 pesummary-1.0.0/pesummary/utils/array.py
+-rw-r--r--   0 charlie    (501) staff       (20)     9559 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/utils/decorators.py
+-rw-r--r--   0 charlie    (501) staff       (20)     8392 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/utils/dict.py
+-rw-r--r--   0 charlie    (501) staff       (20)      507 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/utils/exceptions.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2993 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/utils/list.py
+-rw-r--r--   0 charlie    (501) staff       (20)     3187 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/utils/parameters.py
+-rw-r--r--   0 charlie    (501) staff       (20)    16624 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/utils/pdf.py
+-rw-r--r--   0 charlie    (501) staff       (20)     9255 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/utils/probability_dict.py
+-rw-r--r--   0 charlie    (501) staff       (20)    75613 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/utils/samples_dict.py
+-rw-r--r--   0 charlie    (501) staff       (20)     2991 2022-09-20 20:30:50.000000 pesummary-1.0.0/pesummary/utils/tqdm.py
+-rw-r--r--   0 charlie    (501) staff       (20)    35483 2023-04-23 23:35:45.000000 pesummary-1.0.0/pesummary/utils/utils.py
+drwxr-xr-x   0 charlie    (501) staff       (20)        0 2023-04-23 23:46:58.910666 pesummary-1.0.0/pesummary.egg-info/
+-rw-r--r--   0 charlie    (501) staff       (20)     2550 2023-04-23 23:46:58.000000 pesummary-1.0.0/pesummary.egg-info/PKG-INFO
+-rw-r--r--   0 charlie    (501) staff       (20)    13263 2023-04-23 23:46:58.000000 pesummary-1.0.0/pesummary.egg-info/SOURCES.txt
+-rw-r--r--   0 charlie    (501) staff       (20)        1 2023-04-23 23:46:58.000000 pesummary-1.0.0/pesummary.egg-info/dependency_links.txt
+-rw-r--r--   0 charlie    (501) staff       (20)     1099 2023-04-23 23:46:58.000000 pesummary-1.0.0/pesummary.egg-info/entry_points.txt
+-rw-r--r--   0 charlie    (501) staff       (20)      705 2023-04-23 23:46:58.000000 pesummary-1.0.0/pesummary.egg-info/requires.txt
+-rw-r--r--   0 charlie    (501) staff       (20)       10 2023-04-23 23:46:58.000000 pesummary-1.0.0/pesummary.egg-info/top_level.txt
+-rw-r--r--   0 charlie    (501) staff       (20)     4465 2023-04-23 23:35:45.000000 pesummary-1.0.0/pyproject.toml
+-rw-r--r--   0 charlie    (501) staff       (20)       38 2023-04-23 23:46:58.956707 pesummary-1.0.0/setup.cfg
+-rw-r--r--   0 charlie    (501) staff       (20)     1337 2022-09-20 20:30:50.000000 pesummary-1.0.0/setup.py
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive (GNU)
+POSIX tar archive
```

### Comparing `pesummary-0.9.1/pesummary.egg-info/PKG-INFO` & `pesummary-1.0.0/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,39 +1,49 @@
 Metadata-Version: 2.1
 Name: pesummary
-Version: 0.9.1
+Version: 1.0.0
 Summary: Python package to produce summary pages for Parameter estimation codes
-Home-page: https://git.ligo.org/lscsoft/pesummary
-Author: Charlie Hoy
-Author-email: charlie.hoy@ligo.org
+Author-email: Charlie Hoy <charlie.hoy@ligo.org>
 License: MIT
-Download-URL: https://git.ligo.org/lscsoft/pesummary
-Description: # PESummary
-        
-        ## Release status
-        
-        [![PyPI version](https://badge.fury.io/py/pesummary.svg)](http://badge.fury.io/py/pesummary)
-        [![Conda version](https://img.shields.io/conda/vn/conda-forge/pesummary.svg)](https://anaconda.org/conda-forge/pesummary/)
-        [![PyPI - Downloads](https://img.shields.io/pypi/dm/pesummary)](https://img.shields.io/pypi/dm/pesummary)
-        [![Total Downloads](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)](https://anaconda.org/conda-forge/pycbc/badges/downloads.svg)
-        
-        [![License](https://img.shields.io/pypi/l/pesummary.svg)](https://choosealicense.com/licenses/mit/)
-        [![Python versions](https://img.shields.io/pypi/pyversions/pesummary.svg)](https://img.shields.io/pypi/pyversions/pesummary.svg)
-        [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3937807.svg)](https://doi.org/10.5281/zenodo.3937807)
-        
-        ## Development status
-        
-        [![Pipeline Status](https://git.ligo.org/lscsoft/pesummary/badges/master/pipeline.svg)](https://git.ligo.org/lscsoft/pesummary/commits/master)
-        [![Coverage report](https://docs.ligo.org/lscsoft/pesummary/coverage_badge.svg)](https://docs.ligo.org/lscsoft/pesummary/htmlcov/index.html)
-        
-        This package helps the user to generate summary webpages to visualise the output from any sample generating code.
-        
-         * [Installation instructions](https://docs.ligo.org/lscsoft/pesummary/installation.html)
-         * [Documentation](https://docs.ligo.org/lscsoft/pesummary)
-         * [Issue Tracker](https://git.ligo.org/lscsoft/pesummary/issues)
-        
-Platform: UNKNOWN
-Classifier: Programming Language :: Python :: 3.5
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
+Project-URL: Homepage, https://lscsoft.docs.ligo.org/pesummary
+Project-URL: Documentation, https://lscsoft.docs.ligo.org/pesummary/
+Project-URL: Issue Tracker, https://git.ligo.org/lscsoft/pesummary/-/issues/
+Project-URL: Source Code, https://git.ligo.org/lscsoft/pesummary.git
+Project-URL: Download, https://lscsoft.docs.ligo.org/pesummary/stable/installation.html
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Natural Language :: English
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Requires-Python: >=3.8
 Description-Content-Type: text/markdown
+Provides-Extra: docs
+Provides-Extra: lint
+Provides-Extra: test
+Provides-Extra: extras
+License-File: LICENSE.md
+
+# PESummary
+
+## Release status
+
+[![PyPI version](https://badge.fury.io/py/pesummary.svg)](http://badge.fury.io/py/pesummary)
+[![Conda version](https://img.shields.io/conda/vn/conda-forge/pesummary.svg)](https://anaconda.org/conda-forge/pesummary/)
+[![PyPI - Downloads](https://img.shields.io/pypi/dm/pesummary)](https://img.shields.io/pypi/dm/pesummary)
+[![Total Downloads](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)
+
+[![License](https://img.shields.io/pypi/l/pesummary.svg)](https://choosealicense.com/licenses/mit/)
+[![Python versions](https://img.shields.io/pypi/pyversions/pesummary.svg)](https://img.shields.io/pypi/pyversions/pesummary.svg)
+[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4771187.svg)](https://doi.org/10.5281/zenodo.4771187)
+
+## Development status
+
+[![Pipeline Status](https://git.ligo.org/lscsoft/pesummary/badges/master/pipeline.svg)](https://git.ligo.org/lscsoft/pesummary/commits/master)
+[![Coverage report](https://docs.ligo.org/lscsoft/pesummary/coverage_badge.svg)](https://docs.ligo.org/lscsoft/pesummary/htmlcov/index.html)
+
+This package helps the user to generate summary webpages to visualise the output from any sample generating code.
+
+ * [Installation instructions](https://docs.ligo.org/lscsoft/pesummary/installation.html)
+ * [Documentation](https://docs.ligo.org/lscsoft/pesummary)
+ * [Issue Tracker](https://git.ligo.org/lscsoft/pesummary/issues)
```

### Comparing `pesummary-0.9.1/pesummary.egg-info/entry_points.txt` & `pesummary-1.0.0/pesummary.egg-info/entry_points.txt`

 * *Files 6% similar despite different names*

```diff
@@ -1,19 +1,22 @@
 [console_scripts]
 summaryclassification = pesummary.cli.summaryclassification:main
 summaryclean = pesummary.cli.summaryclean:main
 summarycombine = pesummary.cli.summarycombine:main
+summarycombine_posteriors = pesummary.cli.summarycombine_posteriors:main
 summarycompare = pesummary.cli.summarycompare:main
 summarydetchar = pesummary.cli.summarydetchar:main
+summaryextract = pesummary.cli.summaryextract:main
 summarygracedb = pesummary.cli.summarygracedb:main
 summaryjscompare = pesummary.cli.summaryjscompare:main
 summarymodify = pesummary.cli.summarymodify:main
 summarypages = pesummary.cli.summarypages:main
 summarypageslw = pesummary.cli.summarypageslw:main
 summarypipe = pesummary.cli.summarypipe:main
 summaryplots = pesummary.cli.summaryplots:main
 summarypublication = pesummary.cli.summarypublication:main
 summaryrecreate = pesummary.cli.summaryrecreate:main
 summaryreview = pesummary.cli.summaryreview:main
+summarysplit = pesummary.cli.summarysplit:main
 summarytest = pesummary.cli.summarytest:main
+summarytgr = pesummary.cli.summarytgr:main
 summaryversion = pesummary.cli.summaryversion:main
-
```

### Comparing `pesummary-0.9.1/PKG-INFO` & `pesummary-1.0.0/pesummary.egg-info/PKG-INFO`

 * *Files 12% similar despite different names*

```diff
@@ -1,39 +1,49 @@
 Metadata-Version: 2.1
 Name: pesummary
-Version: 0.9.1
+Version: 1.0.0
 Summary: Python package to produce summary pages for Parameter estimation codes
-Home-page: https://git.ligo.org/lscsoft/pesummary
-Author: Charlie Hoy
-Author-email: charlie.hoy@ligo.org
+Author-email: Charlie Hoy <charlie.hoy@ligo.org>
 License: MIT
-Download-URL: https://git.ligo.org/lscsoft/pesummary
-Description: # PESummary
-        
-        ## Release status
-        
-        [![PyPI version](https://badge.fury.io/py/pesummary.svg)](http://badge.fury.io/py/pesummary)
-        [![Conda version](https://img.shields.io/conda/vn/conda-forge/pesummary.svg)](https://anaconda.org/conda-forge/pesummary/)
-        [![PyPI - Downloads](https://img.shields.io/pypi/dm/pesummary)](https://img.shields.io/pypi/dm/pesummary)
-        [![Total Downloads](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)](https://anaconda.org/conda-forge/pycbc/badges/downloads.svg)
-        
-        [![License](https://img.shields.io/pypi/l/pesummary.svg)](https://choosealicense.com/licenses/mit/)
-        [![Python versions](https://img.shields.io/pypi/pyversions/pesummary.svg)](https://img.shields.io/pypi/pyversions/pesummary.svg)
-        [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3937807.svg)](https://doi.org/10.5281/zenodo.3937807)
-        
-        ## Development status
-        
-        [![Pipeline Status](https://git.ligo.org/lscsoft/pesummary/badges/master/pipeline.svg)](https://git.ligo.org/lscsoft/pesummary/commits/master)
-        [![Coverage report](https://docs.ligo.org/lscsoft/pesummary/coverage_badge.svg)](https://docs.ligo.org/lscsoft/pesummary/htmlcov/index.html)
-        
-        This package helps the user to generate summary webpages to visualise the output from any sample generating code.
-        
-         * [Installation instructions](https://docs.ligo.org/lscsoft/pesummary/installation.html)
-         * [Documentation](https://docs.ligo.org/lscsoft/pesummary)
-         * [Issue Tracker](https://git.ligo.org/lscsoft/pesummary/issues)
-        
-Platform: UNKNOWN
-Classifier: Programming Language :: Python :: 3.5
-Classifier: Programming Language :: Python :: 3.6
-Classifier: Programming Language :: Python :: 3.7
+Project-URL: Homepage, https://lscsoft.docs.ligo.org/pesummary
+Project-URL: Documentation, https://lscsoft.docs.ligo.org/pesummary/
+Project-URL: Issue Tracker, https://git.ligo.org/lscsoft/pesummary/-/issues/
+Project-URL: Source Code, https://git.ligo.org/lscsoft/pesummary.git
+Project-URL: Download, https://lscsoft.docs.ligo.org/pesummary/stable/installation.html
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Natural Language :: English
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Requires-Python: >=3.8
 Description-Content-Type: text/markdown
+Provides-Extra: docs
+Provides-Extra: lint
+Provides-Extra: test
+Provides-Extra: extras
+License-File: LICENSE.md
+
+# PESummary
+
+## Release status
+
+[![PyPI version](https://badge.fury.io/py/pesummary.svg)](http://badge.fury.io/py/pesummary)
+[![Conda version](https://img.shields.io/conda/vn/conda-forge/pesummary.svg)](https://anaconda.org/conda-forge/pesummary/)
+[![PyPI - Downloads](https://img.shields.io/pypi/dm/pesummary)](https://img.shields.io/pypi/dm/pesummary)
+[![Total Downloads](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)
+
+[![License](https://img.shields.io/pypi/l/pesummary.svg)](https://choosealicense.com/licenses/mit/)
+[![Python versions](https://img.shields.io/pypi/pyversions/pesummary.svg)](https://img.shields.io/pypi/pyversions/pesummary.svg)
+[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4771187.svg)](https://doi.org/10.5281/zenodo.4771187)
+
+## Development status
+
+[![Pipeline Status](https://git.ligo.org/lscsoft/pesummary/badges/master/pipeline.svg)](https://git.ligo.org/lscsoft/pesummary/commits/master)
+[![Coverage report](https://docs.ligo.org/lscsoft/pesummary/coverage_badge.svg)](https://docs.ligo.org/lscsoft/pesummary/htmlcov/index.html)
+
+This package helps the user to generate summary webpages to visualise the output from any sample generating code.
+
+ * [Installation instructions](https://docs.ligo.org/lscsoft/pesummary/installation.html)
+ * [Documentation](https://docs.ligo.org/lscsoft/pesummary)
+ * [Issue Tracker](https://git.ligo.org/lscsoft/pesummary/issues)
```

### Comparing `pesummary-0.9.1/LICENSE.md` & `pesummary-1.0.0/LICENSE.md`

 * *Files 12% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 MIT License
 
-Copyright (c) 2018 Charlie Hoy <charlie.hoy@ligo.org>
+Copyright (c) 2018-2021 Charlie Hoy
+              2021-2022 Cardiff University
 
 Permission is hereby granted, free of charge, to any person obtaining a copy
 of this software and associated documentation files (the "Software"), to deal
 in the Software without restriction, including without limitation the rights
 to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
 copies of the Software, and to permit persons to whom the Software is
 furnished to do so, subject to the following conditions:
```

### Comparing `pesummary-0.9.1/pesummary/core/file/formats/bilby.py` & `pesummary-1.0.0/pesummary/core/file/formats/bilby.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,33 +1,34 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
 import numpy as np
-from pesummary.core.file.formats.base_read import Read
+from pesummary.core.file.formats.base_read import SingleAnalysisRead
 from pesummary.core.plots.latex_labels import latex_labels
 from pesummary import conf
 from pesummary.utils.utils import logger
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
+
+def _load_bilby(path):
+    """Wrapper for `bilby.core.result.read_in_result`
+
+    Parameters
+    ----------
+    path: str
+        path to the bilby result file you wish to load
+    """
+    from bilby.core.result import read_in_result
+    return read_in_result(filename=path)
+
 
 def read_bilby(
     path, disable_prior=False, complex_params=[], latex_dict=latex_labels,
-    **kwargs
+    nsamples_for_prior=None, _bilby_class=None, **kwargs
 ):
     """Grab the parameters and samples in a bilby file
 
     Parameters
     ----------
     path: str
         path to the result file you wish to read in
@@ -36,18 +37,20 @@
         Default False
     complex_params: list, optional
         list of parameters stored in the bilby result file which are complex
         and you wish to store the `amplitude` and `angle` as seperate
         posterior distributions
     latex_dict: dict, optional
         list of latex labels for each parameter
+    nsamples_for_prior: int, optional
+        number of samples to draw from the analytic priors
     """
-    from bilby.core.result import read_in_result
-
-    bilby_object = read_in_result(filename=path)
+    if _bilby_class is None:
+        _bilby_class = Bilby
+    bilby_object = _load_bilby(path)
     posterior = bilby_object.posterior
     _original_keys = posterior.keys()
     for key in _original_keys:
         for param in complex_params:
             if param in key and any(np.iscomplex(posterior[key])):
                 posterior[key + "_amp"] = abs(posterior[key])
                 posterior[key + "_angle"] = np.angle(posterior[key])
@@ -76,64 +79,75 @@
                 + bilby_object.search_parameter_keys
                 + bilby_object.fixed_parameter_keys):
             if key not in latex_dict:
                 label = bilby_object.get_latex_labels_from_parameter_keys(
                     [key])[0]
                 latex_dict[key] = label
     try:
-        extra_kwargs = Bilby.grab_extra_kwargs(bilby_object)
+        extra_kwargs = _bilby_class.grab_extra_kwargs(bilby_object)
     except Exception:
         extra_kwargs = {"sampler": {}, "meta_data": {}}
     extra_kwargs["sampler"]["nsamples"] = len(samples)
+    extra_kwargs["sampler"]["pe_algorithm"] = "bilby"
     try:
         version = bilby_object.version
+        if isinstance(version, (list, np.ndarray)):
+            version = version[0]
     except Exception as e:
         version = None
 
     data = {
         "parameters": parameters,
         "samples": samples.tolist(),
         "injection": injection,
         "version": version,
         "kwargs": extra_kwargs
     }
+    if bilby_object.meta_data is not None:
+        if "command_line_args" in bilby_object.meta_data.keys():
+            data["config"] = {
+                "config": bilby_object.meta_data["command_line_args"]
+            }
     if not disable_prior:
-        prior_samples = Bilby.grab_priors(bilby_object, nsamples=len(samples))
+        logger.debug("Drawing prior samples from bilby result file")
+        if nsamples_for_prior is None:
+            nsamples_for_prior = len(samples)
+        prior_samples = Bilby.grab_priors(
+            bilby_object, nsamples=nsamples_for_prior
+        )
         data["prior"] = {"samples": prior_samples}
         if len(prior_samples):
             data["prior"]["analytic"] = prior_samples.analytic
+    else:
+        try:
+            _prior = bilby_object.priors
+            data["prior"] = {
+                "samples": {},
+                "analytic": {key: str(item) for key, item in _prior.items()}
+            }
+        except (AttributeError, KeyError):
+            pass
     return data
 
 
-def write_bilby(
-    parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
-    extension="json", save=True, analytic_priors=None, cls=None,
+def to_bilby(
+    parameters, samples, label=None, analytic_priors=None, cls=None,
     meta_data=None, **kwargs
 ):
-    """Write a set of samples to a bilby file
+    """Convert a set of samples to a bilby object
 
     Parameters
     ----------
     parameters: list
         list of parameters
     samples: 2d list
         list of samples. Columns correspond to a given parameter
-    outdir: str, optional
-        directory to write the dat file
     label: str, optional
         The label of the analysis. This is used in the filename if a filename
         if not specified
-    filename: str, optional
-        The name of the file that you wish to write
-    overwrite: Bool, optional
-        If True, an existing file of the same name will be overwritten
-    extension: str, optional
-        file extension for the bilby result file. Default json.
-    save: Bool, optional
-        if True, save the bilby object to file
     """
     from bilby.core.result import Result
     from bilby.core.prior import Prior, PriorDict
     from pandas import DataFrame
 
     if cls is None:
         cls = Result
@@ -144,22 +158,128 @@
         priors = {param: Prior() for param in parameters}
         search_parameters = parameters
     posterior_data_frame = DataFrame(samples, columns=parameters)
     bilby_object = cls(
         search_parameter_keys=search_parameters, samples=samples, priors=priors,
         posterior=posterior_data_frame, label="pesummary_%s" % label,
     )
+    return bilby_object
+
+
+def _write_bilby(
+    parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
+    extension="json", save=True, analytic_priors=None, cls=None,
+    meta_data=None, **kwargs
+):
+    """Write a set of samples to a bilby file
+
+    Parameters
+    ----------
+    parameters: list
+        list of parameters
+    samples: 2d list
+        list of samples. Columns correspond to a given parameter
+    outdir: str, optional
+        directory to write the dat file
+    label: str, optional
+        The label of the analysis. This is used in the filename if a filename
+        if not specified
+    filename: str, optional
+        The name of the file that you wish to write
+    overwrite: Bool, optional
+        If True, an existing file of the same name will be overwritten
+    extension: str, optional
+        file extension for the bilby result file. Default json.
+    save: Bool, optional
+        if True, save the bilby object to file
+    """
+    bilby_object = to_bilby(
+        parameters, samples, label=None, analytic_priors=None, cls=None,
+        meta_data=None, **kwargs
+    )
     if save:
         _filename = os.path.join(outdir, filename)
         bilby_object.save_to_file(filename=_filename, extension=extension)
     else:
         return bilby_object
 
 
-def prior_samples_from_file(path, cls="PriorDict", nsamples=5000):
+def write_bilby(
+    parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
+    extension="json", save=True, analytic_priors=None, cls=None,
+    meta_data=None, labels=None, **kwargs
+):
+    """Write a set of samples to a bilby file
+
+    Parameters
+    ----------
+    parameters: list
+        list of parameters
+    samples: 2d list
+        list of samples. Columns correspond to a given parameter
+    outdir: str, optional
+        directory to write the dat file
+    label: str, optional
+        The label of the analysis. This is used in the filename if a filename
+        if not specified
+    filename: str, optional
+        The name of the file that you wish to write
+    overwrite: Bool, optional
+        If True, an existing file of the same name will be overwritten
+    extension: str, optional
+        file extension for the bilby result file. Default json.
+    save: Bool, optional
+        if True, save the bilby object to file
+    """
+    from pesummary.io.write import _multi_analysis_write
+
+    func = _write_bilby
+    if not save:
+        func = to_bilby
+    return _multi_analysis_write(
+        func, parameters, samples, outdir=outdir, label=label,
+        filename=filename, overwrite=overwrite, extension=extension,
+        save=save, analytic_priors=analytic_priors, cls=cls,
+        meta_data=meta_data, file_format="bilby", labels=labels,
+        _return=True, **kwargs
+    )
+
+
+def config_from_file(path):
+    """Extract the configuration file stored within a bilby result file
+
+    Parameters
+    ----------
+    path: str
+        path to the bilby result file you wish to load
+    """
+    bilby_object = _load_bilby(path)
+    return config_from_object(bilby_object)
+
+
+def config_from_object(bilby_object):
+    """Extract the configuration file stored within a `bilby.core.result.Result`
+    object (or alike)
+
+    Parameters
+    ----------
+    bilby_object: bilby.core.result.Result (or alike)
+        a bilby.core.result.Result object (or alike) you wish to extract the
+        configuration file from
+    """
+    config = {}
+    if bilby_object.meta_data is not None:
+        if "command_line_args" in bilby_object.meta_data.keys():
+            config = {
+                "config": bilby_object.meta_data["command_line_args"]
+            }
+    return config
+
+
+def prior_samples_from_file(path, cls="PriorDict", nsamples=5000, **kwargs):
     """Return a dict of prior samples from a `bilby` prior file
 
     Parameters
     ----------
     path: str
         path to a `bilby` prior file
     cls: str, optional
@@ -172,15 +292,15 @@
     if isinstance(cls, str):
         cls = getattr(prior, cls)
     _prior = cls(filename=path)
     samples = _prior.sample(size=nsamples)
     return _bilby_prior_dict_to_pesummary_samples_dict(samples, prior=_prior)
 
 
-def prior_samples_from_bilby_object(bilby_object, nsamples=5000):
+def prior_samples_from_bilby_object(bilby_object, nsamples=5000, **kwargs):
     """Return a dict of prior samples from a `bilby.core.result.Result`
     object
 
     Parameters
     ----------
     bilby_object: bilby.core.result.Result
         a bilby.core.result.Result object you wish to draw prior samples from
@@ -202,27 +322,29 @@
     _samples = SamplesDict(samples)
     if prior is not None:
         analytic = {key: str(item) for key, item in prior.items()}
         setattr(_samples, "analytic", analytic)
     return _samples
 
 
-class Bilby(Read):
+class Bilby(SingleAnalysisRead):
     """PESummary wrapper of `bilby` (https://git.ligo.org/lscsoft/bilby). The
     path_to_results_file argument will be passed directly to
     `bilby.core.result.read_in_result`. All functions therefore use `bilby`
     methods and requires `bilby` to be installed.
 
     Parameters
     ----------
     path_to_results_file: str
         path to the results file that you wish to read in with `bilby`.
     disable_prior: Bool, optional
         if True, do not collect prior samples from the `bilby` result file.
         Default False
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
 
     Attributes
     ----------
     parameters: list
         list of parameters stored in the result file
     samples: 2d list
         list of samples stored in the result file
@@ -233,34 +355,30 @@
     extra_kwargs: dict
         dictionary of kwargs that were extracted from the result file
     injection_parameters: dict
         dictionary of injection parameters extracted from the result file
     prior: dict
         dictionary of prior samples keyed by parameters. The prior functions
         are evaluated for 5000 samples.
+    pe_algorithm: str
+        name of the algorithm used to generate the posterior samples
 
     Methods
     -------
     to_dat:
         save the posterior samples to a .dat file
     to_latex_table:
         convert the posterior samples to a latex table
     generate_latex_macros:
         generate a set of latex macros for the stored posterior samples
     """
     def __init__(self, path_to_results_file, **kwargs):
         super(Bilby, self).__init__(path_to_results_file, **kwargs)
         self.load(self._grab_data_from_bilby_file, **kwargs)
 
-    @classmethod
-    def load_file(cls, path, **kwargs):
-        if not os.path.isfile(path):
-            raise Exception("%s does not exist" % (path))
-        return cls(path, **kwargs)
-
     @staticmethod
     def grab_priors(bilby_object, nsamples=5000):
         """Draw samples from the prior functions stored in the bilby file
         """
         try:
             return prior_samples_from_bilby_object(
                 bilby_object, nsamples=nsamples
@@ -275,15 +393,15 @@
         """
         f = bilby_object
         kwargs = {"sampler": {
             conf.log_evidence: np.round(f.log_evidence, 2),
             conf.log_evidence_error: np.round(f.log_evidence_err, 2),
             conf.log_bayes_factor: np.round(f.log_bayes_factor, 2),
             conf.log_noise_evidence: np.round(f.log_noise_evidence, 2)},
-            "meta_data": {}}
+            "meta_data": {}, "other": f.meta_data}
         return kwargs
 
     @staticmethod
     def _grab_data_from_bilby_file(path, **kwargs):
         """Load the results file using the `bilby` library
         """
         return read_bilby(path, **kwargs)
```

### Comparing `pesummary-0.9.1/pesummary/core/file/formats/hdf5.py` & `pesummary-1.0.0/pesummary/core/file/formats/hdf5.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,25 +1,15 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import h5py
 import numpy as np
 from pesummary.core.file.formats.base_read import Read
+from pesummary.utils.dict import load_recursively, paths_to_key
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 def read_hdf5(path, **kwargs):
     """Grab the parameters and samples in a .hdf5 file
 
     Parameters
     ----------
@@ -45,26 +35,26 @@
         parameters you wish to remove from the posterior table
     """
     import deepdish
 
     f = deepdish.io.load(path)
     if path_to_samples is None:
         try:
-            path_to_samples, = Read.paths_to_key("posterior", f)
+            path_to_samples, = paths_to_key("posterior", f)
             path_to_samples = path_to_samples[0]
         except ValueError:
             try:
-                path_to_samples, = Read.paths_to_key("posterior_samples", f)
+                path_to_samples, = paths_to_key("posterior_samples", f)
                 path_to_samples = path_to_samples[0]
             except ValueError:
                 raise ValueError(
                     "Unable to find a 'posterior' or 'posterior_samples' group "
                     "in the file '{}'".format(path)
                 )
-    reduced_f, = Read.load_recursively(path_to_samples, f)
+    reduced_f, = load_recursively(path_to_samples, f)
     parameters = [i for i in reduced_f.keys()]
     if remove_params is not None:
         for param in remove_params:
             if param in parameters:
                 parameters.remove(param)
     data = np.zeros([len(reduced_f[parameters[0]]), len(parameters)])
     for num, par in enumerate(parameters):
@@ -73,15 +63,18 @@
     data = data.tolist()
     for num, par in enumerate(parameters):
         if par == "logL":
             parameters[num] = "log_likelihood"
     return parameters, data
 
 
-def _read_hdf5_with_h5py(path, remove_params=None, path_to_samples=None):
+def _read_hdf5_with_h5py(
+    path, remove_params=None, path_to_samples=None,
+    return_posterior_dataset=False
+):
     """Grab the parameters and samples in a .hdf5 file with h5py
 
     Parameters
     ----------
     path: str
         path to the result file you wish to read in
     remove_params: list, optional
@@ -100,18 +93,21 @@
         if remove_params is not None:
             parameters = [
                 i for i in original_parameters if i not in remove_params
             ]
         else:
             parameters = copy.deepcopy(original_parameters)
         n_samples = len(f[path_to_samples][parameters[0]])
-        samples = [
-            [float(f[path_to_samples][original_parameters.index(i)][num])
-             for i in parameters] for num in range(n_samples)
-        ]
+        try:
+            samples = np.array([
+                f[path_to_samples][original_parameters.index(i)] for i in
+                parameters
+            ]).T
+        except (AttributeError, KeyError, TypeError):
+            samples = np.array([f[path_to_samples][i] for i in parameters]).T
         cond1 = "loglr" not in parameters or "log_likelihood" not in \
             parameters
         cond2 = "likelihood_stats" in f.keys() and "loglr" in \
             f["likelihood_stats"]
         if cond1 and cond2:
             parameters.append("log_likelihood")
             for num, i in enumerate(samples):
@@ -125,24 +121,25 @@
             parameters = [
                 i for i in original_parameters if i not in remove_params
             ]
         else:
             parameters = copy.deepcopy(original_parameters)
         samples = np.array(f[path_to_samples]["samples"])
     elif isinstance(f[path_to_samples], h5py._hl.dataset.Dataset):
-        parameters = f[path_to_samples].dtype.names
-        samples = [[float(i[parameters.index(j)]) for j in parameters] for
-                   i in f[path_to_samples]]
+        parameters = list(f[path_to_samples].dtype.names)
+        samples = np.array(f[path_to_samples]).view((float, len(parameters))).tolist()
+    if return_posterior_dataset:
+        return parameters, samples, f[path_to_samples]
     f.close()
     return parameters, samples
 
 
-def write_hdf5(
+def _write_hdf5(
     parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
-    **kwargs
+    dataset_name="posterior_samples", **kwargs
 ):
     """Write a set of samples to a hdf5 file
 
     Parameters
     ----------
     parameters: list
         list of parameters
@@ -153,20 +150,52 @@
     label: str, optional
         The label of the analysis. This is used in the filename if a filename
         if not specified
     filename: str, optional
         The name of the file that you wish to write
     overwrite: Bool, optional
         If True, an existing file of the same name will be overwritten
+    dataset_name: str, optional
+        name of the dataset to store a set of samples. Default posterior_samples
     """
     from pesummary.utils.samples_dict import SamplesDict
     from pesummary.utils.utils import check_filename
 
     default_filename = "pesummary_{}.h5"
     filename = check_filename(
         default_filename=default_filename, outdir=outdir, label=label, filename=filename,
         overwrite=overwrite
     )
     samples = SamplesDict(parameters, np.array(samples).T)
     _samples = samples.to_structured_array()
     with h5py.File(filename, "w") as f:
-        f.create_dataset("posterior_samples", data=_samples)
+        f.create_dataset(dataset_name, data=_samples)
+
+
+def write_hdf5(
+    parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
+    **kwargs
+):
+    """Write a set of samples to a hdf5 file
+
+    Parameters
+    ----------
+    parameters: list
+        list of parameters
+    samples: 2d list
+        list of samples. Columns correspond to a given parameter
+    outdir: str, optional
+        directory to write the dat file
+    label: str, optional
+        The label of the analysis. This is used in the filename if a filename
+        if not specified
+    filename: str, optional
+        The name of the file that you wish to write
+    overwrite: Bool, optional
+        If True, an existing file of the same name will be overwritten
+    """
+    from pesummary.io.write import _multi_analysis_write
+
+    _multi_analysis_write(
+        _write_hdf5, parameters, samples, outdir=outdir, label=label,
+        filename=filename, overwrite=overwrite, file_format="hdf5", **kwargs
+    )
```

### Comparing `pesummary-0.9.1/pesummary/core/file/formats/base_read.py` & `pesummary-1.0.0/pesummary/gw/file/formats/base_read.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,651 +1,681 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import numpy as np
-import h5py
-from pesummary.utils.samples_dict import SamplesDict, MCMCSamplesDict, Array
+from pesummary.gw.file.standard_names import standard_names
+from pesummary.core.file.formats.base_read import (
+    Read, SingleAnalysisRead, MultiAnalysisRead
+)
 from pesummary.utils.utils import logger
+from pesummary.utils.samples_dict import SamplesDict
+from pesummary.utils.decorators import open_config
+from pesummary.gw.conversions import convert
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
-class Read(object):
+
+def _translate_parameters(parameters, samples):
+    """Translate parameters to a standard names
+
+    Parameters
+    ----------
+    parameters: list
+        list of parameters used in the analysis
+    samples: list
+        list of samples for each parameters
+    """
+    path = ("https://git.ligo.org/lscsoft/pesummary/blob/master/pesummary/"
+            "gw/file/standard_names.py")
+    parameters_not_included = [
+        i for i in parameters if i not in standard_names.keys()
+    ]
+    if len(parameters_not_included) > 0:
+        logger.debug(
+            "PESummary does not have a 'standard name' for the following "
+            "parameters: {}. This means that comparison plots between "
+            "different codes may not show these parameters. If you want to "
+            "assign a standard name for these parameters, please add an MR "
+            "which edits the following file: {}. These parameters will be "
+            "added to the result pages and meta file as is.".format(
+                ", ".join(parameters_not_included), path
+            )
+        )
+    standard_params = [i for i in parameters if i in standard_names.keys()]
+    converted_params = [
+        standard_names[i] if i in standard_params else i for i in
+        parameters
+    ]
+    return converted_params, samples
+
+
+def _add_log_likelihood(parameters, samples):
+    """Add zero log_likelihood samples to the posterior table
+
+    Parameters
+    ----------
+    parameters: list
+        list of parameters stored in the table
+    samples: 2d list
+        list of samples for each parameter. Columns correspond to a given
+        parameter
+    """
+    if "log_likelihood" not in parameters:
+        parameters.append("log_likelihood")
+        samples = np.vstack(
+            [np.array(samples).T, np.zeros(len(samples))]
+        ).T
+    return parameters, samples
+
+
+class GWRead(Read):
     """Base class to read in a results file
 
     Parameters
     ----------
     path_to_results_file: str
         path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
 
     Attributes
     ----------
     parameters: list
         list of parameters stored in the result file
+    converted_parameters: list
+        list of parameters that have been derived from the sampled distributions
     samples: 2d list
         list of samples stored in the result file
     samples_dict: dict
         dictionary of samples stored in the result file keyed by parameters
     input_version: str
         version of the result file passed.
     extra_kwargs: dict
         dictionary of kwargs that were extracted from the result file
+    converted_parameters: list
+        list of parameters that have been added
 
     Methods
     -------
-    downsample:
-        downsample the posterior samples stored in the result file
     to_dat:
         save the posterior samples to a .dat file
     to_latex_table:
         convert the posterior samples to a latex table
     generate_latex_macros:
         generate a set of latex macros for the stored posterior samples
+    to_lalinference:
+        convert the posterior samples to a lalinference result file
+    generate_all_posterior_samples:
+        generate all posterior distributions that may be derived from
+        sampled distributions
     """
     def __init__(self, path_to_results_file, **kwargs):
-        self.path_to_results_file = path_to_results_file
-        self.mcmc_samples = False
-        self.extension = self.extension_from_path(self.path_to_results_file)
-
-    @staticmethod
-    def load_from_function(function, path_to_file, **kwargs):
-        """Load a file according to a given function
-
-        Parameters
-        ----------
-        function: func
-            callable function that will load in your file
-        path_to_file: str
-            path to the file that you wish to load
-        kwargs: dict
-            all kwargs are passed to the function
-        """
-        return function(path_to_file, **kwargs)
-
-    @staticmethod
-    def check_for_weights(parameters, samples):
-        """Check to see if the samples are weighted
+        super(GWRead, self).__init__(path_to_results_file, **kwargs)
 
-        Parameters
-        ----------
-        parameters: list
-            list of parameters stored in the result file
-        samples: np.ndarray
-            array of samples for each parameter
-        """
-        likely_names = ["weights", "weight"]
-        if any(i in parameters for i in likely_names):
-            ind = (
-                parameters.index("weights") if "weights" in parameters else
-                parameters.index("weight")
-            )
-            return Array(np.array(samples).T[ind])
+    @property
+    def calibration_spline_posterior(self):
         return None
 
-    def load(self, function, **kwargs):
+    Read.attrs.update({"approximant": "approximant"})
+
+    def load(self, function, _data=None, **kwargs):
         """Load a results file according to a given function
 
         Parameters
         ----------
         function: func
             callable function that will load in your results file
         """
-        self.data = self.load_from_function(
-            function, self.path_to_results_file, **kwargs)
-        self.parameters = self.data["parameters"]
-        self.samples = self.data["samples"]
-        if "mcmc_samples" in self.data.keys():
-            self.mcmc_samples = self.data["mcmc_samples"]
-        if "injection" in self.data.keys():
-            if isinstance(self.data["injection"], dict):
-                self.injection_parameters = {
-                    key.decode("utf-8") if isinstance(key, bytes) else key: val
-                    for key, val in self.data["injection"].items()
-                }
-            elif isinstance(self.data["injection"], list):
-                self.injection_parameters = [
-                    {
-                        key.decode("utf-8") if isinstance(key, bytes) else
-                        key: val for key, val in i.items()
-                    } for i in self.data["injection"]
-                ]
-            else:
-                self.injection_parameters = self.data["injection"]
-        if "version" in self.data.keys():
-            self.input_version = self.data["version"]
-        else:
-            self.input_version = "No version information found"
-        if "kwargs" in self.data.keys():
-            self.extra_kwargs = self.data["kwargs"]
-        else:
-            self.extra_kwargs = {"sampler": {}, "meta_data": {}}
-            self.extra_kwargs["sampler"]["nsamples"] = len(self.data["samples"])
-        if "prior" in self.data.keys():
-            self.priors = self.data["prior"]
-        if "analytic" in self.data.keys():
-            self.analytic = self.data["analytic"]
-        if "labels" in self.data.keys():
-            self.labels = self.data["labels"]
-        if "config" in self.data.keys():
-            self.config = self.data["config"]
-        if "weights" in self.data.keys():
-            self.weights = self.data["weights"]
-        else:
-            self.weights = self.check_for_weights(
-                self.data["parameters"], self.data["samples"]
+        data = _data
+        if _data is None:
+            data = self.load_from_function(
+                function, self.path_to_results_file, **kwargs
             )
-
-    @property
-    def samples_dict(self):
-        if self.mcmc_samples:
-            return MCMCSamplesDict(
-                self.parameters, [np.array(i).T for i in self.samples]
+        parameters, samples = self.translate_parameters(
+            data["parameters"], data["samples"]
+        )
+        _add_likelihood = kwargs.get("add_zero_likelihood", True)
+        if not self.check_for_log_likelihood(parameters) and _add_likelihood:
+            logger.warning(
+                "Failed to find 'log_likelihood' in result file. Setting "
+                "every sample to have log_likelihood 0"
             )
-        return SamplesDict(self.parameters, np.array(self.samples).T)
-
-    @staticmethod
-    def paths_to_key(key, dictionary, current_path=None):
-        """Return the path to a key stored in a nested dictionary
-
-        Parameters
-        ----------
-        key: str
-            the key that you would like to find
-        dictionary: dict
-            the nested dictionary that has the key stored somewhere within it
-        current_path: str, optional
-            the current level in the dictionary
-        """
-        if current_path is None:
-            current_path = []
-
-        for k, v in dictionary.items():
-            if k == key:
-                yield current_path + [key]
-            else:
-                if isinstance(v, dict):
-                    path = current_path + [k]
-                    for z in Read.paths_to_key(key, v, path):
-                        yield z
+            parameters, samples = self.add_log_likelihood(parameters, samples)
+        data.update(
+            {
+                "parameters": parameters, "samples": samples,
+                "injection": data["injection"]
+            }
+        )
+        super(GWRead, self).load(function, _data=data, **kwargs)
+        if self.injection_parameters is not None:
+            self.injection_parameters = self.convert_injection_parameters(
+                self.injection_parameters, extra_kwargs=self.extra_kwargs,
+                disable_convert=kwargs.get("disable_injection_conversion", False)
+            )
+        if self.priors is not None and len(self.priors):
+            if self.priors["samples"] != {}:
+                priors = self.priors["samples"]
+                self.priors["samples"] = self.convert_and_translate_prior_samples(
+                    priors, disable_convert=kwargs.get(
+                        "disable_prior_conversion", False
+                    )
+                )
 
-    @staticmethod
-    def convert_list_to_item(dictionary):
+    def convert_and_translate_prior_samples(self, priors, disable_convert=False):
         """
         """
-        for key, value in dictionary.items():
-            if isinstance(value, dict):
-                Read.convert_list_to_item(value)
-            else:
-                if isinstance(value, (list, np.ndarray, Array)):
-                    if len(value) == 1 and isinstance(value[0], bytes):
-                        dictionary.update({key: value[0].decode("utf-8")})
-                    elif len(value) == 1:
-                        dictionary.update({key: value[0]})
-        return dictionary
-
-    @staticmethod
-    def load_recursively(key, dictionary):
-        """Return the entry for a key of format 'a/b/c/d'
+        default_parameters = list(priors.keys())
+        default_samples = [
+            [priors[parameter][i] for parameter in default_parameters] for i
+            in range(len(priors[default_parameters[0]]))
+        ]
+        parameters, samples = self.translate_parameters(
+            default_parameters, default_samples
+        )
+        if not disable_convert:
+            return convert(
+                parameters, samples, extra_kwargs=self.extra_kwargs
+            )
+        return SamplesDict(parameters, samples)
+
+    def convert_injection_parameters(
+        self, data, extra_kwargs={"sampler": {}, "meta_data": {}},
+        disable_convert=False, sampled_parameters=None, **kwargs
+    ):
+        """Apply the conversion module to the injection data
 
         Parameters
         ----------
-        key: str
-            key of format 'a/b/c/d'
-        dictionary: dict
-            the dictionary that has the key stored
-        """
-        if "/" in key:
-            key = key.split("/")
-        if isinstance(key, (str, float)):
-            key = [key]
-        if key[-1] in dictionary.keys():
-            try:
-                converted_dictionary = Read.convert_list_to_item(
-                    dictionary[key[-1]]
-                )
-                yield converted_dictionary
-            except AttributeError:
-                yield dictionary[key[-1]]
+        data: dict
+            dictionary of injection data keyed by the parameter
+        extra_kwargs: dict, optional
+            optional kwargs to pass to the conversion module
+        disable_convert: Bool, optional
+            if True, do not convert injection parameters
+        """
+        import math
+        from pesummary.gw.file.injection import GWInjection
+        kwargs.update({"extra_kwargs": extra_kwargs})
+        _data = data.copy()
+        for key, item in data.items():
+            if math.isnan(np.atleast_1d(item)[0]):
+                _ = _data.pop(key)
+        if len(_data):
+            converted = GWInjection(
+                _data, conversion=not disable_convert, conversion_kwargs=kwargs
+            )
+            _param = list(converted.keys())[0]
+            _example = converted[_param]
+            if not len(_example.shape):
+                for key, item in converted.items():
+                    converted[key] = [item]
         else:
-            old, new = key[0], key[1:]
-            for z in Read.load_recursively(new, dictionary[old]):
-                yield z
+            converted = _data
+        for i in sampled_parameters:
+            if i not in list(converted.keys()):
+                converted[i] = float('nan')
+        return converted
 
-    @staticmethod
-    def edit_dictionary(dictionary, path, value):
-        """Replace an entry in a nested dictionary
+    def write(self, package="core", **kwargs):
+        """Save the data to file
 
         Parameters
         ----------
-        dictionary: dict
-            the nested dictionary that you would like to edit
-        path: list
-            the path to the key that you would like to edit
-        value:
-            the replacement
+        package: str, optional
+            package you wish to use when writing the data
+        kwargs: dict, optional
+            all additional kwargs are passed to the pesummary.io.write function
         """
-        from functools import reduce
-        from operator import getitem
+        return super(GWRead, self).write(package="gw", **kwargs)
 
-        edit = dictionary.copy()
-        reduce(getitem, path[:-1], edit)[path[-1]] = value
-        return edit
-
-    @staticmethod
-    def extension_from_path(path):
-        """Return the extension of the file from the file path
+    def _grab_injection_parameters_from_file(self, path, **kwargs):
+        """Extract data from an injection file
 
         Parameters
         ----------
         path: str
-            path to the results file
+            path to injection file
         """
-        extension = path.split(".")[-1]
-        return extension
+        from pesummary.gw.file.injection import GWInjection
+        return super(GWRead, self)._grab_injection_parameters_from_file(
+            path, cls=GWInjection, **kwargs
+        )
+
+    def interpolate_calibration_spline_posterior(self, **kwargs):
+        from pesummary.gw.file.calibration import Calibration
+        from pesummary.utils.utils import iterator
+        if self.calibration_spline_posterior is None:
+            return
+        total = []
+        log_frequencies, amplitudes, phases = self.calibration_spline_posterior
+        keys = list(log_frequencies.keys())
+        _iterator = iterator(
+            None, desc="Interpolating calibration posterior", logger=logger,
+            tqdm=True, total=len(self.samples) * 2 * len(keys)
+        )
+        with _iterator as pbar:
+            for key in keys:
+                total.append(
+                    Calibration.from_spline_posterior_samples(
+                        np.array(log_frequencies[key]),
+                        np.array(amplitudes[key]), np.array(phases[key]),
+                        pbar=pbar, **kwargs
+                    )
+                )
+        return total, log_frequencies.keys()
 
     @staticmethod
-    def guess_path_to_samples(path):
-        """Guess the path to the posterior samples stored in an hdf5 object
+    def translate_parameters(parameters, samples):
+        """Translate parameters to a standard names
 
         Parameters
         ----------
-        path: str
-            path to the results file
-        """
-        def _find_name(name, item):
-            c1 = "posterior_samples" in name or "posterior" in name
-            c2 = isinstance(item, (h5py._hl.dataset.Dataset, np.ndarray))
-            try:
-                c3 = isinstance(item, h5py._hl.group.Group) and isinstance(
-                    item[0], (float, int, np.number)
-                )
-            except AttributeError:
-                c3 = False
-            c4 = (
-                isinstance(item, h5py._hl.group.Group) and "parameter_names" in
-                item.keys() and "samples" in item.keys()
-            )
-            if c1 and c3:
-                paths.append(name)
-            elif c1 and c4:
-                return paths.append(name)
-            elif c1 and c2:
-                return paths.append(name)
-
-        f = h5py.File(path, 'r')
-        paths = []
-        f.visititems(_find_name)
-        f.close()
-        if len(paths) == 1:
-            return paths[0]
-        elif len(paths) > 1:
-            raise ValueError(
-                "Found multiple posterior sample tables in '{}': {}. Not sure "
-                "which to load.".format(
-                    path, ", ".join(paths)
-                )
-            )
-        else:
-            raise ValueError(
-                "Unable to find a posterior samples table in '{}'".format(path)
-            )
-
-    @staticmethod
-    def _grab_params_and_samples_from_json_file(path):
-        """Grab the parameters and samples stored in a .json file
+        parameters: list
+            list of parameters used in the analysis
+        samples: list
+            list of samples for each parameters
         """
-        import json
-
-        with open(path, "r") as f:
-            data = json.load(f)
-        try:
-            path, = Read.paths_to_key("posterior", data)
-            path = path[0]
-            path += "/posterior"
-        except Exception:
-            path, = Read.paths_to_key("posterior_samples", data)
-            path = path[0]
-            path += "/posterior_samples"
-        reduced_data, = Read.load_recursively(path, data)
-        if "content" in list(reduced_data.keys()):
-            reduced_data = reduced_data["content"]
-        parameters = list(reduced_data.keys())
-
-        samples = [[
-            reduced_data[j][i] if not isinstance(reduced_data[j][i], dict)
-            else reduced_data[j][i]["real"] for j in parameters] for i in
-            range(len(reduced_data[parameters[0]]))]
-        return parameters, samples
+        return _translate_parameters(parameters, samples)
 
     @staticmethod
-    def _grab_params_and_samples_from_dat_file(path):
-        """Grab the parameters and samples in a .dat file
-        """
-        dat_file = np.genfromtxt(path, names=True)
-        parameters = [i for i in dat_file.dtype.names]
-        samples = np.atleast_2d(dat_file.tolist())
-        return parameters, samples
+    def _check_definition_of_inclination(parameters):
+        """Check the definition of inclination given the other parameters
 
-    def generate_all_posterior_samples(self, **kwargs):
-        """Empty function
+        Parameters
+        ----------
+        parameters: list
+            list of parameters used in the study
         """
-        pass
+        theta_jn = False
+        spin_angles = ["tilt_1", "tilt_2", "a_1", "a_2"]
+        names = [
+            standard_names[i] for i in parameters if i in standard_names.keys()]
+        if all(i in names for i in spin_angles):
+            theta_jn = True
+        if theta_jn:
+            if "theta_jn" not in names and "inclination" in parameters:
+                logger.warning("Because the spin angles are in your list of "
+                               "parameters, the angle 'inclination' probably "
+                               "refers to 'theta_jn'. If this is a mistake, "
+                               "please change the definition of 'inclination' to "
+                               "'iota' in your results file")
+                index = parameters.index("inclination")
+                parameters[index] = "theta_jn"
+        else:
+            if "inclination" in parameters:
+                index = parameters.index("inclination")
+                parameters[index] = "iota"
+        return parameters
 
     def add_fixed_parameters_from_config_file(self, config_file):
         """Search the conifiguration file and add fixed parameters to the
         list of parameters and samples
 
         Parameters
         ----------
         config_file: str
             path to the configuration file
         """
-        pass
+        self._add_fixed_parameters_from_config_file(
+            config_file, self._add_fixed_parameters)
 
-    def _add_fixed_parameters_from_config_file(self, config_file, function):
-        """Search the conifiguration file and add fixed parameters to the
-        list of parameters and samples
+    @staticmethod
+    @open_config(index=2)
+    def _add_fixed_parameters(parameters, samples, config_file):
+        """Open a LALInference configuration file and add the fixed parameters
+        to the list of parameters and samples
 
         Parameters
         ----------
+        parameters: list
+            list of existing parameters
+        samples: list
+            list of existing samples
         config_file: str
             path to the configuration file
-        function: func
-            function you wish to use to extract the information from the
-            configuration file
         """
-        self.data[0], self.data[1] = function(self.parameters, self.samples, config_file)
+        from pesummary.gw.file.standard_names import standard_names
 
-    def _add_marginalized_parameters_from_config_file(self, config_file, function):
-        """Search the configuration file and add marginalized parameters to the
-        list of parameters and samples
+        config = config_file
+        if not config.error:
+            fixed_data = {}
+            if "engine" in config.sections():
+                fixed_data = {
+                    key.split("fix-")[1]: item for key, item in
+                    config.items("engine") if "fix" in key}
+            for i in fixed_data.keys():
+                fixed_parameter = i
+                fixed_value = fixed_data[i]
+                try:
+                    param = standard_names[fixed_parameter]
+                    if param in parameters:
+                        pass
+                    else:
+                        parameters.append(param)
+                        for num in range(len(samples)):
+                            samples[num].append(float(fixed_value))
+                except Exception:
+                    if fixed_parameter == "logdistance":
+                        if "luminosity_distance" not in parameters:
+                            parameters.append(standard_names["distance"])
+                            for num in range(len(samples)):
+                                samples[num].append(float(fixed_value))
+                    if fixed_parameter == "costheta_jn":
+                        if "theta_jn" not in parameters:
+                            parameters.append(standard_names["theta_jn"])
+                            for num in range(len(samples)):
+                                samples[num].append(float(fixed_value))
+            return parameters, samples
+        return parameters, samples
 
-        Parameters
-        ----------
-        config_file: str
-            path to the configuration file
-        function: func
-            function you wish to use to extract the information from the
-            configuration file
-        """
-        self.data[0], self.data[1] = function(self.parameters, self.samples, config_file)
 
-    def add_injection_parameters_from_file(self, injection_file, **kwargs):
-        """
-        """
-        self.injection_parameters = self._add_injection_parameters_from_file(
-            injection_file, self._grab_injection_parameters_from_file,
-            **kwargs
-        )
+class GWSingleAnalysisRead(GWRead, SingleAnalysisRead):
+    """Base class to read in a results file which contains a single analysis
 
-    def _grab_injection_parameters_from_file(self, path, **kwargs):
-        """
-        """
-        from pesummary.core.file.injection import Injection
+    Parameters
+    ----------
+    path_to_results_file: str
+        path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
 
-        data = Injection.read(path, **kwargs).samples_dict
-        for i in self.parameters:
-            if i not in data.keys():
-                data[i] = float("nan")
-        return data
+    Attributes
+    ----------
+    parameters: list
+        list of parameters stored in the result file
+    converted_parameters: list
+        list of parameters that have been derived from the sampled distributions
+    samples: 2d list
+        list of samples stored in the result file
+    samples_dict: dict
+        dictionary of samples stored in the result file keyed by parameters
+    input_version: str
+        version of the result file passed.
+    extra_kwargs: dict
+        dictionary of kwargs that were extracted from the result file
+    converted_parameters: list
+        list of parameters that have been added
+
+    Methods
+    -------
+    to_dat:
+        save the posterior samples to a .dat file
+    to_latex_table:
+        convert the posterior samples to a latex table
+    generate_latex_macros:
+        generate a set of latex macros for the stored posterior samples
+    to_lalinference:
+        convert the posterior samples to a lalinference result file
+    generate_all_posterior_samples:
+        generate all posterior distributions that may be derived from
+        sampled distributions
+    """
+    def __init__(self, *args, **kwargs):
+        super(GWSingleAnalysisRead, self).__init__(*args, **kwargs)
 
-    def _add_injection_parameters_from_file(self, injection_file, function, **kwargs):
-        """Add the injection parameters from file
+    def check_for_log_likelihood(self, parameters):
+        """Return True if 'log_likelihood' is in a list of sampled parameters
 
         Parameters
         ----------
-        injection_file: str
-            path to injection file
-        function: func
-            funcion you wish to use to extract the information from the
-            injection file
+        parameters: list
+            list of sampled parameters
         """
-        return function(injection_file, **kwargs)
+        if "log_likelihood" in parameters:
+            return True
+        return False
 
-    def write(self, package="core", **kwargs):
-        """Save the data to file
+    def add_log_likelihood(self, parameters, samples):
+        """Add log_likelihood samples to a posterior table
 
         Parameters
         ----------
-        package: str, optional
-            package you wish to use when writing the data
-        kwargs: dict, optional
-            all additional kwargs are passed to the pesummary.io.write function
+        parameters: list
+            list of parameters stored in the table
+        samples: 2d list
+            list of samples for each parameter. Columns correspond to a given
+            parameter
         """
-        from pesummary.io import write
+        return _add_log_likelihood(parameters, samples)
 
-        return write(
-            self.parameters, self.samples, package=package,
-            file_versions=self.input_version, file_kwargs=self.extra_kwargs,
-            **kwargs
-        )
+    def generate_all_posterior_samples(self, **kwargs):
+        """Generate all posterior samples via the conversion module
 
-    def downsample(self, number):
-        """Downsample the posterior samples stored in the result file
+        Parameters
+        ----------
+        **kwargs: dict
+            all kwargs passed to the conversion module
         """
-        from pesummary.utils.utils import resample_posterior_distribution
-
-        if number > self.samples_dict.number_of_samples:
-            raise ValueError(
-                "Failed to downsample the posterior samples to {} because "
-                "there are only {} samples stored in the file.".format(
-                    number, self.samples_dict.number_of_samples
-                )
-            )
-        _samples = np.array(resample_posterior_distribution(
-            np.array(self.samples).T, number
-        ))
-        self.samples = _samples.T.tolist()
-        self.extra_kwargs["sampler"]["nsamples"] = number
+        if "no_conversion" in kwargs.keys():
+            no_conversion = kwargs.pop("no_conversion")
+        else:
+            no_conversion = False
+        if not no_conversion:
+            from pesummary.gw.conversions import convert
+
+            data = convert(
+                self.parameters, self.samples, extra_kwargs=self.extra_kwargs,
+                return_dict=False, **kwargs
+            )
+            self.parameters = data[0]
+            self.converted_parameters = self.parameters.added
+            self.samples = data[1]
+            if kwargs.get("return_kwargs", False):
+                self.extra_kwargs = data[2]
+
+    def convert_injection_parameters(self, *args, **kwargs):
+        return super(GWSingleAnalysisRead, self).convert_injection_parameters(
+            *args, sampled_parameters=self.parameters, **kwargs
+        )
 
-    def to_dat(self, **kwargs):
-        """Save the PESummary results file object to a dat file
+    def to_lalinference(self, **kwargs):
+        """Save the PESummary results file object to a lalinference hdf5 file
 
         Parameters
         ----------
         kwargs: dict
-            all kwargs passed to the pesummary.core.file.formats.dat.write_dat
-            function
+            all kwargs are passed to the pesummary.io.write.write function
         """
-        return self.write(file_format="dat", **kwargs)
+        return self.write(file_format="lalinference", package="gw", **kwargs)
 
-    @staticmethod
-    def latex_table(samples, parameter_dict=None, labels=None):
-        """Return a latex table displaying the passed data.
 
-        Parameters
-        ----------
-        samples_dict: list
-            list of pesummary.utils.utils.SamplesDict objects
-        parameter_dict: dict, optional
-            dictionary of parameters that you wish to include in the latex
-            table. The keys are the name of the parameters and the items are
-            the descriptive text. If None, all parameters are included
-        """
-        table = (
-            "\\begin{table}[hptb]\n\\begin{ruledtabular}\n\\begin{tabular}"
-            "{l %s}\n" % ("c " * len(samples))
-        )
-        if labels:
-            table += (
-                " & " + " & ".join(labels)
-            )
-            table += "\\\ \n\\hline \\\ \n"
-        data = {i: i for i in samples[0].keys()}
-        if parameter_dict is not None:
-            import copy
-
-            data = copy.deepcopy(parameter_dict)
-            for param in parameter_dict.keys():
-                if not all(param in samples_dict.keys() for samples_dict in samples):
-                    logger.warning(
-                        "{} not in list of parameters. Not adding to "
-                        "table".format(param)
-                    )
-                    data.pop(param)
+class GWMultiAnalysisRead(GWRead, MultiAnalysisRead):
+    """Base class to read in a results file which contains multiple analyses
 
-        for param, desc in data.items():
-            table += "{}".format(desc)
-            for samples_dict in samples:
-                median = samples_dict[param].average(type="median")
-                confidence = samples_dict[param].confidence_interval()
-                table += (
-                    " & $%s^{+%s}_{-%s}$" % (
-                        np.round(median, 2),
-                        np.round(confidence[1] - median, 2),
-                        np.round(median - confidence[0], 2)
-                    )
-                )
-            table += "\\\ \n"
-        table += (
-            "\\end{tabular}\n\\end{ruledtabular}\n\\caption{}\n\\end{table}"
-        )
-        return table
+    Parameters
+    ----------
+    path_to_results_file: str
+        path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
+    """
+    def __init__(self, *args, **kwargs):
+        super(GWMultiAnalysisRead, self).__init__(*args, **kwargs)
 
-    @staticmethod
-    def latex_macros(
-        samples, parameter_dict=None, labels=None, rounding="smart"
-    ):
-        """Return a latex table displaying the passed data.
+    def load(self, *args, **kwargs):
+        super(GWMultiAnalysisRead, self).load(*args, **kwargs)
+        if "psd" in self.data.keys():
+            from pesummary.gw.file.psd import PSDDict
 
-        Parameters
-        ----------
-        samples_dict: list
-            list of pesummary.utils.utils.SamplesDict objects
-        parameter_dict: dict, optional
-            dictionary of parameters that you wish to generate macros for. The
-            keys are the name of the parameters and the items are the latex
-            macros name you wish to use. If None, all parameters are included.
-        rounding: int, optional
-            decimal place for rounding. Default uses the
-            `pesummary.utils.utils.smart_round` function to round according to
-            the uncertainty
-        """
-        macros = ""
-        data = {i: i for i in samples[0].keys()}
-        if parameter_dict is not None:
-            import copy
-
-            data = copy.deepcopy(parameter_dict)
-            for param in parameter_dict.keys():
-                if not all(param in samples_dict.keys() for samples_dict in samples):
-                    logger.warning(
-                        "{} not in list of parameters. Not generating "
-                        "macro".format(param)
-                    )
-                    data.pop(param)
-        for param, desc in data.items():
-            for num, samples_dict in enumerate(samples):
-                if labels:
-                    description = "{}{}".format(desc, labels[num])
-                else:
-                    description = desc
+            try:
+                self.psd = {
+                    label: PSDDict(
+                        {ifo: value for ifo, value in psd_data.items()}
+                    ) for label, psd_data in self.data["psd"].items()
+                }
+            except (KeyError, AttributeError):
+                self.psd = self.data["psd"]
+        if "calibration" in self.data.keys():
+            from pesummary.gw.file.calibration import Calibration
 
-                median = samples_dict[param].average(type="median")
-                confidence = samples_dict[param].confidence_interval()
-                if rounding == "smart":
-                    from pesummary.utils.utils import smart_round
-
-                    median, upper, low = smart_round([
-                        median, confidence[1] - median, median - confidence[0]
-                    ])
-                else:
-                    median = np.round(median, rounding)
-                    low = np.round(median - confidence[0], rounding)
-                    upper = np.round(confidence[1] - median, rounding)
-                macros += (
-                    "\\def\\%s{$%s_{-%s}^{+%s}$}\n" % (
-                        description, median, low, upper
-                    )
-                )
-                macros += (
-                    "\\def\\%smedian{$%s$}\n" % (description, median)
-                )
-                macros += (
-                    "\\def\\%supper{$%s$}\n" % (
-                        description, np.round(median + upper, 9)
-                    )
+            try:
+                self.calibration = {
+                    label: {
+                        ifo: Calibration(value) for ifo, value in
+                        calibration_data.items()
+                    } for label, calibration_data in
+                    self.data["calibration"].items()
+                }
+            except (KeyError, AttributeError):
+                self.calibration = self.data["calibration"]
+        if "prior" in self.data.keys() and "calibration" in self.data["prior"].keys():
+            from pesummary.gw.file.calibration import CalibrationDict
+
+            try:
+                self.priors["calibration"] = {
+                    label: CalibrationDict(calibration_data) for
+                    label, calibration_data in
+                    self.data["prior"]["calibration"].items()
+                }
+            except (KeyError, AttributeError):
+                pass
+        if "skymap" in self.data.keys():
+            from pesummary.gw.file.skymap import SkyMapDict, SkyMap
+
+            try:
+                self.skymap = SkyMapDict({
+                    label: SkyMap(skymap["data"], skymap["meta_data"])
+                    for label, skymap in self.data["skymap"].items()
+                })
+            except (KeyError, AttributeError):
+                self.skymap = self.data["skymap"]
+        if "gwdata" in self.data.keys():
+            try:
+                from pesummary.gw.file.strain import StrainDataDict, StrainData
+                mydict = {}
+                for IFO, value in self.data["gwdata"].items():
+                    channel = [ch for ch in value.keys() if "_attrs" not in ch][0]
+                    if "{}_attrs".format(channel) in value.keys():
+                        _attrs = value["{}_attrs".format(channel)]
+                    else:
+                        _attrs = {}
+                    mydict[IFO] = StrainData(value[channel], **_attrs)
+                self.gwdata = StrainDataDict(mydict)
+            except (KeyError, AttributeError):
+                pass
+
+    def convert_and_translate_prior_samples(self, priors, disable_convert=False):
+        """
+        """
+        from pesummary.utils.samples_dict import MultiAnalysisSamplesDict
+
+        mydict = {}
+        for num, label in enumerate(self.labels):
+            if label in priors.keys() and len(priors[label]):
+                default_parameters = list(priors[label].keys())
+                default_samples = np.array(
+                    [priors[label][_param] for _param in default_parameters]
+                ).T
+                parameters, samples = self.translate_parameters(
+                    [default_parameters], [default_samples]
                 )
-                macros += (
-                    "\\def\\%slower{$%s$}\n" % (
-                        description, np.round(median - low, 9)
+                if not disable_convert:
+                    mydict[label] = convert(
+                        parameters[0], samples[0], extra_kwargs=self.extra_kwargs[num]
                     )
-                )
-        return macros
+                else:
+                    mydict[label] = SamplesDict(parameters[0], samples[0])
+            else:
+                mydict[label] = {}
+        return MultiAnalysisSamplesDict(mydict)
+
+    def check_for_log_likelihood(self, parameters):
+        if all("log_likelihood" in p for p in parameters):
+            return True
+        return False
 
-    def to_latex_table(self, parameter_dict=None, save_to_file=None):
-        """Make a latex table displaying the data in the result file.
+    @staticmethod
+    def translate_parameters(parameters, samples):
+        """Translate parameters to a standard names
 
         Parameters
         ----------
-        parameter_dict: dict, optional
-            dictionary of parameters that you wish to include in the latex
-            table. The keys are the name of the parameters and the items are
-            the descriptive text. If None, all parameters are included
-        save_to_file: str, optional
-            name of the file you wish to save the latex table to. If None, print
-            to stdout
-        """
-        import os
-
-        if save_to_file is not None and os.path.isfile("{}".format(save_to_file)):
-            raise FileExistsError(
-                "The file {} already exists.".format(save_to_file)
-            )
-
-        table = self.latex_table([self.samples_dict], parameter_dict)
-        if save_to_file is None:
-            print(table)
-        elif os.path.isfile("{}".format(save_to_file)):
-            logger.warning(
-                "File {} already exists. Printing to stdout".format(save_to_file)
-            )
-            print(table)
+        parameters: list
+            list of parameters used in the analysis
+        samples: list
+            list of samples for each parameters
+        """
+        converted_params = []
+        for _parameters, _samples in zip(parameters, samples):
+            converted_params.append(
+                _translate_parameters(_parameters, _samples)[0]
+            )
+        return converted_params, samples
+
+    def add_log_likelihood(self, parameters, samples):
+        """
+        """
+        parameters_logl, samples_logl = [], []
+        for _parameters, _samples in zip(parameters, samples):
+            pp, ss = _add_log_likelihood(_parameters, _samples)
+            parameters_logl.append(pp)
+            samples_logl.append(ss)
+        return parameters_logl, samples_logl
+
+    def generate_all_posterior_samples(self, labels=None, **conversion_kwargs):
+        if "no_conversion" in conversion_kwargs.keys():
+            no_conversion = conversion_kwargs.pop("no_conversion")
         else:
-            with open(save_to_file, "w") as f:
-                f.writelines([table])
+            no_conversion = False
+        if no_conversion:
+            return
+        from pesummary.gw.conversions import convert
+
+        converted_params, converted_samples, converted_kwargs = [], [], []
+        _converted_params = []
+        for label, param, samples, kwargs in zip(
+                self.labels, self.parameters, self.samples, self.extra_kwargs
+        ):
+            if labels is not None and label not in labels:
+                converted_params.append(param)
+                _converted_params.append([])
+                converted_samples.append(samples)
+                if kwargs.get("return_kwargs", False):
+                    converted_kwargs.append(kwargs)
+                continue
+            if label in conversion_kwargs.keys():
+                _conversion_kwargs = conversion_kwargs[label]
+            else:
+                _conversion_kwargs = conversion_kwargs
+            if _conversion_kwargs.get("evolve_spins", False):
+                if not _conversion_kwargs.get("return_kwargs", False):
+                    _conversion_kwargs["return_kwargs"] = True
+            data = convert(
+                param, samples, extra_kwargs=kwargs, return_dict=False,
+                **_conversion_kwargs
+            )
+            converted_params.append(data[0])
+            _converted_params.append(data[0].added)
+            converted_samples.append(data[1])
+            if kwargs.get("return_kwargs", False):
+                converted_kwargs.append(data[2])
+        self.parameters = converted_params
+        self.converted_parameters = _converted_params
+        self.samples = converted_samples
+        if converted_kwargs != []:
+            self.extra_kwargs = {
+                label: converted_kwargs[num] for num, label in enumerate(
+                    self.labels
+                )
+            }
 
-    def generate_latex_macros(
-        self, parameter_dict=None, save_to_file=None, rounding="smart"
+    def convert_injection_parameters(
+        self, data, extra_kwargs={"sampler": {}, "meta_data": {}},
+        disable_convert=False
     ):
-        """Generate a list of latex macros for each parameter in the result
-        file
-
-        Parameters
-        ----------
-        labels: list, optional
-            list of labels that you want to include in the table
-        parameter_dict: dict, optional
-            dictionary of parameters that you wish to generate macros for. The
-            keys are the name of the parameters and the items are the latex
-            macros name you wish to use. If None, all parameters are included.
-        save_to_file: str, optional
-            name of the file you wish to save the latex table to. If None, print
-            to stdout
-        rounding: int, optional
-            number of decimal points to round the latex macros
-        """
-        import os
-
-        if save_to_file is not None and os.path.isfile("{}".format(save_to_file)):
-            raise FileExistsError(
-                "The file {} already exists.".format(save_to_file)
+        """Apply the conversion module to the injection data
+        """
+        for num, label in enumerate(self.labels):
+            _identifier = label
+            if isinstance(data, dict):
+                _data = data[label]
+            else:
+                _data = data[num]
+                _identifier = num
+            data[_identifier] = super(
+                GWMultiAnalysisRead, self
+            ).convert_injection_parameters(
+                _data, extra_kwargs=extra_kwargs[num],
+                disable_convert=disable_convert,
+                sampled_parameters=self.parameters[num]
             )
-
-        macros = self.latex_macros(
-            [self.samples_dict], parameter_dict, rounding=rounding
-        )
-        if save_to_file is None:
-            print(macros)
-        else:
-            with open(save_to_file, "w") as f:
-                f.writelines([macros])
+        return data
```

### Comparing `pesummary-0.9.1/pesummary/core/file/formats/json.py` & `pesummary-1.0.0/pesummary/core/file/formats/json.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,25 +1,15 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import json
 import numpy as np
 import inspect
+from pesummary.utils.dict import load_recursively, paths_to_key
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 class PESummaryJsonEncoder(json.JSONEncoder):
     """Personalised JSON encoder for PESummary
     """
     def default(self, obj):
         """Return a json serializable object for 'obj'
@@ -33,71 +23,100 @@
             return obj.tolist()
         if inspect.isfunction(obj):
             return str(obj)
         if isinstance(obj, np.integer):
             return int(obj)
         elif isinstance(obj, np.floating):
             return float(obj)
-        elif isinstance(obj, (np.bool, np.bool_, bool)):
+        elif isinstance(obj, (bool, np.bool_)):
             return str(obj)
         elif isinstance(obj, bytes):
             return str(obj)
         elif isinstance(obj, type):
             return str(obj)
         return json.JSONEncoder.default(self, obj)
 
 
-def read_json(path, path_to_samples=None):
+def PESummaryJsonDecoder(obj):
+    if isinstance(obj, dict):
+        if "__array__" in obj.keys() and "content" in obj.keys():
+            return obj["content"]
+        elif "__complex__" in obj.keys():
+            return obj["real"] + obj["imag"] * 1j
+    return obj
+
+
+def read_json(path, path_to_samples=None, decoder=PESummaryJsonDecoder):
     """Grab the parameters and samples in a .json file
 
     Parameters
     ----------
     path: str
         path to the result file you wish to read in
     """
     import json
-    from pesummary.core.file.formats.base_read import Read
 
     with open(path, "r") as f:
-        data = json.load(f)
+        data = json.load(f, object_hook=decoder)
     if not path_to_samples:
         try:
-            path_to_samples, = Read.paths_to_key("posterior", data)
+            path_to_samples, = paths_to_key("posterior", data)
             path_to_samples = path_to_samples[0]
             path_to_samples += "/posterior"
         except ValueError:
             try:
-                path_to_samples, = Read.paths_to_key("posterior_samples", data)
+                path_to_samples, = paths_to_key("posterior_samples", data)
                 path_to_samples = path_to_samples[0]
                 path_to_samples += "/posterior_samples"
             except ValueError:
                 raise ValueError(
                     "Unable to find a 'posterior' or 'posterior_samples' group "
                     "in the file '{}'".format(path_to_samples)
                 )
-    reduced_data, = Read.load_recursively(path_to_samples, data)
+    reduced_data, = load_recursively(path_to_samples, data)
     if "content" in list(reduced_data.keys()):
         reduced_data = reduced_data["content"]
     parameters = list(reduced_data.keys())
     reduced_data = {
         j: list([reduced_data[j]]) if not isinstance(reduced_data[j], list) else
         reduced_data[j] for j in parameters
     }
+    _original_parameters = reduced_data.copy().keys()
+    _non_numeric = []
+    numeric_types = (float, int, np.number)
+    for key in _original_parameters:
+        if any(np.iscomplex(reduced_data[key])):
+            reduced_data[key + "_amp"] = np.abs(reduced_data[key])
+            reduced_data[key + "_angle"] = np.angle(reduced_data[key])
+            reduced_data[key] = np.real(reduced_data[key])
+        elif not all(isinstance(_, numeric_types) for _ in reduced_data[key]):
+            _non_numeric.append(key)
+        elif all(isinstance(_, (bool, np.bool_)) for _ in reduced_data[key]):
+            _non_numeric.append(key)
+
+    parameters = list(reduced_data.keys())
+    if len(_non_numeric):
+        from pesummary.utils.utils import logger
+        logger.info(
+            "Removing the parameters: '{}' from the posterior table as they "
+            "are non-numeric".format(", ".join(_non_numeric))
+        )
+    for key in _non_numeric:
+        parameters.remove(key)
     samples = [
-        [
-            reduced_data[j][i] if not isinstance(reduced_data[j][i], dict)
-            else reduced_data[j][i]["real"] for j in parameters
-        ] for i in range(len(reduced_data[parameters[0]]))
+        [reduced_data[j][i] for j in parameters] for i in
+        range(len(reduced_data[parameters[0]]))
     ]
     return parameters, samples
 
 
-def write_json(
+def _write_json(
     parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
-    indent=4, sort_keys=True, cls=PESummaryJsonEncoder, **kwargs
+    indent=4, sort_keys=True, dataset_name="posterior_samples",
+    cls=PESummaryJsonEncoder, **kwargs
 ):
     """Write a set of samples to a json file
 
     Parameters
     ----------
     parameters: list
         list of parameters
@@ -112,25 +131,64 @@
         The name of the file that you wish to write
     overwrite: Bool, optional
         If True, an existing file of the same name will be overwritten
     indent: int, optional
         The indentation to use in json.dump. Default 4
     sort_keys: Bool, optional
         Whether or not to sort the keys in json.dump. Default True
+    dataset_name: str, optional
+        name of the dataset to store a set of samples. Default posterior_samples
     cls: class, optional
         Class to use as the JsonEncoder. Default PESumamryJsonEncoder
     """
     from pesummary.utils.utils import check_filename
 
     default_filename = "pesummary_{}.json"
     filename = check_filename(
         default_filename=default_filename, outdir=outdir, label=label, filename=filename,
         overwrite=overwrite
     )
     _samples = np.array(samples).T
     data = {
-        "posterior_samples": {
+        dataset_name: {
             param: _samples[num] for num, param in enumerate(parameters)
         }
     }
     with open(filename, "w") as f:
         json.dump(data, f, indent=indent, sort_keys=sort_keys, cls=cls)
+
+
+def write_json(
+    parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
+    indent=4, sort_keys=True, cls=PESummaryJsonEncoder, **kwargs
+):
+    """Write a set of samples to a json file
+
+    Parameters
+    ----------
+    parameters: list
+        list of parameters
+    samples: 2d list
+        list of samples. Columns correspond to a given parameter
+    outdir: str, optional
+        directory to write the dat file
+    label: str, optional
+        The label of the analysis. This is used in the filename if a filename
+        if not specified
+    filename: str, optional
+        The name of the file that you wish to write
+    overwrite: Bool, optional
+        If True, an existing file of the same name will be overwritten
+    indent: int, optional
+        The indentation to use in json.dump. Default 4
+    sort_keys: Bool, optional
+        Whether or not to sort the keys in json.dump. Default True
+    cls: class, optional
+        Class to use as the JsonEncoder. Default PESumamryJsonEncoder
+    """
+    from pesummary.io.write import _multi_analysis_write
+
+    _multi_analysis_write(
+        _write_json, parameters, samples, outdir=outdir, label=label,
+        filename=filename, overwrite=overwrite, indent=indent,
+        sort_keys=sort_keys, cls=cls, file_format="json", **kwargs
+    )
```

### Comparing `pesummary-0.9.1/pesummary/core/file/formats/pesummary.py` & `pesummary-1.0.0/pesummary/core/file/formats/pesummary.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,44 +1,23 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from glob import glob
 import os
-import copy
-
-import math
 import h5py
 import json
 import numpy as np
-import configparser
-import warnings
 
-from pesummary.core.file.formats.base_read import Read
+from pesummary.core.file.formats.base_read import MultiAnalysisRead
 from pesummary.utils.samples_dict import (
     MCMCSamplesDict, MultiAnalysisSamplesDict, SamplesDict, Array
 )
-from pesummary.utils.utils import logger
-
+from pesummary.utils.dict import load_recursively
+from pesummary.utils.decorators import deprecation
 
-deprecation_warning = (
-    "This file format is out-of-date and may not be supported in future "
-    "releases."
-)
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 def write_pesummary(
     *args, cls=None, outdir="./", label=None, config=None, injection_data=None,
     file_kwargs=None, file_versions=None, mcmc_samples=False, hdf5=True, **kwargs
 ):
     """Write a set of samples to a pesummary file
@@ -67,30 +46,29 @@
     mcmc_samples: Bool, optional
         if True, the set of samples provided are from multiple MCMC chains
     hdf5: Bool, optional
         if True, save the pesummary file in hdf5 format
     kwargs: dict
         all other kwargs are passed to the pesummary.core.file.meta_file._MetaFile class
     """
-    from pesummary.utils.utils import _default_filename
     from pesummary.core.file.meta_file import _MetaFile
 
     if cls is None:
         cls = _MetaFile
 
     default_label = "dataset"
     if label is None:
         labels = [default_label]
     elif not isinstance(label, str):
         raise ValueError("label must be a string")
     else:
         labels = [label]
 
     if isinstance(args[0], MultiAnalysisSamplesDict):
-        labels = args[0].keys()
+        labels = list(args[0].keys())
         samples = args[0]
     elif isinstance(args[0], (SamplesDict, MCMCSamplesDict)):
         _samples = args[0]
         if isinstance(args[0], SamplesDict):
             mcmc_samples = False
         else:
             mcmc_samples = True
@@ -112,15 +90,17 @@
 
         try:
             samples = {labels[0]: _samples}
         except NameError:
             pass
 
     if file_kwargs is None:
-        file_kwargs = {label: {} for label in labels}
+        file_kwargs = {
+            label: {"sampler": {}, "meta_data": {}} for label in labels
+        }
     elif not all(label in file_kwargs.keys() for label in labels):
         file_kwargs = {label: file_kwargs for label in labels}
 
     if file_versions is None or isinstance(file_versions, str):
         file_versions = {label: "No version information found" for label in labels}
     elif not all(label in file_versions.keys() for label in labels):
         file_versions = {label: file_versions for label in labels}
@@ -148,21 +128,23 @@
     else:
         obj.save_to_hdf5(
             obj.data, obj.labels, obj.samples, obj.meta_file,
             mcmc_samples=mcmc_samples
         )
 
 
-class PESummary(Read):
+class PESummary(MultiAnalysisRead):
     """This class handles the existing posterior_samples.h5 file
 
     Parameters
     ----------
     path_to_results_file: str
         path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
 
     Attributes
     ----------
     parameters: nd list
         list of parameters stored in the result file for each analysis stored
         in the result file
     samples: 3d list
@@ -185,17 +167,25 @@
         dictionary of prior samples stored in the result file
     config: dict
         dictionary containing the configuration file stored in the result file
     labels: list
         list of analyses stored in the result file
     weights: dict
         dictionary of weights for each sample for each analysis
+    pe_algorithm: dict
+        name of the algorithm used to generate the each analysis
+    preferred: str
+        name of the preferred analysis in the result file
 
     Methods
     -------
+    samples_dict_for_label: dict
+        dictionary of samples for a specific analysis
+    reduced_samples_dict: dict
+        dictionary of samples for one or more analyses
     to_dat:
         save the posterior samples to a .dat file
     to_bilby:
         convert the posterior samples to a bilby.core.result.Result object
     to_latex_table:
         convert the posterior samples to a latex table
     generate_latex_macros:
@@ -207,44 +197,69 @@
         super(PESummary, self).__init__(path_to_results_file, **kwargs)
         self.load(self._grab_data_from_pesummary_file, **self.load_kwargs)
 
     @property
     def load_kwargs(self):
         return dict()
 
+    @property
+    def pe_algorithm(self):
+        _algorithm = {label: None for label in self.labels}
+        for num, _kwargs in enumerate(self.extra_kwargs):
+            _label = self.labels[num]
+            try:
+                _algorithm[_label] = _kwargs["sampler"]["pe_algorithm"]
+            except KeyError:
+                pass
+        return _algorithm
+
+    @property
+    def preferred(self):
+        _preferred = None
+        for num, _kwargs in enumerate(self.extra_kwargs):
+            if "other" in _kwargs.keys() and "preferred" in _kwargs["other"].keys():
+                import ast
+                if ast.literal_eval(_kwargs["other"]["preferred"]):
+                    _preferred = self.labels[num]
+                    break
+        if _preferred is None and len(self.labels) == 1:
+            _preferred = self.labels[0]
+        return _preferred
+
     @classmethod
     def load_file(cls, path, **kwargs):
         if os.path.isdir(path):
             files = glob(path + "/*")
             if "home.html" in files:
                 path = glob(path + "/samples/posterior_samples*")[0]
             else:
                 raise FileNotFoundError(
                     "Unable to find a file called 'posterior_samples' in "
                     "the directory %s" % (path + "/samples"))
-        if not os.path.isfile(path):
-            raise FileNotFoundError("%s does not exist" % path)
-        return cls(path, **kwargs)
+        return super(PESummary, cls).load_file(path, **kwargs)
 
     @staticmethod
     def _grab_data_from_pesummary_file(path, **kwargs):
         """
         """
         func_map = {"h5": PESummary._grab_data_from_hdf5_file,
                     "hdf5": PESummary._grab_data_from_hdf5_file,
                     "json": PESummary._grab_data_from_json_file}
-        return func_map[Read.extension_from_path(path)](path, **kwargs)
+        return func_map[MultiAnalysisRead.extension_from_path(path)](path, **kwargs)
 
     @staticmethod
     def _convert_hdf5_to_dict(dictionary, path="/"):
         """
         """
         mydict = {}
         for key, item in dictionary[path].items():
             if isinstance(item, h5py._hl.dataset.Dataset):
+                _attrs = dict(item.attrs)
+                if len(_attrs):
+                    mydict["{}_attrs".format(key)] = _attrs
                 mydict[key] = np.array(item)
             elif isinstance(item, h5py._hl.group.Group):
                 mydict[key] = PESummary._convert_hdf5_to_dict(
                     dictionary, path=path + key + "/")
         return mydict
 
     @staticmethod
@@ -264,31 +279,39 @@
         function = kwargs.get(
             "grab_data_from_dictionary", PESummary._grab_data_from_dictionary)
         with open(path) as f:
             data = json.load(f)
         return function(data)
 
     @staticmethod
-    def _grab_data_from_dictionary(dictionary):
+    def _grab_data_from_dictionary(dictionary, ignore=[]):
         """
         """
         labels = list(dictionary.keys())
         if "version" in labels:
             labels.remove("version")
+
+        history_dict = None
         if "history" in labels:
+            history_dict = dictionary["history"]
             labels.remove("history")
 
+        if len(ignore):
+            for _ignore in ignore:
+                if _ignore in labels:
+                    labels.remove(_ignore)
+
         parameter_list, sample_list, inj_list, ver_list = [], [], [], []
         meta_data_list, weights_list = [], []
-        prior_dict, config_dict = {}, {}
+        description_dict, prior_dict, config_dict = {}, {}, {}
         mcmc_samples = False
         for num, label in enumerate(labels):
             if label == "version" or label == "history":
                 continue
-            data, = Read.load_recursively(label, dictionary)
+            data, = load_recursively(label, dictionary)
             if "mcmc_chains" in data.keys():
                 mcmc_samples = True
                 dataset = data["mcmc_chains"]
                 chains = list(dataset.keys())
                 parameters = [j for j in dataset[chains[0]].dtype.names]
                 samples = [
                     [np.array(j.tolist()) for j in dataset[chain]] for chain
@@ -331,21 +354,30 @@
                         elif _value.lower() == "none":
                             _value = None
                     return _value
                 inj_list.append({
                     parameter: parse_injection_value(value)
                     for parameter, value in zip(parameters, inj)
                 })
+            else:
+                inj_list.append({
+                    parameter: np.nan for parameter in parameters
+                })
             sample_list.append(samples)
             config = None
             if "config_file" in data.keys():
                 config = data["config_file"]
             config_dict[label] = config
             if "meta_data" in data.keys():
-                meta_data_list.append(data["meta_data"])
+                _meta_data = data["meta_data"]
+                if "sampler" not in _meta_data.keys():
+                    _meta_data["sampler"] = {"nsamples": len(samples)}
+                if "meta_data" not in _meta_data.keys():
+                    _meta_data["meta_data"] = {}
+                meta_data_list.append(_meta_data)
             else:
                 meta_data_list.append({"sampler": {}, "meta_data": {}})
             if "weights" in parameters or b"weights" in parameters:
                 ind = (
                     parameters.index("weights") if "weights" in parameters
                     else parameters.index(b"weights")
                 )
@@ -353,14 +385,19 @@
             else:
                 weights_list.append(None)
             if "version" in data.keys():
                 version = data["version"]
             else:
                 version = "No version information found"
             ver_list.append(version)
+            if "description" in data.keys():
+                description = data["description"]
+            else:
+                description = "No description found"
+            description_dict[label] = description
             if "priors" in data.keys():
                 priors = data["priors"]
             else:
                 priors = dict()
             prior_dict[label] = priors
         reversed_prior_dict = {}
         for label in labels:
@@ -375,93 +412,49 @@
             "injection": inj_list,
             "version": ver_list,
             "kwargs": meta_data_list,
             "weights": {i: j for i, j in zip(labels, weights_list)},
             "labels": labels,
             "config": config_dict,
             "prior": reversed_prior_dict,
-            "mcmc_samples": mcmc_samples
+            "mcmc_samples": mcmc_samples,
+            "history": history_dict,
+            "description": description_dict
         }
 
     @property
-    def samples_dict(self):
-        if self.mcmc_samples:
-            outdict = MCMCSamplesDict(
-                self.parameters[0], [np.array(i).T for i in self.samples[0]]
-            )
-        else:
-            if all("log_likelihood" in i for i in self.parameters):
-                likelihood_inds = [self.parameters[idx].index("log_likelihood")
-                                   for idx in range(len(self.labels))]
-                likelihoods = [[i[likelihood_inds[idx]] for i in self.samples[idx]]
-                               for idx, label in enumerate(self.labels)]
-            else:
-                likelihoods = [None] * len(self.labels)
-            outdict = MultiAnalysisSamplesDict({
-                label:
-                    SamplesDict(
-                        self.parameters[idx], np.array(self.samples[idx]).T
-                    ) for idx, label in enumerate(self.labels)
-            })
-        return outdict
-
-    @property
     def injection_dict(self):
         return {
             label: self.injection_parameters[num] for num, label in
             enumerate(self.labels)
         }
 
-    @staticmethod
-    def save_config_dictionary_to_file(config_dict, filename, outdir="./"):
-        """Save a dictionary containing the configuration settings to a file
-
-        Parameters
-        ----------
-        config_dict: dict
-            dictionary containing the configuration settings
-        filename: str
-            the name of the file you wish to write to
-        outdir: str, optional
-            path indicating where you would like to configuration file to be
-            saved. Default is current working directory
-        """
-        config = configparser.ConfigParser()
-        config.optionxform = str
-        if config_dict is None:
-            logger.warning("No config data found. Unable to write to file")
-            return None
-
-        for key in config_dict.keys():
-            config[key] = config_dict[key]
-
-        _filename = "%s/%s" % (outdir, filename)
-        with open(_filename, "w") as configfile:
-            config.write(configfile)
-        return _filename
-
-    def write_config_to_file(self, label, outdir="./"):
+    @deprecation(
+        "The 'write_config_to_file' method may not be supported in future "
+        "releases. Please use the 'write' method with kwarg 'file_format='ini''"
+    )
+    def write_config_to_file(self, label, outdir="./", filename=None, **kwargs):
         """Write the config file stored as a dictionary to file
 
         Parameters
         ----------
         label: str
             the label for the dictionary that you would like to write to file
         outdir: str, optional
             path indicating where you would like to configuration file to be
             saved. Default is current working directory
-        """
-        if label not in list(self.config.keys()):
-            raise ValueError("The label %s does not exist." % label)
-
-        _filename = self.save_config_dictionary_to_file(
-            self.config[label], outdir=outdir,
-            filename="%s_config.ini" % (label)
+        filename: str, optional
+            name of the file you wish to write the config data to. Default
+            '{label}_config.ini'
+        """
+        PESummary.write(
+            self, _config=True, labels=[label], outdir=outdir, overwrite=True,
+            filenames={label: filename}, **kwargs
         )
-        return _filename
+        return filename
 
     def _labels_for_write(self, labels):
         """Check the input labels and raise an exception if the label does not exist
         in the file
 
         Parameters
         ----------
@@ -477,15 +470,15 @@
                         "The label {} is not present in the file".format(label)
                     )
         return labels
 
     @staticmethod
     def write(
         self, package="core", labels="all", cls_properties=None, filenames=None,
-        _return=False, **kwargs
+        _return=False, _config=False, **kwargs
     ):
         """Save the data to file
 
         Parameters
         ----------
         package: str, optional
             package you wish to use when writing the data
@@ -521,36 +514,46 @@
                         except (KeyError, TypeError):
                             kwargs[prop] = None
             priors = getattr(self, "priors", {label: None})
             if "analytic" in priors.keys() and label in priors["analytic"].keys():
                 kwargs.update({"analytic_priors": priors["analytic"][label]})
             if not len(priors):
                 priors = {}
+            elif label in priors.keys() and priors[label] is None:
+                priors = None
             elif all(label in value.keys() for value in priors.values()):
                 priors = {key: item[label] for key, item in priors.items()}
             elif "samples" in priors.keys() and label in priors["samples"].keys():
                 priors = {"samples": {label: priors["samples"][label]}}
             elif label not in priors.keys():
                 priors = {}
             else:
                 priors = priors[label]
             if filenames is None:
                 filename = None
             elif isinstance(filenames, dict):
                 filename = filenames[label]
             else:
                 filename = filenames
-            _files[label] = write(
-                self.parameters[ind], self.samples[ind], package=package,
-                file_versions=self.input_version[ind], label=label,
-                file_kwargs=self.extra_kwargs[ind], priors=priors,
-                config=getattr(self, "config", {label: None})[label],
-                injection_data=getattr(self, "injection_dict", {label: None}),
-                filename=filename, **kwargs
-            )
+
+            if _config or kwargs.get("file_format", "dat") == "ini":
+                kwargs["file_format"] = "ini"
+                _files[label] = write(
+                    getattr(self, "config", {label: None})[label],
+                    filename=filename, **kwargs
+                )
+            else:
+                _files[label] = write(
+                    self.parameters[ind], self.samples[ind], package=package,
+                    file_versions=self.input_version[ind], label=label,
+                    file_kwargs=self.extra_kwargs[ind], priors=priors,
+                    config=getattr(self, "config", {label: None})[label],
+                    injection_data=getattr(self, "injection_dict", {label: None}),
+                    filename=filename, **kwargs
+                )
         if _return:
             return _files
 
     def to_bilby(self, labels="all", **kwargs):
         """Convert a PESummary metafile to a bilby results object
 
         Parameters
@@ -561,150 +564,37 @@
             all additional kwargs are passed to the pesummary.io.write function
         """
         return PESummary.write(
             self, labels=labels, package="core", file_format="bilby",
             _return=True, **kwargs
         )
 
-    def downsample(self, number):
-        """Downsample the posterior samples stored in the result file
-        """
-        from pesummary.utils.utils import resample_posterior_distribution
-
-        _samples = []
-        for num, samp in enumerate(self.samples):
-            label = self.labels[num]
-            if number > self.samples_dict[label].number_of_samples:
-                raise ValueError(
-                    "Failed to downsample the posterior samples for {} because "
-                    "there are only {} samples stored in the file.".format(
-                        label, self.samples_dict[label].number_of_samples
-                    )
-                )
-            _samples.append(np.array(resample_posterior_distribution(
-                np.array(samp).T, number
-            )).T.tolist())
-            self.extra_kwargs[num]["sampler"]["nsamples"] = number
-        self.samples = _samples
-
     def to_dat(self, labels="all", **kwargs):
         """Convert the samples stored in a PESummary metafile to a .dat file
 
         Parameters
         ----------
         labels: list, optional
             optional list of analyses to save to file
         kwargs: dict, optional
             all additional kwargs are passed to the pesummary.io.write function
         """
         return PESummary.write(
             self, labels=labels, package="core", file_format="dat", **kwargs
         )
 
-    def to_latex_table(self, labels="all", parameter_dict=None, save_to_file=None):
-        """Make a latex table displaying the data in the result file.
-
-        Parameters
-        ----------
-        labels: list, optional
-            list of labels that you want to include in the table
-        parameter_dict: dict, optional
-            dictionary of parameters that you wish to include in the latex
-            table. The keys are the name of the parameters and the items are
-            the descriptive text. If None, all parameters are included
-        save_to_file: str, optional
-            name of the file you wish to save the latex table to. If None, print
-            to stdout
-        """
-        import os
-
-        if save_to_file is not None and os.path.isfile("{}".format(save_to_file)):
-            raise FileExistsError(
-                "The file {} already exists.".format(save_to_file)
-            )
-        if labels != "all" and isinstance(labels, str) and labels not in self.labels:
-            raise ValueError("The label %s does not exist." % (labels))
-        elif labels == "all":
-            labels = list(self.labels)
-        elif isinstance(labels, str):
-            labels = [labels]
-        elif isinstance(labels, list):
-            for ll in labels:
-                if ll not in list(self.labels):
-                    raise ValueError("The label %s does not exist." % (ll))
-
-        table = self.latex_table(
-            [self.samples_dict[label] for label in labels], parameter_dict,
-            labels=labels
-        )
-        if save_to_file is None:
-            print(table)
-        elif os.path.isfile("{}".format(save_to_file)):
-            logger.warning(
-                "File {} already exists. Printing to stdout".format(save_to_file)
-            )
-            print(table)
-        else:
-            with open(save_to_file, "w") as f:
-                f.writelines([table])
-
-    def generate_latex_macros(
-        self, labels="all", parameter_dict=None, save_to_file=None,
-        rounding=2
-    ):
-        """Generate a list of latex macros for each parameter in the result
-        file
-
-        Parameters
-        ----------
-        labels: list, optional
-            list of labels that you want to include in the table
-        parameter_dict: dict, optional
-            dictionary of parameters that you wish to generate macros for. The
-            keys are the name of the parameters and the items are the latex
-            macros name you wish to use. If None, all parameters are included.
-        save_to_file: str, optional
-            name of the file you wish to save the latex table to. If None, print
-            to stdout
-        rounding: int, optional
-            number of decimal points to round the latex macros
-        """
-        import os
-
-        if save_to_file is not None and os.path.isfile("{}".format(save_to_file)):
-            raise FileExistsError(
-                "The file {} already exists.".format(save_to_file)
-            )
-        if labels != "all" and isinstance(labels, str) and labels not in self.labels:
-            raise ValueError("The label %s does not exist." % (labels))
-        elif labels == "all":
-            labels = list(self.labels)
-        elif isinstance(labels, str):
-            labels = [labels]
-        elif isinstance(labels, list):
-            for ll in labels:
-                if ll not in list(self.labels):
-                    raise ValueError("The label %s does not exist." % (ll))
-
-        macros = self.latex_macros(
-            [self.samples_dict[i] for i in labels], parameter_dict,
-            labels=labels, rounding=rounding
-        )
-        if save_to_file is None:
-            print(macros)
-        else:
-            with open(save_to_file, "w") as f:
-                f.writelines([macros])
-
 
 class PESummaryDeprecated(PESummary):
     """
     """
+    @deprecation(
+        "This file format is out-of-date and may not be supported in future "
+        "releases."
+    )
     def __init__(self, path_to_results_file, **kwargs):
-        warnings.warn(deprecation_warning)
         super(PESummaryDeprecated, self).__init__(path_to_results_file, **kwargs)
 
     @property
     def load_kwargs(self):
         return {
             "grab_data_from_dictionary": PESummaryDeprecated._grab_data_from_dictionary
         }
@@ -717,20 +607,21 @@
 
         parameter_list, sample_list, inj_list, ver_list = [], [], [], []
         meta_data_list, weights_list = [], []
         for num, label in enumerate(labels):
             posterior_samples = dictionary["posterior_samples"][label]
             if isinstance(posterior_samples, (h5py._hl.dataset.Dataset, np.ndarray)):
                 parameters = [j for j in posterior_samples.dtype.names]
-                samples = [np.array(j.tolist()) for j in posterior_samples]
+                samples = [np.array(j).tolist() for j in posterior_samples]
             else:
                 parameters = \
                     dictionary["posterior_samples"][label]["parameter_names"].copy()
                 samples = [
-                    j for j in dictionary["posterior_samples"][label]["samples"]
+                    np.array(j).tolist() for j in
+                    dictionary["posterior_samples"][label]["samples"]
                 ].copy()
                 if isinstance(parameters[0], bytes):
                     parameters = [
                         parameter.decode("utf-8") for parameter in parameters
                     ]
             parameter_list.append(parameters)
             if "injection_data" in dictionary.keys():
@@ -750,35 +641,35 @@
                 inj_list.append({
                     parameter: parse_injection_value(value)
                     for parameter, value in zip(parameters, inj)
                 })
             sample_list.append(samples)
             config = None
             if "config_file" in dictionary.keys():
-                config, = Read.load_recursively("config_file", dictionary)
+                config, = load_recursively("config_file", dictionary)
             if "meta_data" in dictionary.keys():
-                data, = Read.load_recursively("meta_data", dictionary)
+                data, = load_recursively("meta_data", dictionary)
                 meta_data_list.append(data[label])
             else:
                 meta_data_list.append({"sampler": {}, "meta_data": {}})
             if "weights" in parameters or b"weights" in parameters:
                 ind = (
                     parameters.index("weights") if "weights" in parameters
                     else parameters.index(b"weights")
                 )
                 weights_list.append(Array([sample[ind] for sample in samples]))
             else:
                 weights_list.append(None)
         if "version" in dictionary.keys():
-            version, = Read.load_recursively("version", dictionary)
+            version, = load_recursively("version", dictionary)
         else:
             version = {label: "No version information found" for label in labels
                        + ["pesummary"]}
         if "priors" in dictionary.keys():
-            priors, = Read.load_recursively("priors", dictionary)
+            priors, = load_recursively("priors", dictionary)
         else:
             priors = dict()
         for label in list(version.keys()):
             if label != "pesummary" and isinstance(version[label], bytes):
                 ver_list.append(version[label].decode("utf-8"))
             elif label != "pesummary":
                 ver_list.append(version[label])
```

### Comparing `pesummary-0.9.1/pesummary/core/file/mcmc.py` & `pesummary-1.0.0/pesummary/core/file/mcmc.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,26 +1,14 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from pesummary.utils.utils import logger
 import numpy as np
 import copy
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 STEP_NUMBER_PARAMS = ["cycle"]
 algorithms = ["burnin_by_step_number", "burnin_by_first_n"]
 
 
 def _number_of_negative_steps(samples, logger_level="debug"):
     """Return the number of samples that have step < 0 for each dictionary
 
@@ -28,15 +16,20 @@
     ----------
     samples: pesummary.utils.samples_dict.MCMCSamplesDict
         MCMCSamplesDict object containing the samples for multiple mcmc chains
     logger_level: str, optional
         logger level to use when printing information to stdout. Default debug
     """
     _samples = copy.deepcopy(samples)
-    parameters = list(_samples.parameters)
+    try:
+        parameters = set.intersection(
+            *[set(_params) for _params in _samples.parameters.values()]
+        )
+    except AttributeError:
+        parameters = list(_samples.parameters)
     step_param = [
         alternative for alternative in STEP_NUMBER_PARAMS if alternative
         in parameters
     ]
     if not len(step_param):
         logger.warning(
             "Unable to find a step number in the MCMCSamplesDict object. "
@@ -70,14 +63,21 @@
     samples: pesummary.utils.samples_dict.MCMCSamplesDict
         MCMCSamplesDict object containing the samples for multiple mcmc chains
     logger_level: str, optional
         logger level to use when printing information to stdout. Default debug
     """
     _samples = copy.deepcopy(samples)
     n_samples = _number_of_negative_steps(_samples, logger_level=logger_level)
+    getattr(logger, logger_level)(
+        "Removing the first {} as burnin".format(
+            ", ".join(
+                ["{} samples from {}".format(val, key) for key, val in n_samples.items()]
+            )
+        )
+    )
     return _samples.discard_samples(n_samples)
 
 
 def burnin_by_first_n(samples, N, step_number=False, logger_level="debug"):
     """Discard the first N samples as burnin
 
     Parameters
@@ -91,12 +91,18 @@
     logger_level: str, optional
         logger level to use when printing information to stdout. Default debug
     """
     _samples = copy.deepcopy(samples)
     n_samples = {key: N for key in _samples.keys()}
     if step_number:
         n_samples = {
-            key: item + N for key, item in _number_of_negative_steps(
-                _samples, logger_level=logger_level
-            ).items()
+            key: item + N if item is not None else N for key, item in
+            _number_of_negative_steps(_samples, logger_level=logger_level).items()
         }
+    getattr(logger, logger_level)(
+        "Removing the first {} as burnin".format(
+            ", ".join(
+                ["{} samples from {}".format(val, key) for key, val in n_samples.items()]
+            )
+        )
+    )
     return _samples.discard_samples(n_samples)
```

### Comparing `pesummary-0.9.1/pesummary/core/file/meta_file.py` & `pesummary-1.0.0/pesummary/core/file/meta_file.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,43 +1,31 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import inspect
 import os
 import numpy as np
 import json
 import copy
-
+from getpass import getuser
+import pandas as pd
 import pesummary
 from pesummary import __version__
-from pesummary.core.inputs import PostProcessing
+from pesummary.utils.dict import Dict
 from pesummary.utils.samples_dict import SamplesDict
 from pesummary.utils.utils import make_dir, logger
 from pesummary.utils.decorators import open_config
 from pesummary import conf
 
-
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 DEFAULT_HDF5_KEYS = ["version", "history"]
 
 
 def recursively_save_dictionary_to_hdf5_file(
-        f, dictionary, current_path=None, extra_keys=DEFAULT_HDF5_KEYS,
-        compression=None
+    f, dictionary, current_path=None, extra_keys=DEFAULT_HDF5_KEYS,
+    compression=None
 ):
     """Recursively save a dictionary to a hdf5 file
 
     Parameters
     ----------
     f: h5py._hl.files.File
         the open hdf5 file that you would like the data to be saved to
@@ -55,29 +43,37 @@
     for key in extra_keys:
         if key in dictionary:
             _safe_create_hdf5_group(hdf5_file=f, key=key)
     if current_path is None:
         current_path = []
 
     for k, v in dictionary.items():
+        if isinstance(v, pd.DataFrame):
+            v = v.to_dict(orient="list")
         if isinstance(v, dict):
             if k not in f["/" + "/".join(current_path)].keys():
                 f["/".join(current_path)].create_group(k)
             path = current_path + [k]
             recursively_save_dictionary_to_hdf5_file(
                 f, v, path, extra_keys=extra_keys, compression=compression
             )
         else:
+            if isinstance(dictionary, Dict):
+                attrs = dictionary.extra_kwargs
+            else:
+                attrs = {}
             create_hdf5_dataset(
                 key=k, value=v, hdf5_file=f, current_path=current_path,
-                compression=compression
+                compression=compression, attrs=attrs
             )
 
 
-def create_hdf5_dataset(key, value, hdf5_file, current_path, compression=None):
+def create_hdf5_dataset(
+    key, value, hdf5_file, current_path, compression=None, attrs={}
+):
     """
     Create a hdf5 dataset in place
 
     Parameters
     ----------
     key: str
         Key for the new dataset
@@ -89,14 +85,16 @@
     hdf5_file: h5py.File
         hdf5 file object
     current_path: str
         Current string withing the hdf5 file
     compression: int, optional
         optional filter to apply for compression. If you do not want to
         apply compression, compression = None. Default None.
+    attrs: dict, optional
+        optional list of attributes to store alongside the dataset
     """
     error_message = "Cannot process {}={} from list with type {} for hdf5"
     array_types = (list, pesummary.utils.samples_dict.Array, np.ndarray)
     numeric_types = (float, int, np.number)
     string_types = (str, bytes)
     SOFTLINK = False
 
@@ -109,14 +107,16 @@
             data = np.array(value, dtype="S")
         elif isinstance(value[0], array_types):
             data = np.array(np.vstack(value))
         elif isinstance(value[0], (tuple, np.record, np.recarray)):
             data = value
         elif all(isinstance(_value, (bool, np.bool_)) for _value in value):
             data = np.array([str(_value) for _value in value], dtype="S")
+        elif all(_value is None for _value in value):
+            data = np.array(["None"] * len(value), dtype="S")
         elif isinstance(value[0], np.void) and value.dtype.names:
             data = value
         elif math.isnan(value[0]):
             data = np.array(["NaN"] * len(value), dtype="S")
         elif isinstance(value[0], numeric_types):
             data = np.array(value)
         else:
@@ -139,30 +139,40 @@
         )
     elif isinstance(value, string_types):
         data = np.array([value], dtype="S")
     elif isinstance(value, numeric_types):
         data = np.array([value])
     elif isinstance(value, (bool, np.bool_)):
         data = np.array([str(value)], dtype="S")
+    elif isinstance(value, complex):
+        key += "_amp"
+        data = np.array(np.abs(value))
     elif value == {}:
         data = np.array(np.array("NaN"))
     elif inspect.isclass(value) or inspect.isfunction(value):
         data = np.array([value.__module__ + value.__name__], dtype="S")
     elif inspect.ismodule(value):
         data = np.array([value.__name__], dtype="S")
+    elif value is None:
+        data = np.array(["None"], dtype="S")
     else:
         raise TypeError(error_message.format(key, value, type(value)))
     if not SOFTLINK:
         if compression is not None and len(data) > conf.compression_min_length:
             kwargs = {"compression": "gzip", "compression_opts": compression}
         else:
             kwargs = {}
-        hdf5_file["/".join(current_path)].create_dataset(
-            key, data=data, **kwargs
-        )
+        try:
+            dset = hdf5_file["/".join(current_path)].create_dataset(
+                key, data=data, **kwargs
+            )
+        except ValueError:
+            dset = hdf5_file.create_dataset(key, data=data, **kwargs)
+        if len(attrs):
+            dset.attrs.update(attrs)
 
 
 class PESummaryJsonEncoder(json.JSONEncoder):
     """Personalised JSON encoder for PESummary
     """
     def default(self, obj):
         """Return a json serializable object for 'obj'
@@ -176,15 +186,15 @@
             return obj.tolist()
         if inspect.isfunction(obj):
             return str(obj)
         if isinstance(obj, np.integer):
             return int(obj)
         elif isinstance(obj, np.floating):
             return float(obj)
-        elif isinstance(obj, (np.bool, np.bool_, bool)):
+        elif isinstance(obj, (bool, np.bool_)):
             return str(obj)
         elif isinstance(obj, bytes):
             return str(obj)
         elif isinstance(obj, type):
             return str(obj)
         return json.JSONEncoder.default(self, obj)
 
@@ -195,15 +205,16 @@
     def __init__(
         self, samples, labels, config, injection_data, file_versions,
         file_kwargs, webdir=None, result_files=None, hdf5=False, priors={},
         existing_version=None, existing_label=None, existing_samples=None,
         existing_injection=None, existing_metadata=None, existing_config=None,
         existing_priors={}, existing_metafile=None, outdir=None, existing=None,
         package_information={}, mcmc_samples=False, filename=None,
-        external_hdf5_links=False, hdf5_compression=None, history=None
+        external_hdf5_links=False, hdf5_compression=None, history=None,
+        descriptions=None
     ):
         self.data = {}
         self.webdir = webdir
         self.result_files = result_files
         self.samples = samples
         self.labels = labels
         self.config = config
@@ -211,30 +222,46 @@
         self.file_versions = file_versions
         self.file_kwargs = file_kwargs
         self.hdf5 = hdf5
         self.file_name = filename
         self.external_hdf5_links = external_hdf5_links
         self.hdf5_compression = hdf5_compression
         self.history = history
+        self.descriptions = descriptions
         if self.history is None:
             from pesummary.utils.utils import history_dictionary
 
-            self.history = history_dictionary(creator='')
+            try:
+                _user = getuser()
+            except (ImportError, KeyError):
+                _user = ''
+            self.history = history_dictionary(creator=_user)
+        if self.descriptions is None:
+            self.descriptions = {
+                label: "No description found" for label in self.labels
+            }
+        elif not all(label in self.descriptions.keys() for label in self.labels):
+            for label in self.labels:
+                if label not in self.descriptions.keys():
+                    self.descriptions[label] = "No description found"
         self.priors = priors
         self.existing_version = existing_version
         self.existing_labels = existing_label
         self.existing_samples = existing_samples
         self.existing_injection = existing_injection
         self.existing_file_kwargs = existing_metadata
         self.existing_config = existing_config
         self.existing_priors = existing_priors
         self.existing_metafile = existing_metafile
         self.existing = existing
         self.outdir = outdir
         self.package_information = package_information
+        if not len(package_information):
+            from pesummary.core.cli.inputs import _Input
+            self.package_information = _Input.get_package_information()
         self.mcmc_samples = mcmc_samples
 
         if self.existing_labels is None:
             self.existing_labels = [None]
         if self.existing is not None:
             self.add_existing_data()
 
@@ -264,50 +291,65 @@
             if self.hdf5:
                 self._file_name = base.format("h5")
             else:
                 self._file_name = base.format("json")
 
     @property
     def meta_file(self):
-        return os.path.join(self.outdir, self.file_name)
+        return os.path.join(os.path.abspath(self.outdir), self.file_name)
 
     def make_dictionary(self):
         """Wrapper function for _make_dictionary
         """
         self._make_dictionary()
 
-    def _make_dictionary(self):
-        """Generate a single dictionary which stores all information
-        """
+    @property
+    def _dictionary_structure(self):
         if self.mcmc_samples:
             posterior = "mcmc_chains"
         else:
             posterior = "posterior_samples"
         dictionary = {
             label: {
                 posterior: {}, "injection_data": {}, "version": {},
                 "meta_data": {}, "priors": {}, "config_file": {}
             } for label in self.labels
         }
         dictionary["version"] = self.package_information
         dictionary["version"]["pesummary"] = [__version__]
         dictionary["history"] = self.history
+        return dictionary
+
+    def _make_dictionary(self):
+        """Generate a single dictionary which stores all information
+        """
+        if self.mcmc_samples:
+            posterior = "mcmc_chains"
+        else:
+            posterior = "posterior_samples"
+        dictionary = self._dictionary_structure
+        if self.file_kwargs is not None and isinstance(self.file_kwargs, dict):
+            if "webpage_url" in self.file_kwargs.keys():
+                dictionary["history"]["webpage_url"] = self.file_kwargs["webpage_url"]
+            else:
+                dictionary["history"]["webpage_url"] = "None"
         for num, label in enumerate(self.labels):
             parameters = self.samples[label].keys()
             samples = np.array([self.samples[label][i] for i in parameters]).T
             dictionary[label][posterior] = {
                 "parameter_names": list(parameters), "samples": samples.tolist()
             }
             dictionary[label]["injection_data"] = {
                 "parameters": list(parameters),
                 "samples": [
                     self.injection_data[label][i] for i in parameters
                 ]
             }
             dictionary[label]["version"] = [self.file_versions[label]]
+            dictionary[label]["description"] = [self.descriptions[label]]
             dictionary[label]["meta_data"] = self.file_kwargs[label]
             if self.config != {} and self.config[num] is not None and \
                     not isinstance(self.config[num], dict):
                 config = self._grab_config_data_from_data_file(self.config[num])
                 dictionary[label]["config_file"] = config
             elif self.config[num] is not None:
                 dictionary[label]["config_file"] = self.config[num]
@@ -486,41 +528,79 @@
         for label in labels:
             meta_file_data[label] = "external:{}|{}".format(
                 sub_file_name.format(label=label), label
             )
         return meta_file_data, sub_file_data
 
     @staticmethod
+    def convert_posterior_samples_to_numpy(labels, samples, mcmc_samples=False):
+        """Convert a dictionary of multiple posterior samples from a
+        column-major dictionary to a row-major numpy array
+
+        Parameters
+        ----------
+        labels: list
+            list of unique labels for each analysis
+        samples: MultiAnalysisSamplesDict
+            dictionary of multiple posterior samples to convert to a numpy
+            array.
+        mcmc_samples: Bool, optional
+            if True, the dictionary contains seperate mcmc chains
+
+        Examples
+        --------
+        >>> dictionary = MultiAnalysisSamplesDict(
+        ...     {"label": {"mass_1": [1,2,3], "mass_2": [1,2,3]}}
+        ... )
+        >>> dictionary = _Metafile.convert_posterior_samples_to_numpy(
+        ...     dictionary.keys(), dictionary
+        ... )
+        >>> print(dictionary)
+        ... {"label": rec.array([(1., 1.), (2., 2.), (3., 3.)],
+        ...           dtype=[('mass_1', '<f4'), ('mass_2', '<f4')])}
+        """
+        converted_samples = {
+            label: _MetaFile._convert_posterior_samples_to_numpy(
+                samples[label], mcmc_samples=mcmc_samples
+            ) for label in labels
+        }
+        return converted_samples
+
+    @staticmethod
     def save_to_hdf5(
         data, labels, samples, meta_file, no_convert=False,
         extra_keys=DEFAULT_HDF5_KEYS, mcmc_samples=False,
-        external_hdf5_links=False, compression=None
+        external_hdf5_links=False, compression=None, _class=None
     ):
         """Save the metafile as a hdf5 file
         """
         import h5py
 
+        if _class is None:
+            _class = _MetaFile
         if mcmc_samples:
             key = "mcmc_chains"
         else:
             key = "posterior_samples"
         if not no_convert:
+            _samples = _class.convert_posterior_samples_to_numpy(
+                labels, samples, mcmc_samples=mcmc_samples
+            )
             for label in labels:
-                data[label][key] = _MetaFile._convert_posterior_samples_to_numpy(
-                    samples[label], mcmc_samples=mcmc_samples
-                )
-                data[label]["injection_data"] = \
-                    _MetaFile._convert_posterior_samples_to_numpy(
-                        SamplesDict({
-                            param: samp for param, samp in zip(
-                                data[label]["injection_data"]["parameters"],
-                                data[label]["injection_data"]["samples"]
-                            )
-                        }), index=[0]
-                )
+                data[label][key] = _samples[label]
+                if "injection_data" in data[label].keys():
+                    data[label]["injection_data"] = \
+                        _class._convert_posterior_samples_to_numpy(
+                            SamplesDict({
+                                param: samp for param, samp in zip(
+                                    data[label]["injection_data"]["parameters"],
+                                    data[label]["injection_data"]["samples"]
+                                )
+                            }), index=[0]
+                    )
         if external_hdf5_links:
             from pathlib import Path
 
             _dir = Path(meta_file).parent
             name = "_{label}.h5"
             _subfile = os.path.join(_dir, name)
             meta_file_data, sub_file_data = (
@@ -575,41 +655,40 @@
         """
         """
         from pesummary.utils.utils import _add_existing_data
 
         self = _add_existing_data(self)
 
 
-class MetaFile(PostProcessing):
+class MetaFile(object):
     """This class handles the creation of a metafile storing all information
     from the analysis
     """
     def __init__(self, inputs, history=None):
         from pesummary.utils.utils import logger
-
-        super(MetaFile, self).__init__(inputs)
         logger.info("Starting to generate the meta file")
         meta_file = _MetaFile(
-            self.samples, self.labels, self.config,
-            self.injection_data, self.file_version, self.file_kwargs,
-            hdf5=self.hdf5, webdir=self.webdir, result_files=self.result_files,
-            existing_version=self.existing_file_version, existing_label=self.existing_labels,
-            priors=self.priors, existing_samples=self.existing_samples,
-            existing_injection=self.existing_injection_data,
-            existing_metadata=self.existing_file_kwargs,
-            existing_config=self.existing_config, existing=self.existing,
-            existing_priors=self.existing_priors,
-            existing_metafile=self.existing_metafile,
-            package_information=self.package_information,
-            mcmc_samples=self.mcmc_samples, filename=self.filename,
-            external_hdf5_links=self.external_hdf5_links,
-            hdf5_compression=self.hdf5_compression, history=history
+            inputs.samples, inputs.labels, inputs.config,
+            inputs.injection_data, inputs.file_version, inputs.file_kwargs,
+            hdf5=inputs.hdf5, webdir=inputs.webdir, result_files=inputs.result_files,
+            existing_version=inputs.existing_file_version, existing_label=inputs.existing_labels,
+            priors=inputs.priors, existing_samples=inputs.existing_samples,
+            existing_injection=inputs.existing_injection_data,
+            existing_metadata=inputs.existing_file_kwargs,
+            existing_config=inputs.existing_config, existing=inputs.existing,
+            existing_priors=inputs.existing_priors,
+            existing_metafile=inputs.existing_metafile,
+            package_information=inputs.package_information,
+            mcmc_samples=inputs.mcmc_samples, filename=inputs.filename,
+            external_hdf5_links=inputs.external_hdf5_links,
+            hdf5_compression=inputs.hdf5_compression, history=history,
+            descriptions=inputs.descriptions
         )
         meta_file.make_dictionary()
-        if not self.hdf5:
+        if not inputs.hdf5:
             meta_file.save_to_json(meta_file.data, meta_file.meta_file)
         else:
             meta_file.save_to_hdf5(
                 meta_file.data, meta_file.labels, meta_file.samples,
                 meta_file.meta_file, mcmc_samples=meta_file.mcmc_samples,
                 external_hdf5_links=meta_file.external_hdf5_links,
                 compression=meta_file.hdf5_compression
```

### Comparing `pesummary-0.9.1/pesummary/core/file/read.py` & `pesummary-1.0.0/pesummary/core/file/read.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,43 +1,42 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from pesummary.core.file.formats.base_read import Read
 from pesummary.core.file.formats.bilby import Bilby
 from pesummary.core.file.formats.default import Default
 from pesummary.core.file.formats.pesummary import PESummary, PESummaryDeprecated
 from pesummary.utils.utils import logger
 import os
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 def is_bilby_hdf5_file(path):
     """Determine if the results file is a bilby hdf5 results file
 
     Parameters
     ----------
     path: str
         path to the results file
     """
-    import deepdish
+    import h5py
     try:
-        f = deepdish.io.load(path)
+        f = h5py.File(path, "r")
         if "bilby" in f["version"]:
             return True
+        elif "bilby" in str(f["version"][0]):
+            return True
+        return False
+    except (KeyError, TypeError):
+        try:
+            if "bilby" in f["meta_data"]["loaded_modules"].keys():
+                return True
+            return False
+        except Exception:
+            return False
     except Exception:
         return False
     return False
 
 
 def is_bilby_json_file(path):
     """Determine if the results file is a bilby json results file
@@ -49,14 +48,16 @@
     """
     import json
     with open(path, "r") as f:
         data = json.load(f)
     try:
         if "bilby" in data["version"]:
             return True
+        elif "bilby" in data["version"][0]:
+            return True
         else:
             return False
     except Exception:
         return False
 
 
 def _is_pesummary_hdf5_file(path, check_function):
@@ -147,15 +148,21 @@
     """
     if "posterior_samples" in f.keys():
         try:
             import collections
 
             labels = f["posterior_samples"].keys()
             if isinstance(labels, collections.abc.KeysView):
-                return True
+                _label = list(labels)[0]
+                try:
+                    _param = list(f["posterior_samples"][_label].keys())[0]
+                    samples = f["posterior_samples"][_label][_param]
+                    return True
+                except Exception:
+                    return False
             else:
                 return False
         except Exception:
             return False
     else:
         return False
 
@@ -170,20 +177,20 @@
     """
     labels = f.keys()
     if "version" not in labels:
         return False
     try:
         if all(
                 "posterior_samples" in f[label].keys() for label in labels if
-                label != "version" and label != "history"
+                label != "version" and label != "history" and label != "strain"
         ):
             return True
         elif all(
                 "mcmc_chains" in f[label].keys() for label in labels if
-                label != "version" and label != "history"
+                label != "version" and label != "history" and label != "strain"
         ):
             return True
         else:
             return False
     except Exception:
         return False
 
@@ -291,15 +298,18 @@
         function loops through all possible options
     **kwargs: dict, optional
         all additional kwargs are passed directly to the load_file class method
     """
     path = os.path.expanduser(path)
     extension = Read.extension_from_path(path)
 
+    if extension in ["gz"]:
+        from pesummary.utils.utils import unzip
+        path = unzip(path)
     if extension in ["hdf5", "h5", "hdf"]:
         options = _file_format(file_format, HDF5_LOAD)
         return _read(path, options, default=DEFAULT, **kwargs)
     elif extension == "json":
         options = _file_format(file_format, JSON_LOAD)
         return _read(path, options, default=DEFAULT, **kwargs)
     else:
-        return DEFAULT["default"](path, **kwargs)
+        return DEFAULT["default"](path, file_format=file_format, **kwargs)
```

### Comparing `pesummary-0.9.1/pesummary/core/css/navbar.css` & `pesummary-1.0.0/pesummary/core/css/navbar.css`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/css/image_styles.css` & `pesummary-1.0.0/pesummary/core/css/image_styles.css`

 * *Files 11% similar despite different names*

```diff
@@ -1,14 +1,14 @@
 img {                                                                           
     border-radius: 5px;                                                         
     cursor: pointer;                                                            
     transition: 0.3s;                                                           
 }                                                                               
                                                                                 
-img:hover {opacity: 0.5;}
+img:hover {opacity: 0.8;}
 
 .popover {
     max-width: 550px;
     width: 550px;
 }
 
 .btn-xs {
```

### Comparing `pesummary-0.9.1/pesummary/core/css/table.css` & `pesummary-1.0.0/pesummary/core/css/table.css`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/css/side_bar.css` & `pesummary-1.0.0/pesummary/core/css/side_bar.css`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/html_to_json.js` & `pesummary-1.0.0/pesummary/core/js/html_to_json.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/side_bar.js` & `pesummary-1.0.0/pesummary/core/js/side_bar.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/multi_dropbar.js` & `pesummary-1.0.0/pesummary/core/js/multi_dropbar.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/grab.js` & `pesummary-1.0.0/pesummary/core/js/grab.js`

 * *Files 10% similar despite different names*

#### js-beautify {}

```diff
@@ -81,46 +81,60 @@
     ----------
     param: str
         name of the parameter that you want to link to
     */
     var header = document.getElementsByTagName("h1")[0]
     var el = document.getElementsByTagName("h7")[1]
     var approx = el.innerHTML
+    var url = window.location.pathname
+    var filename = url.substring(url.lastIndexOf('/') + 1)
 
-    if (param == approximant) {
-
-        if (approx == "Comparison" && param == "Comparison") {
-            approx = "None"
+    if (label == "switch") {
+        if (approx == "Comparison") {
+            var new_filename = filename.replace(approx, param + "_" + param)
+        } else if (param == "Comparison") {
+            var new_filename = filename.replace(approx + "_" + approx, param)
+        } else {
+            var re = new RegExp(approx, "g")
+            var new_filename = filename.replace(re, param);
         }
+        setTimeout(function() {
+            window.location = "error.html"
+        }, 450)
+        window.location = "./" + new_filename;
+    } else {
+        if (param == approximant) {
 
-        var url = window.location.pathname
-        var filename = url.substring(url.lastIndexOf('/') + 1)
-        if (param == "home") {
-            if (filename == "home.html") {
-                window.location = "./home.html"
-            } else {
-                window.location = "../home.html"
-            }
-        } else if (param == "Downloads") {
-            if (filename == "home.html") {
-                window.location = "./html/Downloads.html"
-            } else {
-                window.location = "../html/Downloads.html"
+            if (approx == "Comparison" && param == "Comparison") {
+                approx = "None"
             }
-        } else if (param == "About") {
-            if (filename == "home.html") {
-                window.location = "./html/About.html"
+            if (param == "home") {
+                if (filename == "home.html") {
+                    window.location = "./home.html"
+                } else {
+                    window.location = "../home.html"
+                }
+            } else if (param == "Downloads") {
+                if (filename == "home.html") {
+                    window.location = "./html/Downloads.html"
+                } else {
+                    window.location = "../html/Downloads.html"
+                }
+            } else if (param == "About") {
+                if (filename == "home.html") {
+                    window.location = "./html/About.html"
+                } else {
+                    window.location = "../html/About.html"
+                }
+            } else if (param == "Comparison") {
+                if (filename == "home.html") {
+                    window.location = "./html/Comparison.html"
+                } else {
+                    window.location = "../html/Comparison.html"
+                }
+            } else if (filename == "home.html") {
+                _option1(label, approx, param)
             } else {
-                window.location = "../html/About.html"
+                _option2(label, approx, param)
             }
-        } else if (param == "Comparison") {
-            if (filename == "home.html") {
-                window.location = "./html/Comparison.html"
-            } else {
-                window.location = "../html/Comparison.html"
-            }
-        } else if (filename == "home.html") {
-            _option1(label, approx, param)
-        } else {
-            _option2(label, approx, param)
         }
     }
```

### Comparing `pesummary-0.9.1/pesummary/core/js/multiple_posteriors.js` & `pesummary-1.0.0/pesummary/core/js/multiple_posteriors.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/html_to_csv.js` & `pesummary-1.0.0/pesummary/core/js/html_to_csv.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/html_to_shell.js` & `pesummary-1.0.0/pesummary/core/js/html_to_shell.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/combine_corner.js` & `pesummary-1.0.0/pesummary/core/js/combine_corner.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/search.js` & `pesummary-1.0.0/pesummary/core/js/search.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/js/modal.js` & `pesummary-1.0.0/pesummary/core/js/modal.js`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/core/inputs.py` & `pesummary-1.0.0/pesummary/core/cli/inputs.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,38 +1,29 @@
-# Copyrigh (C) 2019 Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
-import re
 import socket
 from glob import glob
 import pkg_resources
+from pathlib import Path
+from getpass import getuser
 
+import math
 import numpy as np
-import pesummary
 from pesummary.core.file.read import read as Read
 from pesummary.utils.exceptions import InputError
+from pesummary.utils.decorators import deprecation
 from pesummary.utils.samples_dict import SamplesDict, MCMCSamplesDict
 from pesummary.utils.utils import (
-    guess_url, logger, make_dir, make_cache_style_file
+    guess_url, logger, make_dir, make_cache_style_file, list_match
 )
 from pesummary import conf
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class _Input(object):
     """Super class to handle the command line arguments
     """
     @staticmethod
     def is_pesummary_metafile(proposed_file):
         """Determine if a file is a PESummary metafile or not
@@ -69,57 +60,84 @@
             return result
         else:
             return False
 
     @staticmethod
     def grab_data_from_metafile(
         existing_file, webdir, compare=None, read_function=Read,
-        nsamples=None, **kwargs
+        _replace_with_pesummary_kwargs={}, nsamples=None,
+        disable_injection=False, keep_nan_likelihood_samples=False,
+        reweight_samples=False, **kwargs
     ):
         """Grab data from an existing PESummary metafile
 
         Parameters
         ----------
         existing_file: str
             path to the existing metafile
         webdir: str
             the directory to store the existing configuration file
         compare: list, optional
             list of labels for events stored in an existing metafile that you
             wish to compare
         read_function: func, optional
             PESummary function to use to read in the existing file
+        _replace_with_pesummary_kwargs: dict, optional
+            dictionary of kwargs that you wish to replace with the data stored
+            in the PESummary file
         nsamples: int, optional
             Number of samples to use. Default all available samples
         kwargs: dict
             All kwargs are passed to the `generate_all_posterior_samples`
             method
         """
-        f = read_function(existing_file)
+        f = read_function(
+            existing_file,
+            remove_nan_likelihood_samples=not keep_nan_likelihood_samples
+        )
+        for ind, label in enumerate(f.labels):
+            kwargs[label] = kwargs.copy()
+            for key, item in _replace_with_pesummary_kwargs.items():
+                try:
+                    kwargs[label][key] = eval(
+                        item.format(file="f", ind=ind, label=label)
+                    )
+                except TypeError:
+                    _item = item.split("['{label}']")[0]
+                    kwargs[label][key] = eval(
+                        _item.format(file="f", ind=ind, label=label)
+                    )
+                except (AttributeError, KeyError, NameError):
+                    pass
+
         if nsamples is not None:
             f.downsample(nsamples)
+
         if not f.mcmc_samples:
-            f.generate_all_posterior_samples(**kwargs)
             labels = f.labels
-            indicies = np.arange(len(labels))
         else:
             labels = list(f.samples_dict.keys())
-            indicies = np.arange(len(labels))
+        indicies = np.arange(len(labels))
 
         if compare:
             indicies = []
             for i in compare:
                 if i not in labels:
                     raise InputError(
                         "Label '%s' does not exist in the metafile. The list "
                         "of available labels are %s" % (i, labels)
                     )
                 indicies.append(labels.index(i))
             labels = compare
 
+        if not f.mcmc_samples:
+            f.generate_all_posterior_samples(labels=labels, **kwargs)
+        if reweight_samples:
+            f.reweight_samples(reweight_samples, labels=labels, **kwargs)
+
         parameters = f.parameters
         if not f.mcmc_samples:
             samples = [np.array(i).T for i in f.samples]
             DataFrame = {
                 label: SamplesDict(parameters[ind], samples[ind])
                 for label, ind in zip(labels, indicies)
             }
@@ -131,43 +149,52 @@
                         label: f.samples_dict[label] for label in labels
                     }
                 )
             }
             labels = f.labels
             indicies = np.arange(len(labels))
             _parameters = lambda label: DataFrame[f.labels[0]].parameters
-        if f.injection_parameters != []:
+        if not disable_injection and f.injection_parameters != []:
             inj_values = f.injection_dict
             for label in labels:
                 for param in DataFrame[label].keys():
                     if param not in f.injection_dict[label].keys():
                         f.injection_dict[label][param] = float("nan")
         else:
             inj_values = {
-                i: [float("nan")] * len(DataFrame[i]) for i in labels
+                i: {
+                    param: float("nan") for param in DataFrame[i].parameters
+                } for i in labels
             }
         for i in inj_values.keys():
             for param in inj_values[i].keys():
                 if inj_values[i][param] == "nan":
                     inj_values[i][param] = float("nan")
                 if isinstance(inj_values[i][param], bytes):
                     inj_values[i][param] = inj_values[i][param].decode("utf-8")
 
-        if hasattr(f, "priors") and f.priors != {}:
+        if hasattr(f, "priors") and f.priors is not None and f.priors != {}:
             priors = f.priors
         else:
             priors = {label: {} for label in labels}
 
         config = []
         if f.config is not None and not all(i is None for i in f.config):
             config = []
             for i in labels:
                 config_dir = os.path.join(webdir, "config")
-                filename = f.write_config_to_file(i, outdir=config_dir)
-                config.append(filename)
+                filename = f.write_config_to_file(
+                    i, outdir=config_dir, _raise=False,
+                    filename="{}_config.ini".format(i)
+                )
+                _config = os.path.join(config_dir, filename)
+                if filename is not None and os.path.isfile(_config):
+                    config.append(_config)
+                else:
+                    config.append(None)
         else:
             for i in labels:
                 config.append(None)
 
         if f.weights is not None:
             weights = {i: f.weights[i] for i in labels}
         else:
@@ -187,22 +214,26 @@
                 )
             },
             "prior": priors,
             "config": config,
             "labels": labels,
             "weights": weights,
             "indicies": indicies,
-            "mcmc_samples": f.mcmc_samples
+            "mcmc_samples": f.mcmc_samples,
+            "open_file": f,
+            "descriptions": f.description
         }
 
     @staticmethod
     def grab_data_from_file(
-        file, label, config=None, injection=None, read_function=Read,
+        file, label, webdir, config=None, injection=None, read_function=Read,
         file_format=None, nsamples=None, disable_prior_sampling=False,
-        path_to_samples=None, **kwargs
+        nsamples_for_prior=None, path_to_samples=None,
+        keep_nan_likelihood_samples=False, reweight_samples=False,
+        **kwargs
     ):
         """Grab data from a result file containing posterior samples
 
         Parameters
         ----------
         file: str
             path to the result file
@@ -219,26 +250,29 @@
             If None, the read function loops through all possible options
         kwargs: dict
             Dictionary of keyword arguments fed to the
             `generate_all_posterior_samples` method
         """
         f = read_function(
             file, file_format=file_format, disable_prior=disable_prior_sampling,
-            path_to_samples=path_to_samples
+            nsamples_for_prior=nsamples_for_prior, path_to_samples=path_to_samples,
+            remove_nan_likelihood_samples=not keep_nan_likelihood_samples
         )
         if config is not None:
             f.add_fixed_parameters_from_config_file(config)
 
         if nsamples is not None:
             f.downsample(nsamples)
         f.generate_all_posterior_samples(**kwargs)
         if injection:
             f.add_injection_parameters_from_file(
                 injection, conversion_kwargs=kwargs
             )
+        if reweight_samples:
+            f.reweight_samples(reweight_samples)
         parameters = f.parameters
         samples = np.array(f.samples).T
         DataFrame = {label: SamplesDict(parameters, samples)}
         kwargs = f.extra_kwargs
         if hasattr(f, "injection_parameters"):
             injection = f.injection_parameters
             if injection is not None:
@@ -248,30 +282,88 @@
             else:
                 injection = {i: j for i, j in zip(
                     parameters, [float("nan")] * len(parameters))}
         else:
             injection = {i: j for i, j in zip(
                 parameters, [float("nan")] * len(parameters))}
         version = f.input_version
-        if hasattr(f, "priors"):
+        if hasattr(f, "priors") and f.priors is not None:
             priors = {key: {label: item} for key, item in f.priors.items()}
         else:
             priors = {label: []}
-        if hasattr(f, "weights"):
+        if hasattr(f, "weights") and f.weights is not None:
             weights = f.weights
         else:
             weights = None
-        return {
+        data = {
             "samples": DataFrame,
             "injection_data": {label: injection},
             "file_version": {label: version},
             "file_kwargs": {label: kwargs},
             "prior": priors,
-            "weights": {label: weights}
+            "weights": {label: weights},
+            "open_file": f,
+            "descriptions": {label: f.description}
         }
+        if hasattr(f, "config") and f.config is not None:
+            if config is None:
+                config_dir = os.path.join(webdir, "config")
+                filename = "{}_config.ini".format(label)
+                logger.debug(
+                    "Successfully extracted config data from the provided "
+                    "input file. Saving the data to the file '{}'".format(
+                        os.path.join(config_dir, filename)
+                    )
+                )
+                _filename = f.write(
+                    filename=filename, outdir=config_dir, file_format="ini",
+                    _raise=False
+                )
+                data["config"] = _filename
+            else:
+                logger.info(
+                    "Ignoring config data extracted from the input file and "
+                    "using the config file provided"
+                )
+        return data
+
+    @property
+    def result_files(self):
+        return self._result_files
+
+    @result_files.setter
+    def result_files(self, result_files):
+        self._result_files = result_files
+        if self._result_files is not None:
+            for num, ff in enumerate(self._result_files):
+                func = None
+                if not os.path.isfile(ff) and "@" in ff:
+                    from pesummary.io.read import _fetch_from_remote_server
+                    func = _fetch_from_remote_server
+                elif not os.path.isfile(ff) and "https://" in ff:
+                    from pesummary.io.read import _fetch_from_url
+                    func = _fetch_from_url
+                elif not os.path.isfile(ff) and "*" in ff:
+                    from pesummary.utils.utils import glob_directory
+                    func = glob_directory
+                if func is not None:
+                    _data = func(ff)
+                    if isinstance(_data, (np.ndarray, list)) and len(_data) > 0:
+                        self._result_files[num] = _data[0]
+                        if len(_data) > 1:
+                            _ = [
+                                self._result_files.insert(num + 1, d) for d in
+                                _data[1:][::-1]
+                            ]
+                    elif isinstance(_data, np.ndarray):
+                        raise InputError(
+                            "Unable to find any files matching '{}'".format(ff)
+                        )
+                    else:
+                        self._result_files[num] = _data
 
     @property
     def seed(self):
         return self._seed
 
     @seed.setter
     def seed(self, seed):
@@ -293,14 +385,16 @@
         return self._existing_metafile
 
     @existing_metafile.setter
     def existing_metafile(self, existing_metafile):
         from glob import glob
 
         self._existing_metafile = existing_metafile
+        if self._existing_metafile is None:
+            return
         if not os.path.isdir(os.path.join(self.existing, "samples")):
             raise InputError("Please provide a valid existing directory")
         _dir = os.path.join(self.existing, "samples")
         files = glob(os.path.join(_dir, "posterior_samples*"))
         dir_content = glob(os.path.join(_dir, "*.h5"))
         dir_content.extend(glob(os.path.join(_dir, "*.json")))
         dir_content.extend(glob(os.path.join(_dir, "*.hdf5")))
@@ -374,15 +468,15 @@
     @property
     def user(self):
         return self._user
 
     @user.setter
     def user(self, user):
         try:
-            self._user = os.environ["USER"]
+            self._user = getuser()
             logger.info(
                 conf.overwrite.format("user", conf.user, self._user)
             )
         except KeyError as e:
             logger.info(
                 "Failed to grab user information because {}. Default will be "
                 "used".format(e)
@@ -456,14 +550,22 @@
 
     @property
     def labels(self):
         return self._labels
 
     @labels.setter
     def labels(self, labels):
+        if self.result_files is not None:
+            if any(self.is_pesummary_metafile(s) for s in self.result_files):
+                logger.warning(
+                    "labels argument is ignored when a pesummary metafile is "
+                    "input. Stored analyses will use their stored labels. If "
+                    "you wish to change the labels, please use `summarymodify`"
+                )
+                labels = self.default_labels()
         if not hasattr(self, "._labels"):
             if labels is None:
                 labels = self.default_labels()
             elif self.mcmc_samples and len(labels) != 1:
                 raise InputError(
                     "Please provide a single label that corresponds to all "
                     "mcmc samples"
@@ -482,14 +584,32 @@
             if self.add_to_existing:
                 for i in labels:
                     if i in self.existing_labels:
                         raise InputError(
                             "The label '%s' already exists in the existing "
                             "metafile. Please pass another unique label"
                         )
+
+            if len(self.result_files) != len(labels) and not self.mcmc_samples:
+                import copy
+                _new_labels = copy.deepcopy(labels)
+                idx = 1
+                while len(_new_labels) < len(self.result_files):
+                    _new_labels.extend(
+                        [_label + str(idx) for _label in labels]
+                    )
+                    idx += 1
+                _new_labels = _new_labels[:len(self.result_files)]
+                logger.info(
+                    "You have passed {} result files and {} labels. Setting "
+                    "labels = {}".format(
+                        len(self.result_files), len(labels), _new_labels
+                    )
+                )
+                labels = _new_labels
             self._labels = labels
 
     @property
     def config(self):
         return self._config
 
     @config.setter
@@ -500,14 +620,17 @@
             )
         if config is None and not self.meta_file:
             self._config = [None] * len(self.labels)
         elif self.meta_file:
             self._config = [None] * len(self.labels)
         else:
             self._config = config
+        for num, ff in enumerate(self._config):
+            if isinstance(ff, str) and ff.lower() == "none":
+                self._config[num] = None
 
     @property
     def injection_file(self):
         return self._injection_file
 
     @injection_file.setter
     def injection_file(self, injection_file):
@@ -558,65 +681,73 @@
 
     @file_format.setter
     def file_format(self, file_format):
         if file_format is None:
             self._file_format = [None] * len(self.labels)
         elif len(file_format) == 1 and len(file_format) != len(self.labels):
             logger.warning(
-                "Only one file format specified. Assuming all files are of this format"
+                "Only one file format specified. Assuming all files are of "
+                "this format"
             )
-            self._file_format = [file_format] * len(self.labels)
+            self._file_format = [file_format[0]] * len(self.labels)
         elif len(file_format) != len(self.labels):
             raise InputError(
-                "Please provide a file format for each result file"
+                "Please provide a file format for each result file. If you "
+                "wish to specify the file format for the second result file "
+                "and not for any of the others, for example, simply pass 'None "
+                "{format} None'"
             )
         else:
+            for num, ff in enumerate(file_format):
+                if ff.lower() == "none":
+                    file_format[num] = None
             self._file_format = file_format
 
     @property
     def samples(self):
         return self._samples
 
     @samples.setter
     def samples(self, samples):
         if isinstance(samples, dict):
             return samples
         self._set_samples(samples)
 
     def _set_samples(
-        self, samples, ignore_keys=["prior", "weights", "labels", "indicies"]
+        self, samples,
+        ignore_keys=["prior", "weights", "labels", "indicies", "open_file"]
     ):
         """Extract the samples and store them as attributes of self
 
         Parameters
         ----------
         samples: list
             A list containing the paths to result files
         ignore_keys: list, optional
             A list containing properties of the read file that you do not want to be
             stored as attributes of self
         """
         if not samples:
             raise InputError("Please provide a results file")
-        if len(samples) != len(self.labels) and not self.mcmc_samples:
-            logger.info(
-                "You have passed {} result files and {} labels. Setting "
-                "labels = {}".format(
-                    len(samples), len(self.labels), self.labels[:len(samples)]
-                )
+        _samples_generator = (self.is_pesummary_metafile(s) for s in samples)
+        if any(_samples_generator) and not all(_samples_generator):
+            raise InputError(
+                "It seems that you have passed a combination of pesummary "
+                "metafiles and non-pesummary metafiles. This is currently "
+                "not supported."
             )
-            self.labels = self.labels[:len(samples)]
         labels, labels_dict = None, {}
         weights_dict = {}
         if self.mcmc_samples:
             nsamples = 0.
         for num, i in enumerate(samples):
             idx = num
             if not self.mcmc_samples:
-                logger.info("Assigning {} to {}".format(self.labels[num], i))
+                if not self.is_pesummary_metafile(samples[num]):
+                    logger.info("Assigning {} to {}".format(self.labels[num], i))
             else:
                 num = 0
             if not os.path.isfile(i):
                 raise InputError("File %s does not exist" % (i))
             if self.is_pesummary_metafile(samples[num]):
                 data = self.grab_data_from_input(
                     i, self.labels[num], config=None, injection=None
@@ -624,14 +755,25 @@
                 self.mcmc_samples = data["mcmc_samples"]
             else:
                 data = self.grab_data_from_input(
                     i, self.labels[num], config=self.config[num],
                     injection=self.injection_file[num],
                     file_format=self.file_format[num]
                 )
+                if "config" in data.keys():
+                    msg = (
+                        "Overwriting the provided config file for '{}' with "
+                        "the config information stored in the input "
+                        "file".format(self.labels[num])
+                    )
+                    if self.config[num] is None:
+                        logger.debug(msg)
+                    else:
+                        logger.info(msg)
+                    self.config[num] = data.pop("config")
                 if self.mcmc_samples:
                     data["samples"] = {
                         "{}_mcmc_chain_{}".format(key, idx): item for key, item
                         in data["samples"].items()
                     }
             for key, item in data.items():
                 if key not in ignore_keys:
@@ -694,16 +836,30 @@
                                     "{}/{}".format(key, label), pp[key][label]
                                 )
                     else:
                         self.add_to_prior_dict(
                             "samples/{}".format(label), []
                         )
             if "labels" in data.keys():
+                _duplicated = [
+                    _ for _ in data["labels"] if num != 0 and _ in labels
+                ]
                 if num == 0:
                     labels = data["labels"]
+                elif len(_duplicated):
+                    raise InputError(
+                        "The labels stored in the supplied files are not "
+                        "unique. The label{}: '{}' appear{} in two or more "
+                        "files. Please provide unique labels for each "
+                        "analysis.".format(
+                            "s" if len(_duplicated) > 1 else "",
+                            ", ".join(_duplicated),
+                            "" if len(_duplicated) > 1 else "s"
+                        )
+                    )
                 else:
                     labels += data["labels"]
                 labels_dict[num] = data["labels"]
         if self.mcmc_samples:
             try:
                 self.file_kwargs[self.labels[0]]["sampler"].update(
                     {"nsamples": nsamples, "nchains": len(self.result_files)}
@@ -831,14 +987,34 @@
         if nsamples is not None:
             logger.info(
                 "{} samples will be used for each result file".format(nsamples)
             )
             self._nsamples = int(nsamples)
 
     @property
+    def reweight_samples(self):
+        return self._reweight_samples
+
+    @reweight_samples.setter
+    def reweight_samples(self, reweight_samples):
+        from pesummary.core.reweight import options
+        self._reweight_samples = self._check_reweight_samples(
+            reweight_samples, options
+        )
+
+    def _check_reweight_samples(self, reweight_samples, options):
+        if reweight_samples and reweight_samples not in options.keys():
+            logger.warning(
+                "Unknown reweight function: '{}'. Not reweighting posterior "
+                "and/or prior samples".format(reweight_samples)
+            )
+            return False
+        return reweight_samples
+
+    @property
     def path_to_samples(self):
         return self._path_to_samples
 
     @path_to_samples.setter
     def path_to_samples(self, path_to_samples):
         self._path_to_samples = path_to_samples
         if path_to_samples is None:
@@ -928,14 +1104,79 @@
         if not self.hdf5 and hdf5_compression is not None:
             logger.warning(
                 "You can only apply compression when saving the meta "
                 "file in hdf5 format. Turning compression off."
             )
             self._hdf5_compression = None
 
+    @property
+    def existing_plot(self):
+        return self._existing_plot
+
+    @existing_plot.setter
+    def existing_plot(self, existing_plot):
+        self._existing_plot = existing_plot
+        if self._existing_plot is not None:
+            from pathlib import Path
+            import shutil
+            if isinstance(self._existing_plot, list):
+                logger.warning(
+                    "Assigning {} to all labels".format(
+                        ", ".join(self._existing_plot)
+                    )
+                )
+                self._existing_plot = {
+                    label: self._existing_plot for label in self.labels
+                }
+            _does_not_exist = (
+                "The plot {} does not exist. Not adding plot to summarypages."
+            )
+            keys_to_remove = []
+            for key, _plot in self._existing_plot.items():
+                if isinstance(_plot, list):
+                    allowed = []
+                    for _subplot in _plot:
+                        if not os.path.isfile(_subplot):
+                            logger.warning(_does_not_exist.format(_subplot))
+                        else:
+                            _filename = os.path.join(
+                                self.webdir, "plots", Path(_subplot).name
+                            )
+                            try:
+                                shutil.copyfile(_subplot, _filename)
+                            except shutil.SameFileError:
+                                pass
+                            allowed.append(_filename)
+                    if not len(allowed):
+                        keys_to_remove.append(key)
+                    elif len(allowed) == 1:
+                        self._existing_plot[key] = allowed[0]
+                    else:
+                        self._existing_plot[key] = allowed
+                else:
+                    if not os.path.isfile(_plot):
+                        logger.warning(_does_not_exist.format(_plot))
+                        keys_to_remove.append(key)
+                    else:
+                        _filename = os.path.join(
+                            self.webdir, "plots", Path(_plot).name
+                        )
+                        try:
+                            shutil.copyfile(_plot, _filename)
+                        except shutil.SameFileError:
+                            _filename = os.path.join(
+                                self.webdir, "plots", key + "_" + Path(_plot).name
+                            )
+                            shutil.copyfile(_plot, _filename)
+                        self._existing_plot[key] = _filename
+            for key in keys_to_remove:
+                del self._existing_plot[key]
+            if not len(self._existing_plot):
+                self._existing_plot = None
+
     def add_to_prior_dict(self, path, data):
         """Add priors to the prior dictionary
 
         Parameters
         ----------
         path: str
             the location where you wish to store the prior. If this is inside
@@ -991,53 +1232,66 @@
             path = path.split("/")
         else:
             path = [path]
         tree = build_tree(self._priors, path)
         nested_dictionary = get_nested_dictionary(self._priors, path[:-1])
         nested_dictionary[path[-1]] = data
 
-    def grab_priors_from_inputs(self, priors):
+    def grab_priors_from_inputs(self, priors, read_func=None, read_kwargs={}):
         """
         """
-        from pesummary.core.file.read import read as Read
+        if read_func is None:
+            from pesummary.core.file.read import read as Read
+            read_func = Read
 
         prior_dict = {}
         if priors is not None:
             prior_dict = {"samples": {}, "analytic": {}}
             for i in priors:
                 if not os.path.isfile(i):
                     raise InputError("The file {} does not exist".format(i))
             if len(priors) != len(self.labels) and len(priors) == 1:
                 logger.warning(
                     "You have only specified a single prior file for {} result "
                     "files. Assuming the same prior file for all result "
                     "files".format(len(self.labels))
                 )
-                data = Read(priors[0])
+                data = read_func(
+                    priors[0], nsamples=self.nsamples_for_prior
+                )
                 for i in self.labels:
                     prior_dict["samples"][i] = data.samples_dict
                     try:
-                        prior_dict["analytic"][i] = data.analytic
+                        if data.analytic is not None:
+                            prior_dict["analytic"][i] = data.analytic
                     except AttributeError:
                         continue
             elif len(priors) != len(self.labels):
                 raise InputError(
                     "Please provide a prior file for each result file"
                 )
             else:
                 for num, i in enumerate(priors):
                     if i.lower() == "none":
                         continue
                     logger.info(
                         "Assigning {} to {}".format(self.labels[num], i)
                     )
-                    data = Read(priors[num])
+                    if self.labels[num] in read_kwargs.keys():
+                        grab_data_kwargs = read_kwargs[self.labels[num]]
+                    else:
+                        grab_data_kwargs = read_kwargs
+                    data = read_func(
+                        priors[num], nsamples=self.nsamples_for_prior,
+                        **grab_data_kwargs
+                    )
                     prior_dict["samples"][self.labels[num]] = data.samples_dict
                     try:
-                        prior_dict["analytic"][self.labels[num]] = data.analytic
+                        if data.analytic is not None:
+                            prior_dict["analytic"][self.labels[num]] = data.analytic
                     except AttributeError:
                         continue
         return prior_dict
 
     @property
     def grab_data_kwargs(self):
         return {
@@ -1066,29 +1320,36 @@
         mcmc: Bool, optional
             if True, the result file is an mcmc chain
         """
         if label in self.grab_data_kwargs.keys():
             grab_data_kwargs = self.grab_data_kwargs[label]
         else:
             grab_data_kwargs = self.grab_data_kwargs
+
         if self.is_pesummary_metafile(file):
-            existing_data = self.grab_data_from_metafile(
+            data = self.grab_data_from_metafile(
                 file, self.webdir, compare=self.compare_results,
-                nsamples=self.nsamples, **grab_data_kwargs
+                nsamples=self.nsamples, reweight_samples=self.reweight_samples,
+                disable_injection=self.disable_injection,
+                keep_nan_likelihood_samples=self.keep_nan_likelihood_samples,
+                **grab_data_kwargs
             )
-            return existing_data
         else:
             data = self.grab_data_from_file(
-                file, label, config=config, injection=injection,
+                file, label, self.webdir, config=config, injection=injection,
                 file_format=file_format, nsamples=self.nsamples,
                 disable_prior_sampling=self.disable_prior_sampling,
+                nsamples_for_prior=self.nsamples_for_prior,
                 path_to_samples=self.path_to_samples[label],
+                reweight_samples=self.reweight_samples,
+                keep_nan_likelihood_samples=self.keep_nan_likelihood_samples,
                 **grab_data_kwargs
             )
-            return data
+        self._open_result_files.update({file: data["open_file"]})
+        return data
 
     @property
     def email(self):
         return self._email
 
     @email.setter
     def email(self, email):
@@ -1257,14 +1518,38 @@
             logger.debug(
                 "Using all parameters stored in the result file for the "
                 "corner plots. This may take some time."
             )
         return corner_params
 
     @property
+    def pe_algorithm(self):
+        return self._pe_algorithm
+
+    @pe_algorithm.setter
+    def pe_algorithm(self, pe_algorithm):
+        self._pe_algorithm = pe_algorithm
+        if pe_algorithm is None:
+            return
+        if len(pe_algorithm) != len(self.labels):
+            raise ValueError("Please provide an algorithm for each result file")
+        for num, (label, _algorithm) in enumerate(zip(self.labels, pe_algorithm)):
+            if "pe_algorithm" in self.file_kwargs[label]["sampler"].keys():
+                _stored = self.file_kwargs[label]["sampler"]["pe_algorithm"]
+                if _stored != _algorithm:
+                    logger.warning(
+                        "Overwriting the pe_algorithm extracted from the file "
+                        "'{}': {} with the algorithm provided from the command "
+                        "line: {}".format(
+                            self.result_files[num], _stored, _algorithm
+                        )
+                    )
+            self.file_kwargs[label]["sampler"]["pe_algorithm"] = _algorithm
+
+    @property
     def notes(self):
         return self._notes
 
     @notes.setter
     def notes(self, notes):
         self._notes = notes
         if notes is not None:
@@ -1283,14 +1568,135 @@
             except IOError as e:
                 logger.warning(
                     "Failed to read {}. Unable to put notes on "
                     "summarypages".format(notes)
                 )
 
     @property
+    def descriptions(self):
+        return self._descriptions
+
+    @descriptions.setter
+    def descriptions(self, descriptions):
+        import json
+        if hasattr(self, "_descriptions") and not len(descriptions):
+            return
+        elif not len(descriptions):
+            self._descriptions = None
+            return
+
+        if len(descriptions) and isinstance(descriptions, dict):
+            data = descriptions
+        elif len(descriptions):
+            descriptions = descriptions[0]
+        _is_file = not isinstance(descriptions, dict)
+        if hasattr(self, "_descriptions"):
+            logger.warning(
+                "Ignoring descriptions found in result file and using "
+                "descriptions in '{}'".format(descriptions)
+            )
+        self._descriptions = None
+        if _is_file and not os.path.isfile(descriptions):
+            logger.warning(
+                "No such file called {}. Unable to add descriptions".format(
+                    descriptions
+                )
+            )
+            return
+        if _is_file:
+            try:
+                with open(descriptions, "r") as f:
+                    data = json.load(f)
+            except json.decoder.JSONDecodeError:
+                logger.warning(
+                    "Unable to open file '{}'. Not storing descriptions".format(
+                        descriptions
+                    )
+                )
+                return
+        if not all(label in data.keys() for label in self.labels):
+            not_included = [
+                label for label in self.labels if label not in data.keys()
+            ]
+            logger.debug(
+                "No description found for '{}'. Using default "
+                "description".format(", ".join(not_included))
+            )
+            for label in not_included:
+                data[label] = "No description found"
+        if len(data.keys()) > len(self.labels):
+            logger.warning(
+                "Descriptions file contains descriptions for analyses other "
+                "than {}. Ignoring other descriptions".format(
+                    ", ".join(self.labels)
+                )
+            )
+            other = [
+                analysis for analysis in data.keys() if analysis not in
+                self.labels
+            ]
+            for analysis in other:
+                _ = data.pop(analysis)
+        _remove = []
+        for key, desc in data.items():
+            if not isinstance(desc, (str, bytes)):
+                logger.warning(
+                    "Unknown description '{}' for '{}'. The description should "
+                    "be a string or bytes object"
+                )
+                _remove.append(key)
+        if len(_remove):
+            for analysis in _remove:
+                _ = data.pop(analysis)
+        self._descriptions = data
+
+    @property
+    def preferred(self):
+        return self._preferred
+
+    @preferred.setter
+    def preferred(self, preferred):
+        if preferred is not None and preferred not in self.labels:
+            logger.warning(
+                "'{}' not in list of labels. Unable to stored as the "
+                "preferred analysis".format(preferred)
+            )
+            self._preferred = None
+        elif preferred is not None:
+            logger.debug(
+                "Setting '{}' as the preferred analysis".format(preferred)
+            )
+            self._preferred = preferred
+        elif len(self.labels) == 1:
+            self._preferred = self.labels[0]
+        else:
+            self._preferred = None
+        if self._preferred is not None:
+            try:
+                self.file_kwargs[self._preferred]["other"].update(
+                    {"preferred": "True"}
+                )
+            except KeyError:
+                self.file_kwargs[self._preferred].update(
+                    {"other": {"preferred": "True"}}
+                )
+        for _label in self.labels:
+            if self._preferred is not None and _label == self._preferred:
+                continue
+            try:
+                self.file_kwargs[_label]["other"].update(
+                    {"preferred": "False"}
+                )
+            except KeyError:
+                self.file_kwargs[_label].update(
+                    {"other": {"preferred": "False"}}
+                )
+        return
+
+    @property
     def public(self):
         return self._public
 
     @public.setter
     def public(self, public):
         self._public = public
         if public != conf.public:
@@ -1334,21 +1740,18 @@
         return self._ignore_parameters
 
     @ignore_parameters.setter
     def ignore_parameters(self, ignore_parameters):
         self._ignore_parameters = ignore_parameters
         if ignore_parameters is not None:
             for num, label in enumerate(self.labels):
-                removed_parameters = [
-                    param for param in list(self.samples[label].keys()) for
-                    ignore in ignore_parameters if re.match(
-                        re.compile(ignore), param
-                    )
-                ]
-                if removed_parameters == []:
+                removed_parameters = list_match(
+                    list(self.samples[label].keys()), ignore_parameters
+                )
+                if not len(removed_parameters):
                     logger.warning(
                         "Failed to remove any parameters from {}".format(
                             self.result_files[num]
                         )
                     )
                 else:
                     logger.warning(
@@ -1356,53 +1759,25 @@
                             ", ".join(removed_parameters),
                             self.result_files[num]
                         )
                     )
                     for ignore in removed_parameters:
                         self.samples[label].pop(ignore)
 
-    @property
-    def default_files_to_copy(self):
-        files_to_copy = []
-        path = pkg_resources.resource_filename("pesummary", "core")
-        scripts = glob(os.path.join(path, "js", "*.js"))
-        for i in scripts:
-            files_to_copy.append(
-                [i, os.path.join(self.webdir, "js", os.path.basename(i))]
-            )
-        scripts = glob(os.path.join(path, "css", "*.css"))
-        for i in scripts:
-            files_to_copy.append(
-                [i, os.path.join(self.webdir, "css", os.path.basename(i))]
-            )
-
-        if not all(i is None for i in self.config):
-            for num, i in enumerate(self.config):
-                if i is not None and self.webdir not in i:
-                    filename = "_".join(
-                        [self.labels[num], "config.ini"]
-                    )
-                    files_to_copy.append(
-                        [i, os.path.join(self.webdir, "config", filename)]
-                    )
-        return files_to_copy
-
     @staticmethod
     def _make_directories(webdir, dirs):
         """Make the directories to store the information
         """
         for i in dirs:
             if not os.path.isdir(os.path.join(webdir, i)):
                 make_dir(os.path.join(webdir, i))
 
     def make_directories(self):
         """Make the directories to store the information
         """
-        if self.publication:
-            self.default_directories.append("plots/publication")
         self._make_directories(self.webdir, self.default_directories)
 
     @staticmethod
     def _copy_files(paths):
         """Copy the relevant file to the web directory
 
         Parameters
@@ -1484,302 +1859,351 @@
         ], dtype=[(col, "S20") for col in headings]).view(np.recarray)
         return {
             "packages": packages, "environment": [package_dir],
             "manager": _package.package_manager
         }
 
 
-class Input(_Input):
-    """Class to handle the core command line arguments
-
-    Parameters
-    ----------
-    opts: argparse.Namespace
-        Namespace object containing the command line options
-
-    Attributes
-    ----------
-    result_files: list
-        list of result files passed
-    compare_results: list
-        list of labels stored in the metafile that you wish to compare
-    add_to_existing: Bool
-        True if we are adding to an existing web directory
-    existing_samples: dict
-        dictionary of samples stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    existing_injection_data: dict
-        dictionary of injection data stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    existing_file_version: dict
-        dictionary of file versions stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    existing_config: list
-        list of configuration files stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    existing_labels: list
-        list of labels stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    user: str
-        the user who submitted the job
-    webdir: str
-        the directory to store the webpages, plots and metafile produced
-    baseurl: str
-        the base url of the webpages
-    labels: list
-        list of labels used to distinguish the result files
-    config: list
-        list of configuration files for each result file
-    injection_file: list
-        list of injection files for each result file
-    publication: Bool
-        if true, publication quality plots are generated. Default False
-    kde_plot: Bool
-        if true, kde plots are generated instead of histograms. Default False
-    samples: dict
-        dictionary of posterior samples stored in the result files
-    priors: dict
-        dictionary of prior samples stored in the result files
-    custom_plotting: list
-        list containing the directory and name of python file which contains
-        custom plotting functions. Default None
-    email: str
-        the email address of the user
-    dump: Bool
-        if True, all plots will be dumped onto a single html page. Default False
-    hdf5: Bool
-        if True, the metafile is stored in hdf5 format. Default False
-    notes: str
-        notes that you wish to add to the webpages
-    disable_comparison: Bool
-        if True, comparison plots and pages are not produced
-    disable_interactive: Bool
-        if True, interactive plots are not produced
+class BaseInput(_Input):
+    """Class to handle and store base command line arguments
     """
-    def __init__(self, opts, ignore_copy=False, extra_options=None):
+    def __init__(self, opts, ignore_copy=False, checkpoint=None, gw=False):
         self.opts = opts
+        self.gw = gw
+        self.restart_from_checkpoint = self.opts.restart_from_checkpoint
+        if checkpoint is not None:
+            for key, item in vars(checkpoint).items():
+                setattr(self, key, item)
+            logger.info(
+                "Loaded command line arguments: {}".format(self.opts)
+            )
+            self.restart_from_checkpoint = True
+            self._restarted_from_checkpoint = True
+            return
         self.seed = self.opts.seed
-        self.style_file = self.opts.style_file
         self.result_files = self.opts.samples
+        self.user = self.opts.user
+        self.existing = self.opts.existing
+        self.add_to_existing = False
+        if self.existing is not None:
+            self.add_to_existing = True
+            self.existing_metafile = True
+        self.webdir = self.opts.webdir
+        self._restarted_from_checkpoint = False
+        self.resume_file_dir = conf.checkpoint_dir(self.webdir)
+        self.resume_file = conf.resume_file
+        self._resume_file_path = os.path.join(
+            self.resume_file_dir, self.resume_file
+        )
+        self.make_directories()
+        self.email = self.opts.email
+        self.pe_algorithm = self.opts.pe_algorithm
+        self.multi_process = self.opts.multi_process
+        self.package_information = self.get_package_information()
+        if not ignore_copy:
+            self.copy_files()
+        self.write_current_state()
+
+    @property
+    def default_directories(self):
+        return ["checkpoint"]
+
+    @property
+    def default_files_to_copy(self):
+        return []
+
+    def write_current_state(self):
+        """Write the current state of the input class to file
+        """
+        from pesummary.io import write
+        write(
+            self, outdir=self.resume_file_dir, file_format="pickle",
+            filename=self.resume_file, overwrite=True
+        )
+        logger.debug(
+            "Written checkpoint file: {}".format(self._resume_file_path)
+        )
+
+
+class SamplesInput(BaseInput):
+    """Class to handle and store sample specific command line arguments
+    """
+    def __init__(self, *args, extra_options=None, **kwargs):
+        """
+        """
+        super(SamplesInput, self).__init__(*args, **kwargs)
+        if self.result_files is not None:
+            self._open_result_files = {path: None for path in self.result_files}
         self.meta_file = False
         if self.result_files is not None and len(self.result_files) == 1:
             self.meta_file = self.is_pesummary_metafile(self.result_files[0])
-        self.existing = self.opts.existing
         self.compare_results = self.opts.compare_results
-        self.add_to_existing = False
+        self.disable_injection = self.opts.disable_injection
         if self.existing is not None:
-            self.add_to_existing = True
-            self.existing_metafile = None
             self.existing_data = self.grab_data_from_metafile(
                 self.existing_metafile, self.existing,
                 compare=self.compare_results
             )
             self.existing_samples = self.existing_data["samples"]
             self.existing_injection_data = self.existing_data["injection_data"]
             self.existing_file_version = self.existing_data["file_version"]
             self.existing_file_kwargs = self.existing_data["file_kwargs"]
             self.existing_priors = self.existing_data["prior"]
             self.existing_config = self.existing_data["config"]
             self.existing_labels = self.existing_data["labels"]
             self.existing_weights = self.existing_data["weights"]
         else:
+            self.existing_metafile = None
             self.existing_labels = None
             self.existing_weights = None
             self.existing_samples = None
             self.existing_file_version = None
             self.existing_file_kwargs = None
             self.existing_priors = None
             self.existing_config = None
             self.existing_injection_data = None
-        self.user = self.opts.user
-        self.webdir = self.opts.webdir
-        self.baseurl = self.opts.baseurl
-        self.filename = self.opts.filename
         self.mcmc_samples = self.opts.mcmc_samples
         self.labels = self.opts.labels
         self.weights = {i: None for i in self.labels}
         self.config = self.opts.config
         self.injection_file = self.opts.inj_file
-        self.publication = self.opts.publication
-        self.publication_kwargs = self.opts.publication_kwargs
-        self.default_directories = [
-            "samples", "plots", "js", "html", "css", "plots/corner", "config"
-        ]
-        self.make_directories()
         self.regenerate = self.opts.regenerate
         if extra_options is not None:
             for opt in extra_options:
                 setattr(self, opt, getattr(self.opts, opt))
-        self.kde_plot = self.opts.kde_plot
+        self.nsamples_for_prior = self.opts.nsamples_for_prior
         self.priors = self.opts.prior_file
         self.disable_prior_sampling = self.opts.disable_prior_sampling
         self.path_to_samples = self.opts.path_to_samples
         self.file_format = self.opts.file_format
         self.nsamples = self.opts.nsamples
+        self.keep_nan_likelihood_samples = self.opts.keep_nan_likelihood_samples
+        self.reweight_samples = self.opts.reweight_samples
         self.samples = self.opts.samples
         self.ignore_parameters = self.opts.ignore_parameters
         self.burnin_method = self.opts.burnin_method
         self.burnin = self.opts.burnin
+        self.same_parameters = []
+        if self.mcmc_samples:
+            self._samples = {label: self.samples.T for label in self.labels}
+        self.write_current_state()
+
+    @property
+    def analytic_prior_dict(self):
+        return {
+            label: "\n".join(
+                [
+                    "{} = {}".format(key, value) for key, value in
+                    self.priors["analytic"][label].items()
+                ]
+            ) if "analytic" in self.priors.keys() and label in
+            self.priors["analytic"].keys() else None for label in self.labels
+        }
+
+    @property
+    def same_parameters(self):
+        return self._same_parameters
+
+    @same_parameters.setter
+    def same_parameters(self, same_parameters):
+        self._same_parameters = self.intersect_samples_dict(self.samples)
+
+    def intersect_samples_dict(self, samples):
+        parameters = [
+            list(samples[key].keys()) for key in samples.keys()
+        ]
+        params = list(set.intersection(*[set(l) for l in parameters]))
+        return params
+
+    def grab_key_data_from_result_files(self):
+        """Grab the mean, median, maxL and standard deviation for all
+        parameters for all each result file
+        """
+        key_data = {
+            key: samples.key_data for key, samples in self.samples.items()
+        }
+        for key, val in self.samples.items():
+            for j in val.keys():
+                _inj = self.injection_data[key][j]
+                key_data[key][j]["injected"] = (
+                    _inj[0] if not math.isnan(_inj) and isinstance(
+                        _inj, (list, np.ndarray)
+                    ) else _inj
+                )
+        return key_data
+
+
+class PlottingInput(SamplesInput):
+    """Class to handle and store plotting specific command line arguments
+    """
+    def __init__(self, *args, **kwargs):
+        super(PlottingInput, self).__init__(*args, **kwargs)
+        self.style_file = self.opts.style_file
+        self.publication = self.opts.publication
+        self.publication_kwargs = self.opts.publication_kwargs
+        self.kde_plot = self.opts.kde_plot
         self.custom_plotting = self.opts.custom_plotting
         self.add_to_corner = self.opts.add_to_corner
-        self.email = self.opts.email
-        self.dump = self.opts.dump
-        self.hdf5 = not self.opts.save_to_json
-        self.external_hdf5_links = self.opts.external_hdf5_links
-        self.hdf5_compression = self.opts.hdf5_compression
+        self.corner_params = self.add_to_corner
         self.palette = self.opts.palette
         self.include_prior = self.opts.include_prior
         self.colors = self.opts.colors
         self.linestyles = self.opts.linestyles
         self.disable_corner = self.opts.disable_corner
-        self.notes = self.opts.notes
         self.disable_comparison = self.opts.disable_comparison
         self.disable_interactive = self.opts.disable_interactive
-        self.multi_process = self.opts.multi_process
+        self.disable_expert = not self.opts.enable_expert
         self.multi_threading_for_plots = self.multi_process
-        self.package_information = self.get_package_information()
-        if not ignore_copy:
-            self.copy_files()
+        self.write_current_state()
 
+    @property
+    def default_directories(self):
+        dirs = super(PlottingInput, self).default_directories
+        dirs += ["plots", "plots/corner", "plots/publication"]
+        return dirs
 
-class PostProcessing(object):
-    """Super class to post process the input data
 
-    Parameters
-    ----------
-    inputs: argparse.Namespace
-        Namespace object containing the command line options
-    colors: list, optional
-        colors that you wish to use to distinguish different result files
+class WebpageInput(SamplesInput):
+    """Class to handle and store webpage specific command line arguments
+    """
+    def __init__(self, *args, **kwargs):
+        super(WebpageInput, self).__init__(*args, **kwargs)
+        self.baseurl = self.opts.baseurl
+        self.existing_plot = self.opts.existing_plot
+        self.pe_algorithm = self.opts.pe_algorithm
+        self.notes = self.opts.notes
+        self.dump = self.opts.dump
+        self.hdf5 = not self.opts.save_to_json
+        self.external_hdf5_links = self.opts.external_hdf5_links
+        self.file_kwargs["webpage_url"] = self.baseurl + "/home.html"
+        self.write_current_state()
 
-    Attributes
-    ----------
-    result_files: list
-        list of result files passed
-    compare_results: list
-        list of labels stored in the metafile that you wish to compare
-    add_to_existing: Bool
-        True if we are adding to an existing web directory
-    existing_samples: dict
-        dictionary of samples stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    existing_injection_data: dict
-        dictionary of injection data stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    existing_file_version: dict
-        dictionary of file versions stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    existing_config: list
-        list of configuration files stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    existing_labels: list
-        list of labels stored in an existing metafile. None if
-        `self.add_to_existing` is False
-    user: str
-        the user who submitted the job
-    webdir: str
-        the directory to store the webpages, plots and metafile produced
-    baseurl: str
-        the base url of the webpages
-    labels: list
-        list of labels used to distinguish the result files
-    config: list
-        list of configuration files for each result file
-    injection_file: list
-        list of injection files for each result file
-    publication: Bool
-        if true, publication quality plots are generated. Default False
-    kde_plot: Bool
-        if true, kde plots are generated instead of histograms. Default False
-    samples: dict
-        dictionary of posterior samples stored in the result files
-    priors: dict
-        dictionary of prior samples stored in the result files
-    custom_plotting: list
-        list containing the directory and name of python file which contains
-        custom plotting functions. Default None
-    email: str
-        the email address of the user
-    dump: Bool
-        if True, all plots will be dumped onto a single html page. Default False
-    hdf5: Bool
-        if True, the metafile is stored in hdf5 format. Default False
-    same_parameters: dict
-        list of parameters that are common in all result files
-    disable_comparison: bool
-        whether or not to make comparison plots/pages when mutiple results
-        files are present
+    @property
+    def default_directories(self):
+        dirs = super(WebpageInput, self).default_directories
+        dirs += ["js", "html", "css"]
+        return dirs
+
+    @property
+    def default_files_to_copy(self):
+        files_to_copy = super(WebpageInput, self).default_files_to_copy
+        path = pkg_resources.resource_filename("pesummary", "core")
+        scripts = glob(os.path.join(path, "js", "*.js"))
+        for i in scripts:
+            files_to_copy.append(
+                [i, os.path.join(self.webdir, "js", os.path.basename(i))]
+            )
+        scripts = glob(os.path.join(path, "css", "*.css"))
+        for i in scripts:
+            files_to_copy.append(
+                [i, os.path.join(self.webdir, "css", os.path.basename(i))]
+            )
+        return files_to_copy
+
+
+class WebpagePlusPlottingInput(PlottingInput, WebpageInput):
+    """Class to handle and store webpage and plotting specific command line
+    arguments
     """
-    def __init__(self, inputs, colors="default"):
-        self.inputs = inputs
-        self.filename = self.inputs.filename
-        self.result_files = self.inputs.result_files
-        self.existing = self.inputs.existing
-        self.compare_results = self.inputs.compare_results
-        self.add_to_existing = False
-        if self.existing is not None:
-            self.add_to_existing = True
-            self.existing_metafile = self.inputs.existing_metafile
-            self.existing_samples = self.inputs.existing_samples
-            self.existing_injection_data = self.inputs.existing_injection_data
-            self.existing_file_version = self.inputs.existing_file_version
-            self.existing_file_kwargs = self.inputs.existing_file_kwargs
-            self.existing_priors = self.inputs.existing_priors
-            self.existing_config = self.inputs.existing_config
-            self.existing_labels = self.inputs.existing_labels
-            self.existing_weights = self.inputs.existing_weights
-        else:
-            self.existing_metafile = None
-            self.existing_labels = None
-            self.existing_weights = None
-            self.existing_samples = None
-            self.existing_file_version = None
-            self.existing_file_kwargs = None
-            self.existing_priors = None
-            self.existing_config = None
-            self.existing_injection_data = None
-        self.user = self.inputs.user
-        self.host = self.inputs.host
-        self.webdir = self.inputs.webdir
-        self.baseurl = self.inputs.baseurl
-        self.mcmc_samples = self.inputs.mcmc_samples
-        self.labels = self.inputs.labels
-        self.weights = self.inputs.weights
-        self.config = self.inputs.config
-        self.injection_file = self.inputs.injection_file
-        self.injection_data = self.inputs.injection_data
-        self.publication = self.inputs.publication
-        self.kde_plot = self.inputs.kde_plot
-        self.samples = self.inputs.samples
-        self.priors = self.inputs.priors
-        self.custom_plotting = self.inputs.custom_plotting
-        self.corner_params = self.inputs.add_to_corner
-        self.email = self.inputs.email
-        self.dump = self.inputs.dump
-        self.hdf5 = self.inputs.hdf5
-        self.external_hdf5_links = self.inputs.external_hdf5_links
-        self.hdf5_compression = self.inputs.hdf5_compression
-        self.file_version = self.inputs.file_version
-        self.file_kwargs = self.inputs.file_kwargs
-        self.palette = self.inputs.palette
-        self.colors = self.inputs.colors
-        self.linestyles = self.inputs.linestyles
-        self.include_prior = self.inputs.include_prior
-        self.notes = self.inputs.notes
-        self.disable_comparison = self.inputs.disable_comparison
-        self.disable_interactive = self.inputs.disable_interactive
-        self.disable_corner = self.inputs.disable_corner
-        self.multi_process = self.inputs.multi_threading_for_plots
-        self.package_information = self.inputs.package_information
-        self.same_parameters = []
-        if self.mcmc_samples:
-            self.samples = {label: self.samples.T for label in self.labels}
+    def __init__(self, *args, **kwargs):
+        super(WebpagePlusPlottingInput, self).__init__(*args, **kwargs)
+        self.copy_files()
 
     @property
-    def same_parameters(self):
-        return self._same_parameters
+    def default_directories(self):
+        return super(WebpagePlusPlottingInput, self).default_directories
 
-    @same_parameters.setter
-    def same_parameters(self, same_parameters):
-        parameters = [list(self.samples[key]) for key in self.samples.keys()]
-        params = list(set.intersection(*[set(l) for l in parameters]))
-        self._same_parameters = params
+    @property
+    def default_files_to_copy(self):
+        return super(WebpagePlusPlottingInput, self).default_files_to_copy
+
+
+class MetaFileInput(SamplesInput):
+    """Class to handle and store metafile specific command line arguments
+    """
+    def __init__(self, *args, **kwargs):
+        kwargs.update({"ignore_copy": True})
+        super(MetaFileInput, self).__init__(*args, **kwargs)
+        self.copy_files()
+        self.filename = self.opts.filename
+        self.hdf5 = not self.opts.save_to_json
+        self.hdf5_compression = self.opts.hdf5_compression
+        self.external_hdf5_links = self.opts.external_hdf5_links
+        self.descriptions = self.opts.descriptions
+        self.preferred = self.opts.preferred
+        self.write_current_state()
+
+    @property
+    def default_directories(self):
+        dirs = super(MetaFileInput, self).default_directories
+        dirs += ["samples", "config"]
+        return dirs
+
+    @property
+    def default_files_to_copy(self):
+        files_to_copy = super(MetaFileInput, self).default_files_to_copy
+        if not all(i is None for i in self.config):
+            for num, i in enumerate(self.config):
+                if i is not None and self.webdir not in i:
+                    filename = "_".join(
+                        [self.labels[num], "config.ini"]
+                    )
+                    files_to_copy.append(
+                        [i, os.path.join(self.webdir, "config", filename)]
+                    )
+        for num, _file in enumerate(self.result_files):
+            if not self.mcmc_samples:
+                filename = "{}_{}".format(self.labels[num], Path(_file).name)
+            else:
+                filename = "chain_{}_{}".format(num, Path(_file).name)
+            files_to_copy.append(
+                [_file, os.path.join(self.webdir, "samples", filename)]
+            )
+        return files_to_copy
+
+
+class WebpagePlusPlottingPlusMetaFileInput(MetaFileInput, WebpagePlusPlottingInput):
+    """Class to handle and store webpage, plotting and metafile specific command
+    line arguments
+    """
+    def __init__(self, *args, **kwargs):
+        super(WebpagePlusPlottingPlusMetaFileInput, self).__init__(
+            *args, **kwargs
+        )
+
+    @property
+    def default_directories(self):
+        return super(WebpagePlusPlottingPlusMetaFileInput, self).default_directories
+
+    @property
+    def default_files_to_copy(self):
+        return super(WebpagePlusPlottingPlusMetaFileInput, self).default_files_to_copy
+
+
+@deprecation(
+    "The Input class is deprecated. Please use either the BaseInput, "
+    "SamplesInput, PlottingInput, WebpageInput, WebpagePlusPlottingInput, "
+    "MetaFileInput or the WebpagePlusPlottingPlusMetaFileInput class"
+)
+class Input(WebpagePlusPlottingPlusMetaFileInput):
+    pass
+
+
+def load_current_state(resume_file):
+    """Load a pickle file containing checkpoint information
+
+    Parameters
+    ----------
+    resume_file: str
+        path to a checkpoint file
+    """
+    from pesummary.io import read
+    if not os.path.isfile(resume_file):
+        logger.info(
+            "Unable to find resume file. Not restarting from checkpoint"
+        )
+        return
+    logger.info(
+        "Reading checkpoint file: {}".format(resume_file)
+    )
+    state = read(resume_file, checkpoint=True)
+    return state
```

### Comparing `pesummary-0.9.1/pesummary/core/notebook/notebook.py` & `pesummary-1.0.0/pesummary/core/notebook/notebook.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,28 +1,17 @@
-# Copyright (C) 2020  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 try:
     import nbformat as nbf
 except ImportError:
     raise ImportError("'nbformat' is required for this module")
 import os
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class NoteBook(object):
     """Class to handle the creation of a jupyter notebook
 
     Attributes
     ----------
     cells: dict
```

### Comparing `pesummary-0.9.1/pesummary/core/finish.py` & `pesummary-1.0.0/pesummary/core/webpage/base.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,72 +1,102 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-
-import subprocess
-import os
-
-from pesummary.utils.utils import logger
-from pesummary.core.inputs import PostProcessing
-
-
-class FinishingTouches(PostProcessing):
-    """Class to handle the finishing touches
-
-    Parameters
-    ----------
-    parser: argparser
-        The parser containing the command line arguments
+# Licensed under an MIT style license -- see LICENSE.md
+
+import numpy as np
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
+
+class Base(object):
+    """Meta class containing helper functions for generating webpages
     """
-    def __init__(self, inputs, **kwargs):
-        super(FinishingTouches, self).__init__(inputs)
-        self.send_email()
-        logger.info("Complete. Webpages can be viewed at the following url "
-                    "%s" % (self.baseurl + "/home.html"))
-
-    def send_email(self, message=None):
-        """Send notification email.
-        """
-        if self.email:
-            logger.info("Sending email to %s" % (self.email))
-            try:
-                self._email_notify(message)
-            except Exception as e:
-                logger.info("Unable to send notification email because %s" % (
-                    e))
-
-    def _email_message(self, message=None):
-        """Message that will be send in the email.
-        """
-        if not message:
-            message = ("Hi %s,\n\nYour output page is ready on %s. You can "
-                       "view the result at %s \n"
-                       % (self.user, self.host, self.baseurl + "/home.html"))
-        return message
-
-    def _email_notify(self, message):
-        """Subprocess to send the notification email.
-        """
-        subject = "Output page available at %s" % (self.host)
-        message = self._email_message(message)
-        cmd = 'echo -e "%s" | mail -s "%s" "%s"' % (
-            message, subject, self.email)
-        ess = subprocess.Popen(cmd, shell=True)
-        ess.wait()
+    def close(self):
+        """Close the opened html file.
+        """
+        self.html_file.close()
+
+    def _check_content(self, content):
+        """Make sure that the content has new line in string
 
-    def remove_tmp_directories(self):
-        """Remove the temp directories created by PESummary
+        Parameters
+        ----------
+        content: str
+            string that you want to check
         """
-        from pesummary.utils import utils
+        if len(content) and content[-1] != "\n":
+            content += "\n"
+        return content
+
+    def add_content(self, content, indent=0):
+        """Add content to the html page
+
+        Parameters
+        ----------
+        content: str/list, optional
+            either a single string or a list of string that you want to add to
+            your html page
+        indent: int, optional
+            the indent of the line
+        """
+        if type(content) == list:
+            for i in np.arange(len(content)):
+                content[i] == self._check_content(content[i])
+                self.html_file.write(" " * indent + content)
+        else:
+            content = self._check_content(content)
+            self.html_file.write(" " * indent + content)
+
+    def make_div(self, indent=0, _class=None, _style=None, _id=None):
+        """Make a div of your choice
+
+        indent: int, optional
+            the indent of the line
+        _class: str, optional
+            the class name of your div
+        _style: str, optional
+            the style of your div
+        _id: str, optional
+            the id of your div
+        """
+        string = "<div"
+        if _class:
+            string += " class='%s'" % (_class)
+        if _style:
+            string += " style='%s'" % (_style)
+        if _id is not None:
+            string += " id='%s'" % (_id)
+        string += ">\n"
+        self.add_content(string, indent=indent)
+
+    def end_div(self, indent=0):
+        """End a div of your choice
+
+        Parameters
+        ----------
+        indent: int, optional
+            the indent of the new line
+        """
+        self.add_content("</div>", indent)
 
-        utils.remove_tmp_directories()
+    def make_container(self, style=None, indent=0, display=None, container_id=None):
+        """Make a container for your webpage
+
+        Parameters
+        ----------
+        indent: int, optional
+            the indent of the new line
+        """
+        if not style:
+            style = "margin-top:3em; margin-bottom:5em; background-color:#FFFFFF; " + \
+                    "box-shadow: 0 0 5px grey; max-width: 1400px"
+        if display is not None:
+            style += "; display:{}".format(display)
+        self.make_div(indent, _class="container", _style=style, _id=container_id)
+
+    def end_container(self, indent=0):
+        """End a container
+
+         Parameters
+         ----------
+         indent: int, optional
+             the indent of the new line
+         """
+        self.end_div(indent)
```

### Comparing `pesummary-0.9.1/pesummary/core/plots/plot.py` & `pesummary-1.0.0/pesummary/core/plots/plot.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,39 +1,25 @@
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from pesummary.utils.utils import (
-    logger, number_of_columns_for_legend, _check_latex_install,
-    get_matplotlib_style_file, gelman_rubin,
+    logger, number_of_columns_for_legend, _check_latex_install, gelman_rubin,
 )
-from pesummary.core.plots.kde import kdeplot
-from pesummary.core.plots.figure import figure, subplots, ExistingFigure
+from pesummary.core.plots.seaborn.kde import kdeplot
+from pesummary.core.plots.figure import figure, ExistingFigure
 from pesummary import conf
 
-import matplotlib.style
 import matplotlib.lines as mlines
 import corner
 import copy
 from itertools import cycle
 
 import numpy as np
 from scipy import signal
 
-matplotlib.style.use(get_matplotlib_style_file())
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 _check_latex_install()
 
 _default_legend_kwargs = dict(
     bbox_to_anchor=(0.0, 1.02, 1.0, 0.102), loc=3, handlelength=3, mode="expand",
     borderaxespad=0.0,
 )
 
@@ -53,33 +39,38 @@
     fig: matplotlib.pyplot.figure
         existing figure you wish to use
     color: str, optional
         color you wish to use for the autocorrelation plot
     grid: Bool, optional
         if True, plot a grid
     """
+    import warnings
+    warnings.filterwarnings("ignore", category=RuntimeWarning)
     logger.debug("Generating the autocorrelation function for %s" % (param))
     if fig is None:
         fig, ax = figure(gca=True)
     else:
         ax = fig.gca()
     samples = samples[int(len(samples) / 2):]
     x = samples - np.mean(samples)
     y = np.conj(x[::-1])
     acf = np.fft.ifftshift(signal.fftconvolve(y, x, mode="full"))
     N = np.array(samples).shape[0]
     acf = acf[0:N]
+    # Hack to make test pass with python3.8
+    if color == "$":
+        color = conf.color
     ax.plot(
         acf / acf[0], linestyle=" ", marker="o", markersize=markersize,
         color=color
     )
     ax.ticklabel_format(axis="x", style="plain")
     ax.set_xlabel("lag")
     ax.set_ylabel("ACF")
-    ax.grid(b=grid)
+    ax.grid(visible=grid)
     fig.tight_layout()
     return fig
 
 
 def _autocorrelation_plot_mcmc(
     param, samples, colorcycle=conf.colorcycle, grid=True
 ):
@@ -104,15 +95,15 @@
             param, ss, fig=fig, markersize=1.25, color=next(cycol), grid=grid
         )
     return fig
 
 
 def _sample_evolution_plot(
     param, samples, latex_label, inj_value=None, fig=None, color=conf.color,
-    markersize=0.5, grid=True
+    markersize=0.5, grid=True, z=None, z_label=None, **kwargs
 ):
     """Generate a scatter plot showing the evolution of the samples for a
     given parameter for a given approximant.
 
     Parameters
     ----------
     param: str
@@ -132,22 +123,29 @@
     """
     logger.debug("Generating the sample scatter plot for %s" % (param))
     if fig is None:
         fig, ax = figure(gca=True)
     else:
         ax = fig.gca()
     n_samples = len(samples)
-    ax.plot(
-        range(n_samples), samples, linestyle=" ", marker="o",
-        markersize=markersize, color=color
+    add_cbar = True if z is not None else False
+    if z is None:
+        z = color
+    s = ax.scatter(
+        range(n_samples), samples, marker="o", s=markersize, c=z,
+        **kwargs
     )
+    if add_cbar:
+        cbar = fig.colorbar(s)
+        if z_label is not None:
+            cbar.set_label(z_label)
     ax.ticklabel_format(axis="x", style="plain")
     ax.set_xlabel("samples")
     ax.set_ylabel(latex_label)
-    ax.grid(b=grid)
+    ax.grid(visible=grid)
     fig.tight_layout()
     return fig
 
 
 def _sample_evolution_plot_mcmc(
     param, samples, latex_label, inj_value=None, colorcycle=conf.colorcycle,
     grid=True
@@ -178,15 +176,15 @@
             color=next(cycol), grid=grid
         )
     return fig
 
 
 def _1d_cdf_plot(
     param, samples, latex_label, fig=None, color=conf.color, title=True,
-    grid=True
+    grid=True, linestyle="-", **kwargs
 ):
     """Generate the cumulative distribution function for a given parameter for
     a given approximant.
 
     Parameters
     ----------
     param: str
@@ -200,14 +198,18 @@
     color: str, optional09
         color you wish to use to plot the scatter points
     title: Bool, optional
         if True, add a title to the 1d cdf plot showing giving the median
         and symmetric 90% credible intervals
     grid: Bool, optional
         if True, plot a grid
+    linestyle: str, optional
+        linestyle to use for plotting the CDF. Default "-"
+    **kwargs: dict, optional
+        all additional kwargs passed to ax.plot
     """
     logger.debug("Generating the 1d CDF for %s" % (param))
     if fig is None:
         fig, ax = figure(gca=True)
     else:
         ax = fig.gca()
     sorted_samples = copy.deepcopy(samples)
@@ -218,23 +220,26 @@
     lower_percentile = np.percentile(samples, 5)
     median = np.median(samples)
     upper = np.round(upper_percentile - median, 2)
     lower = np.round(median - lower_percentile, 2)
     median = np.round(median, 2)
     if title:
         ax.set_title(r"$%s^{+%s}_{-%s}$" % (median, upper, lower))
-    ax.plot(sorted_samples, np.linspace(0, 1, len(sorted_samples)), color=color)
-    ax.grid(b=grid)
+    ax.plot(
+        sorted_samples, np.linspace(0, 1, len(sorted_samples)), color=color,
+        linestyle=linestyle, **kwargs
+    )
+    ax.grid(visible=grid)
     ax.set_ylim([0, 1.05])
     fig.tight_layout()
     return fig
 
 
 def _1d_cdf_plot_mcmc(
-    param, samples, latex_label, colorcycle=conf.colorcycle, grid=True
+    param, samples, latex_label, colorcycle=conf.colorcycle, grid=True, **kwargs
 ):
     """Generate the cumulative distribution function for a given parameter
     for a given set of mcmc chains
 
     Parameters
     ----------
     param: str
@@ -243,30 +248,32 @@
         2d array containing the samples for param for each mcmc chain
     latex_label: str
         latex label for param
     colorcycle: list, str
         color cycle you wish to use for the different mcmc chains
     grid: Bool, optional
         if True, plot a grid
+    **kwargs: dict, optional
+        all additional kwargs passed to _1d_cdf_plot
     """
     cycol = cycle(colorcycle)
     fig, ax = figure(gca=True)
     for ss in samples:
         fig = _1d_cdf_plot(
             param, ss, latex_label, fig=fig, color=next(cycol), title=False,
-            grid=grid
+            grid=grid, **kwargs
         )
     gelman = gelman_rubin(samples)
     ax.set_title("Gelman-Rubin: {}".format(gelman))
     return fig
 
 
 def _1d_cdf_comparison_plot(
     param, samples, colors, latex_label, labels, linestyles=None, grid=True,
-    legend_kwargs=_default_legend_kwargs, latex_friendly=False
+    legend_kwargs=_default_legend_kwargs, latex_friendly=False, **kwargs
 ):
     """Generate a plot to compare the cdfs for a given parameter for different
     approximants.
 
     Parameters
     ----------
     param: str
@@ -283,181 +290,362 @@
         label to prepend the approximant in the legend
     grid: Bool, optional
         if True, plot a grid
     legend_kwargs: dict, optional
         optional kwargs to pass to ax.legend()
     latex_friendly: Bool, optional
         if True, make the label latex friendly. Default False
+    **kwargs: dict, optional
+        all additional kwargs passed to _1d_cdf_plot
     """
     logger.debug("Generating the 1d comparison CDF for %s" % (param))
     if linestyles is None:
         linestyles = ["-"] * len(samples)
     fig, ax = figure(figsize=(8, 6), gca=True)
     handles = []
     for num, i in enumerate(samples):
         fig = _1d_cdf_plot(
             param, i, latex_label, fig=fig, color=colors[num], title=False,
-            grid=grid
+            grid=grid, linestyle=linestyles[num], **kwargs
         )
         if latex_friendly:
             labels = copy.deepcopy(labels)
             labels[num] = labels[num].replace("_", "\_")
         handles.append(mlines.Line2D([], [], color=colors[num], label=labels[num]))
     ncols = number_of_columns_for_legend(labels)
     legend = ax.legend(handles=handles, ncol=ncols, **legend_kwargs)
     for num, legobj in enumerate(legend.legendHandles):
         legobj.set_linewidth(1.75)
         legobj.set_linestyle(linestyles[num])
     ax.set_xlabel(latex_label)
     ax.set_ylabel("Cumulative Density Function")
-    ax.grid(b=grid)
+    ax.grid(visible=grid)
     ax.set_ylim([0, 1.05])
     fig.tight_layout()
     return fig
 
 
+def _1d_analytic_plot(
+    param, x, pdf, latex_label, inj_value=None, prior=None, fig=None, ax=None,
+    title=True, color=conf.color, autoscale=True, grid=True, set_labels=True,
+    plot_percentile=True, xlims=None, label=None, linestyle="-",
+    linewidth=1.75, injection_color=conf.injection_color,
+    _default_inj_kwargs={"linewidth": 2.5, "linestyle": "-"}, **plot_kwargs
+):
+    """Generate a plot to display a PDF
+
+    Parameters
+    ----------
+    param: str
+        name of the parameter that you wish to plot
+
+    latex_label: str
+        latex label for param
+    inj_value: float, optional
+        value that was injected
+    prior: list
+        list of prior samples for param
+    weights: list
+        list of weights for each sample
+    fig: matplotlib.pyplot.figure, optional
+        existing figure you wish to use
+    ax: matplotlib.pyplot.axes._subplots.AxesSubplot, optional
+        existing axis you wish to use
+    color: str, optional
+        color you wish to use to plot the scatter points
+    title: Bool, optional
+        if True, add a title to the 1d cdf plot showing giving the median
+        and symmetric 90% credible intervals
+    autoscale: Bool, optional
+        autoscale the x axis
+    grid: Bool, optional
+        if True, plot a grid
+    set_labels: Bool, optional
+        if True, add labels to the axes
+    plot_percentile: Bool, optional
+        if True, plot dashed vertical lines showing the 90% symmetric credible
+        intervals
+    xlims: list, optional
+        x axis limits you wish to use
+    label: str, optional
+        label you wish to use for the plot
+    linestyle: str, optional
+        linestyle you wish to use for the plot
+    linewidth: float, optional
+        linewidth to use for the plot
+    injection_color: str, optional
+        color of vertical line showing the injected value
+    """
+    from pesummary.utils.array import Array
+
+    if ax is None and fig is None:
+        fig, ax = figure(gca=True)
+    elif ax is None:
+        ax = fig.gca()
+
+    pdf = Array(x, weights=pdf)
+
+    ax.plot(pdf, pdf.weights, color=color, linestyle=linestyle, label=label)
+    _xlims = ax.get_xlim()
+    percentile = pdf.confidence_interval([5, 95])
+    median = pdf.average("median")
+    if title:
+        upper = np.round(percentile[1] - median, 2)
+        lower = np.round(median - percentile[0], 2)
+        median = np.round(median, 2)
+        ax.set_title(r"$%s^{+%s}_{-%s}$" % (median, upper, lower))
+    if plot_percentile:
+        for pp in percentile:
+            ax.axvline(
+                pp, color=color, linestyle="--", linewidth=linewidth
+            )
+    if set_labels:
+        ax.set_xlabel(latex_label)
+        ax.set_ylabel("Probability Density")
+
+    if inj_value is not None:
+        ax.axvline(
+            inj_value, color=injection_color, **_default_inj_kwargs
+        )
+    ax.grid(visible=grid)
+    ax.set_xlim(xlims)
+    if autoscale:
+        ax.set_xlim(_xlims)
+    if fig is None:
+        return ax
+    fig.tight_layout()
+    return fig
+
+
 def _1d_histogram_plot(
-    param, samples, latex_label, inj_value=None, kde=False, prior=None,
-    weights=None, xlow=None, xhigh=None, fig=None, title=True, color=conf.color,
-    autoscale=True, bins=50, histtype="step", grid=True
+    param, samples, latex_label, inj_value=None, kde=False, hist=True,
+    prior=None, weights=None, fig=None, ax=None, title=True, color=conf.color,
+    autoscale=True, grid=True, kde_kwargs={}, hist_kwargs={}, set_labels=True,
+    plot_percentile=True, xlims=None, max_vline=1, label=None, linestyle="-",
+    injection_color=conf.injection_color, _default_hist_kwargs={
+        "density": True, "bins": 50, "histtype": "step", "linewidth": 1.75
+    }, _default_kde_kwargs={"shade": True, "alpha_shade": 0.1},
+    _default_inj_kwargs={"linewidth": 2.5, "linestyle": "-"}, **plot_kwargs
 ):
     """Generate the 1d histogram plot for a given parameter for a given
     approximant.
 
     Parameters
     ----------
     param: str
         name of the parameter that you wish to plot
     samples: list
         list of samples for param
     latex_label: str
         latex label for param
-    inj_value: float
+    inj_value: float, optional
         value that was injected
-    kde: Bool
-        if true, a kde is plotted instead of a histogram
+    kde: Bool, optional
+        if True, a kde is plotted instead of a histogram
+    hist: Bool, optional
+        if True, plot a histogram
     prior: list
         list of prior samples for param
     weights: list
         list of weights for each sample
     fig: matplotlib.pyplot.figure, optional
         existing figure you wish to use
-    color: str, optional09
+    ax: matplotlib.pyplot.axes._subplots.AxesSubplot, optional
+        existing axis you wish to use
+    color: str, optional
         color you wish to use to plot the scatter points
     title: Bool, optional
         if True, add a title to the 1d cdf plot showing giving the median
         and symmetric 90% credible intervals
     autoscale: Bool, optional
         autoscale the x axis
-    bins: int, optional
-        number of bins to use for histogram
-    histtype: str, optional
-        histogram type to use when plotting
     grid: Bool, optional
         if True, plot a grid
+    kde_kwargs, dict, optional
+        optional kwargs to pass to the kde class
+    hist_kwargs: dict, optional
+        optional kwargs to pass to matplotlib.pyplot.hist
+    set_labels: Bool, optional
+        if True, add labels to the axes
+    plot_percentile: Bool, optional
+        if True, plot dashed vertical lines showing the 90% symmetric credible
+        intervals
+    xlims: list, optional
+        x axis limits you wish to use
+    max_vline: int, optional
+        if number of peaks < max_vline draw peaks as vertical lines rather
+        than histogramming the data
+    label: str, optional
+        label you wish to use for the plot
+    linestyle: str, optional
+        linestyle you wish to use for the plot
+    injection_color: str, optional
+        color of vertical line showing the injected value
     """
-    from pesummary.utils.samples_dict import Array
+    from pesummary.utils.array import Array
 
     logger.debug("Generating the 1d histogram plot for %s" % (param))
-    samples = Array(samples)
-    if fig is None:
+    samples = Array(samples, weights=weights)
+    if ax is None and fig is None:
         fig, ax = figure(gca=True)
-    else:
+    elif ax is None:
         ax = fig.gca()
-    if np.ptp(samples) == 0:
-        ax.axvline(samples[0], color=conf.color)
-        xlims = ax.get_xlim()
-    elif not kde:
-        ax.hist(
-            samples, histtype=histtype, bins=bins, color=color, density=True,
-            linewidth=1.75, weights=weights
-        )
-        xlims = ax.get_xlim()
-        if prior is not None:
+
+    if len(set(samples)) <= max_vline:
+        for _ind, _sample in enumerate(set(samples)):
+            _label = None
+            if _ind == 0:
+                _label = label
+            ax.axvline(_sample, color=color, label=_label)
+        _xlims = ax.get_xlim()
+    else:
+        if hist:
+            _default_hist_kwargs.update(hist_kwargs)
             ax.hist(
-                prior, color=conf.prior_color, alpha=0.2, edgecolor="w",
-                density=True, linewidth=1.75, histtype="bar", bins=bins,
+                samples, weights=weights, color=color, label=label,
+                linestyle=linestyle, **_default_hist_kwargs, **plot_kwargs
             )
-    else:
-        kwargs = {"shade": True, "alpha_shade": 0.1, "linewidth": 1.0}
-        if xlow is not None or xhigh is not None:
-            kwargs.update({"xlow": xlow, "xhigh": xhigh})
-        else:
-            kwargs.update({"clip": [samples.minimum, samples.maximum]})
-        x = kdeplot(samples, color=color, ax=ax, **kwargs)
-        xlims = ax.get_xlim()
-        if prior is not None:
-            kdeplot(prior, color=conf.prior_color, ax=ax, **kwargs)
-    ax.set_xlabel(latex_label)
-    ax.set_ylabel("Probability Density")
-    percentile = samples.confidence_interval([5, 95])
+            _xlims = ax.get_xlim()
+            if prior is not None:
+                _prior_hist_kwargs = _default_hist_kwargs.copy()
+                _prior_hist_kwargs["histtype"] = "bar"
+                _ = ax.hist(
+                    prior, color=conf.prior_color, alpha=0.2, edgecolor="w",
+                    linestyle=linestyle, **_prior_hist_kwargs, **plot_kwargs
+                )
+        if kde:
+            _kde_kwargs = kde_kwargs.copy()
+            kwargs = _default_kde_kwargs
+            kwargs.update({
+                "kde_kwargs": _kde_kwargs,
+                "kde_kernel": _kde_kwargs.pop("kde_kernel", None),
+                "variance_atol": _kde_kwargs.pop("variance_atol", 1e-8),
+                "weights": weights
+            })
+            kwargs.update(plot_kwargs)
+            x = kdeplot(
+                samples, color=color, ax=ax, linestyle=linestyle, **kwargs
+            )
+            _xlims = ax.get_xlim()
+            if prior is not None:
+                kdeplot(
+                    prior, color=conf.prior_color, ax=ax, linestyle=linestyle,
+                    **kwargs
+                )
+
+    if set_labels:
+        ax.set_xlabel(latex_label)
+        ax.set_ylabel("Probability Density")
+
     if inj_value is not None:
         ax.axvline(
-            inj_value, color=conf.injection_color, linestyle="-", linewidth=2.5
+            inj_value, color=injection_color, **_default_inj_kwargs
         )
-    ax.axvline(percentile[0], color=color, linestyle="--", linewidth=1.75)
-    ax.axvline(percentile[1], color=color, linestyle="--", linewidth=1.75)
+    percentile = samples.confidence_interval([5, 95])
     median = samples.average("median")
+    if plot_percentile:
+        for pp in percentile:
+            ax.axvline(
+                pp, color=color, linestyle="--",
+                linewidth=hist_kwargs.get("linewidth", 1.75)
+            )
     if title:
         upper = np.round(percentile[1] - median, 2)
-        lower = np.round(median - percentile[0], 2)
+        lower = np.abs(np.round(median - percentile[0], 2))
         median = np.round(median, 2)
         ax.set_title(r"$%s^{+%s}_{-%s}$" % (median, upper, lower))
-    ax.grid(b=grid)
+    ax.grid(visible=grid)
+    ax.set_xlim(xlims)
     if autoscale:
-        ax.set_xlim(xlims)
+        ax.set_xlim(_xlims)
+    if fig is None:
+        return ax
     fig.tight_layout()
     return fig
 
 
 def _1d_histogram_plot_mcmc(
-    param, samples, latex_label, inj_value=None, kde=False, prior=None,
-    weights=None, xlow=None, xhigh=None, colorcycle=conf.colorcycle, grid=True
+    param, samples, latex_label, colorcycle=conf.colorcycle, **kwargs
 ):
     """Generate a 1d histogram plot for a given parameter for a given
     set of mcmc chains
 
     Parameters
     ----------
     param: str
         name of the parameter that you wish to plot
     samples: np.ndarray
         2d array of samples for param for each mcmc chain
     latex_label: str
         latex label for param
-    inj_value: float
-        value that was injected
-    kde: Bool
-        if true, a kde is plotted instead of a histogram
-    prior: list
-        list of prior samples for param
-    weights: list
-        list of weights for each sample
     colorcycle: list, str
         color cycle you wish to use for the different mcmc chains
-    grid: Bool, optional
-        if True, plot a grid
+    **kwargs: dict, optional
+        all additional kwargs passed to _1d_histogram_plot
     """
     cycol = cycle(colorcycle)
     fig, ax = figure(gca=True)
     for ss in samples:
         fig = _1d_histogram_plot(
-            param, ss, latex_label, inj_value=inj_value, kde=kde, prior=prior,
-            weights=weights, xlow=xlow, xhigh=xhigh, fig=fig, color=next(cycol),
-            title=False, autoscale=False, grid=grid
+            param, ss, latex_label, color=next(cycol), title=False,
+            autoscale=False, fig=fig, **kwargs
         )
     gelman = gelman_rubin(samples)
     ax.set_title("Gelman-Rubin: {}".format(gelman))
     return fig
 
 
+def _1d_histogram_plot_bootstrap(
+    param, samples, latex_label, colorcycle=conf.colorcycle, nsamples=1000,
+    ntests=100, shade=False, plot_percentile=False, kde=True, hist=False,
+    **kwargs
+):
+    """Generate a bootstrapped 1d histogram plot for a given parameter
+
+    Parameters
+    ----------
+    param: str
+        name of the parameter that you wish to plot
+    samples: np.ndarray
+        array of samples for param
+    latex_label: str
+        latex label for param
+    colorcycle: list, str
+        color cycle you wish to use for the different tests
+    nsamples: int, optional
+        number of samples to randomly draw from samples. Default 1000
+    ntests: int, optional
+        number of tests to perform. Default 100
+    **kwargs: dict, optional
+        all additional kwargs passed to _1d_histogram_plot
+    """
+    if nsamples > len(samples):
+        nsamples = int(len(samples) / 2)
+    _samples = [
+        np.random.choice(samples, size=nsamples, replace=False) for _ in
+        range(ntests)
+    ]
+    cycol = cycle(colorcycle)
+    fig, ax = figure(gca=True)
+    for ss in _samples:
+        fig = _1d_histogram_plot(
+            param, ss, latex_label, color=next(cycol), title=False,
+            autoscale=False, fig=fig, shade=shade,
+            plot_percentile=plot_percentile, kde=kde, hist=hist, **kwargs
+        )
+    ax.set_title("Ntests: {}, Nsamples per test: {}".format(ntests, nsamples))
+    fig.tight_layout()
+    return fig
+
+
 def _1d_comparison_histogram_plot(
-    param, samples, colors, latex_label, labels, kde=False, linestyles=None,
-    xlow=None, xhigh=None, max_vline=1, figsize=(8, 6), grid=True,
-    legend_kwargs=_default_legend_kwargs, latex_friendly=False
+    param, samples, colors, latex_label, labels, inj_value=None, kde=False,
+    hist=True, linestyles=None, kde_kwargs={}, hist_kwargs={}, max_vline=1,
+    figsize=(8, 6), grid=True, legend_kwargs=_default_legend_kwargs,
+    latex_friendly=False, max_inj_line=1, injection_color="k", **kwargs
 ):
     """Generate the a plot to compare the 1d_histogram plots for a given
     parameter for different approximants.
 
     Parameters
     ----------
     param: str
@@ -478,63 +666,83 @@
         list of linestyles for each set of samples
     grid: Bool, optional
         if True, plot a grid
     legend_kwargs: dict, optional
         optional kwargs to pass to ax.legend()
     latex_friendly: Bool, optional
         if True, make the label latex friendly. Default False
+    inj_value: float/list, optional
+        either a single injection value which will be used for all histograms
+        or a list of injection values, one for each histogram
+    injection_color: str/list, optional
+        either a single color which will be used for all vertical line showing
+        the injected value or a list of colors, one for each injection
+    **kwargs: dict, optional
+        all additional kwargs passed to _1d_histogram_plot
     """
     logger.debug("Generating the 1d comparison histogram plot for %s" % (param))
     if linestyles is None:
         linestyles = ["-"] * len(samples)
+    if inj_value is None:
+        inj_value = [None] * len(samples)
+    elif isinstance(inj_value, (list, np.ndarray)) and len(inj_value) != len(samples):
+        raise ValueError(
+            "Please provide an injection for each analysis or a single "
+            "injection value which will be used for all histograms"
+        )
+    elif not isinstance(inj_value, (list, np.ndarray)):
+        inj_value = [inj_value] * len(samples)
+
+    if isinstance(injection_color, str):
+        injection_color = [injection_color] * len(samples)
+    elif len(injection_color) != len(samples):
+        raise ValueError(
+            "Please provide an injection color for each analysis or a single "
+            "injection color which will be used for all lines showing the "
+            "injected values"
+        )
+
+    flat_injection = np.array([_ for _ in inj_value if _ is not None]).flatten()
+    if len(set(flat_injection)) > max_inj_line:
+        logger.warning(
+            "Number of unique injection values ({}) is more than the maximum "
+            "allowed injection value ({}). Not plotting injection value. If "
+            "this is a mistake, please increase `max_inj_line`".format(
+                len(set(flat_injection)), max_inj_line
+            )
+        )
+        inj_value = [None] * len(samples)
+
     fig, ax = figure(figsize=figsize, gca=True)
     handles = []
+    hist_kwargs.update({"linewidth": 2.5})
     for num, i in enumerate(samples):
         if latex_friendly:
             labels = copy.deepcopy(labels)
             labels[num] = labels[num].replace("_", "\_")
-        if len(set(i)) <= max_vline:
-            for _ind, _sample in enumerate(set(i)):
-                _label = None
-                if _ind == 0:
-                    _label = labels[num]
-                ax.axvline(_sample, color=colors[num], label=_label)
-        elif not kde:
-            ax.hist(
-                i, histtype="step", bins=50, color=colors[num],
-                label=labels[num], linewidth=2.5, density=True,
-                linestyle=linestyles[num],
-            )
-        else:
-            kwargs = {
-                "shade": True, "alpha_shade": 0.05, "linewidth": 1.5,
-                "label": labels[num]
-            }
-            if xlow is not None or xhigh is not None:
-                kwargs.update({"xlow": xlow, "xhigh": xhigh})
-            else:
-                kwargs.update({"clip": [np.min(i), np.max(i)]})
-            kdeplot(i, color=colors[num], ax=ax, **kwargs)
-        ax.axvline(
-            x=np.percentile(i, 95), color=colors[num], linestyle="--",
-            linewidth=2.5
-        )
-        ax.axvline(
-            x=np.percentile(i, 5), color=colors[num], linestyle="--",
-            linewidth=2.5
+        fig = _1d_histogram_plot(
+            param, i, latex_label, kde=kde, hist=hist, kde_kwargs=kde_kwargs,
+            max_vline=max_vline, grid=grid, title=False, autoscale=False,
+            label=labels[num], color=colors[num], fig=fig, hist_kwargs=hist_kwargs,
+            inj_value=inj_value[num], injection_color=injection_color[num],
+            linestyle=linestyles[num], _default_inj_kwargs={
+                "linewidth": 4., "linestyle": "-", "alpha": 0.4
+            }, **kwargs
         )
         handles.append(mlines.Line2D([], [], color=colors[num], label=labels[num]))
+    ax = fig.gca()
     ncols = number_of_columns_for_legend(labels)
     legend = ax.legend(handles=handles, ncol=ncols, **legend_kwargs)
     for num, legobj in enumerate(legend.legendHandles):
         legobj.set_linewidth(1.75)
         legobj.set_linestyle(linestyles[num])
     ax.set_xlabel(latex_label)
     ax.set_ylabel("Probability Density")
-    ax.grid(b=grid)
+    ax.autoscale(axis='x')
+    ax.grid(visible=grid)
     fig.tight_layout()
     return fig
 
 
 def _comparison_box_plot(param, samples, colors, latex_label, labels, grid=True):
     """Generate a box plot to compare 1d_histograms for a given parameter
 
@@ -562,15 +770,15 @@
     middle = (maximum + minimum) * 0.5
     ax.boxplot(samples, widths=0.2, vert=False, whis=np.inf, labels=labels)
     for num, i in enumerate(labels):
         ax.annotate(i, xy=(middle, 1), xytext=(middle, num + 1.0 + 0.2), ha="center")
     ax.set_yticks([])
     ax.set_xlabel(latex_label)
     fig.tight_layout()
-    ax.grid(b=grid)
+    ax.grid(visible=grid)
     return fig
 
 
 def _make_corner_plot(
     samples, latex_labels, corner_parameters=None, parameters=None, **kwargs
 ):
     """Generate the corner plots for a given approximant
@@ -586,15 +794,15 @@
     approximant: str
         name of approximant that was used to generate the samples
     latex_labels: dict
         dictionary of latex labels for each parameter
     """
     logger.debug("Generating the corner plot")
     # set the default kwargs
-    default_kwargs = conf.corner_kwargs
+    default_kwargs = conf.corner_kwargs.copy()
     if parameters is None:
         parameters = list(samples.keys())
     if corner_parameters is not None:
         included_parameters = [i for i in parameters if i in corner_parameters]
     else:
         included_parameters = parameters
     xs = np.zeros([len(included_parameters), len(samples[parameters[0]])])
```

### Comparing `pesummary-0.9.1/pesummary/core/plots/bounded_1d_kde.py` & `pesummary-1.0.0/pesummary/core/plots/bounded_1d_kde.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,29 +1,21 @@
-# Copyright (C) 2018  Charlie Hoy     <charlie.hoy@ligo.org>
-#                     Michael Puerrer <michael.puerrer@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
+import copy
 import numpy as np
 from scipy.stats import gaussian_kde as kde
-from scipy.ndimage.filters import gaussian_filter1d
+from scipy.ndimage import gaussian_filter1d
 from pesummary.utils.decorators import deprecation
 from pesummary.utils.utils import logger
 
+__author__ = [
+    "Charlie Hoy <charlie.hoy@ligo.org>",
+    "Michael Puerrer <michael.puerrer@ligo.org>"
+]
+
 
 def transform_logit(x, a=0., b=1.):
     """
     """
     return np.log((x - a) / (b - x))
 
 
@@ -74,15 +66,15 @@
         The lower bound of the distribution
     xhigh: float
         The upper bound of the distribution
     """
     def __init__(self, pts, xlow=None, xhigh=None, *args, **kwargs):
         pts = np.atleast_1d(pts)
         if pts.ndim != 1:
-            raise TypeError("Bounded_1d_kde can only be one-dimensional")
+            raise TypeError("BoundedKDE can only be one-dimensional")
         super(BoundedKDE, self).__init__(pts.T, *args, **kwargs)
         self._xlow = xlow
         self._xhigh = xhigh
 
     @property
     def xlow(self):
         """The lower bound of the x domain
@@ -125,21 +117,22 @@
         Whether or not to apply smoothing. Default False
     """
     allowed = ["logit"]
 
     def __init__(
         self, pts, xlow=None, xhigh=None, transform="logit", inv_transform=None,
         dydx=None, alpha=1.5, N=100, smooth=3, apply_smoothing=False,
-        weights=None, *args, **kwargs
+        weights=None, same_input=True, *args, **kwargs
     ):
         import pandas
 
         self.inv_transform = inv_transform
         self.dydx = dydx
         self.transform = transform
+        self.same_input = same_input
         if isinstance(pts, pandas.core.series.Series):
             pts = np.array(pts)
         _args = np.hstack(np.argwhere((pts > xlow) & (pts < xhigh)))
         pts = pts[_args]
         if weights is not None:
             if isinstance(weights, pandas.core.series.Series):
                 weights = np.array(weights)
@@ -177,32 +170,45 @@
                 raise ValueError(
                     "Please provide an inverse transformation and the "
                     "derivative of the transform"
                 )
         self._transform = transform
 
     def __call__(self, pts):
+        _original = copy.deepcopy(pts)
         _args = np.argwhere((pts > self.xlow) & (pts < self.xhigh))
         if len(_args) != len(np.atleast_1d(pts)):
             logger.info(
                 "Removing {} samples as they are outside of the allowed "
                 "domain".format(len(np.atleast_1d(pts)) - len(_args))
             )
         if not len(_args):
             return np.zeros_like(pts)
+
         pts = np.hstack(pts[_args])
         pts = self.transform(np.atleast_1d(pts), self.xlow, self.xhigh)
         delta = np.max(pts) - np.min(pts)
         ymin = np.min(pts) - ((self.alpha - 1.) / 2) * delta
         ymax = np.max(pts) + ((self.alpha - 1.) / 2) * delta
         y = np.linspace(ymin, ymax, self.N)
         x = self.inv_transform(y, self.xlow, self.xhigh)
         Y = self.evaluate(y) * np.abs(self.dydx(x, self.xlow, self.xhigh))
         if self.apply_smoothing:
             Y = gaussian_filter1d(Y, sigma=self.smooth)
+        if self.same_input:
+            from scipy.interpolate import interp1d
+
+            f = interp1d(x, Y)
+            _args = np.argwhere(
+                (_original > np.amin(x)) & (_original < np.amax(x))
+            )
+            _Y = f(_original[_args])
+            Y = np.zeros(len(_original))
+            Y[_args] = _Y
+            return Y
         return x, Y
 
 
 class ReflectionBoundedKDE(BoundedKDE):
     """Represents a one-dimensional Gaussian kernel density estimator
     for a probability distribution function that exists on a bounded
     domain. The bounds are treated as reflections
```

### Comparing `pesummary-0.9.1/pesummary/core/plots/population.py` & `pesummary-1.0.0/pesummary/core/plots/population.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,26 +1,15 @@
-# Copyright (C) 2020  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import numpy as np
 from pesummary.core.plots.figure import figure
 from pesummary.utils.utils import logger
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 def scatter_plot(
     parameters, sample_dict, latex_labels, colors=None, xerr=None, yerr=None
 ):
     """Produce a plot which shows a population of runs over a certain parameter
     space. If errors are given, then plot error bars.
```

### Comparing `pesummary-0.9.1/pesummary/core/plots/kde.py` & `pesummary-1.0.0/pesummary/gw/conversions/evolve.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,320 +1,372 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-#                     Seaborn authors
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import numpy as np
-from scipy import stats
-import pandas as pd
-import matplotlib.pyplot as plt
-from seaborn.distributions import _bivariate_kdeplot
-from seaborn.utils import _kde_support
-from seaborn.distributions import _statsmodels_univariate_kde
-import warnings
-from pesummary.utils.utils import logger
+import multiprocessing
+
+from pesummary.gw.conversions import (
+    tilt_angles_and_phi_12_from_spin_vectors_and_L
+)
+from pesummary.utils.utils import iterator, logger
+from pesummary.utils.exceptions import EvolveSpinError
+from pesummary.utils.decorators import array_input
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 try:
-    import statsmodels.nonparametric.api as smnp
-    _has_statsmodels = True
+    from lal import MTSUN_SI, MSUN_SI
+    import lalsimulation
+    from lalsimulation import (
+        SimInspiralGetSpinFreqFromApproximant, SIM_INSPIRAL_SPINS_CASEBYCASE,
+        SIM_INSPIRAL_SPINS_FLOW, SimInspiralSpinTaylorPNEvolveOrbit
+    )
 except ImportError:
-    _has_statsmodels = False
+    pass
 
 
-def _scipy_univariate_kde(data, bw, gridsize, cut, clip, xlow=None, xhigh=None):
-    """Compute a univariate kernel density estimate using scipy."""
-    if xlow is not None or xhigh is not None:
-        from pesummary.core.plots.bounded_1d_kde import Bounded_1d_kde
-
-        if xlow is None:
-            xlow = np.min(data)
-        if xhigh is None:
-            xhigh = np.max(data)
-        try:
-            x_grid = np.linspace(xlow - 10e-10, xhigh + 10e-10, 10**3)
-            kde = Bounded_1d_kde(data, bw_method=bw, xlow=xlow, xhigh=xhigh)
-            post_grid = kde(np.atleast_1d(x_grid).T)
-            return x_grid, post_grid
-        except TypeError:
-            logger.warning(
-                "Failed to produce bounded 1d kde plot. Reverting to standard"
-            )
-    try:
-        kde = stats.gaussian_kde(data, bw_method=bw)
-    except TypeError:
-        kde = stats.gaussian_kde(data)
-        if bw != "scott":  # scipy default
-            msg = ("Ignoring bandwidth choice, "
-                   "please upgrade scipy to use a different bandwidth.")
-            warnings.warn(msg, UserWarning)
-    if isinstance(bw, str):
-        bw = "scotts" if bw == "scott" else bw
-        bw = getattr(kde, "%s_factor" % bw)() * np.std(data)
-    grid = _kde_support(data, bw, gridsize, cut, clip)
-    y = kde(grid)
-    return grid, y
-
-
-def _univariate_kdeplot(data, shade, vertical, kernel, bw, gridsize, cut,
-                        clip, legend, ax, cumulative=False, xlow=None,
-                        xhigh=None, **kwargs):
-    """Plot a univariate kernel density estimate on one of the axes."""
-
-    # Sort out the clipping
-    if clip is None:
-        clip = (-np.inf, np.inf)
-
-    # Calculate the KDE
-    if _has_statsmodels and xlow is None and xhigh is None:
-        # Prefer using statsmodels for kernel flexibility
-        x, y = _statsmodels_univariate_kde(data, kernel, bw,
-                                           gridsize, cut, clip,
-                                           cumulative=cumulative)
+def evolve_spins(*args, evolve_limit="ISCO", **kwargs):
+    """Evolve spins to a given limit.
+
+    Parameters
+    ----------
+    *args: tuple
+        all arguments passed to either evolve_angles_forwards or
+        evolve_angles_backwards
+    evolve_limit: str/float, optional
+        limit to evolve frequencies. If evolve_limit=='infinite_separation' or
+        evolve_limit==0, evolve spins to infinite separation. if
+        evolve_limit=='ISCO', evolve spins to ISCO frequency. If any other
+        float, evolve spins to that frequency.
+    **kwargs: dict, optional
+        all kwargs passed to either evolve_angles_forwards or evolve_angles_backwards
+    """
+    _infinite_string = "infinite_separation"
+    cond1 = isinstance(evolve_limit, str) and evolve_limit.lower() == _infinite_string
+    if cond1 or evolve_limit == 0:
+        return evolve_angles_backwards(*args, **kwargs)
     else:
-        # Fall back to scipy if missing statsmodels
-        if kernel != "gau":
-            kernel = "gau"
-            msg = "Kernel other than `gau` requires statsmodels."
-            warnings.warn(msg, UserWarning)
-        if cumulative:
-            raise ImportError("Cumulative distributions are currently "
-                              "only implemented in statsmodels. "
-                              "Please install statsmodels.")
-        x, y = _scipy_univariate_kde(data, bw, gridsize, cut, clip, xlow=xlow,
-                                     xhigh=xhigh)
-
-    # Make sure the density is nonnegative
-    y = np.amax(np.c_[np.zeros_like(y), y], axis=1)
-
-    # Flip the data if the plot should be on the y axis
-    if vertical:
-        x, y = y, x
-
-    # Check if a label was specified in the call
-    label = kwargs.pop("label", None)
-    alpha_shade = kwargs.pop("alpha_shade", 0.25)
-
-    # Otherwise check if the data object has a name
-    if label is None and hasattr(data, "name"):
-        label = data.name
-
-    # Decide if we're going to add a legend
-    legend = label is not None and legend
-    label = "_nolegend_" if label is None else label
-
-    # Use the active color cycle to find the plot color
-    facecolor = kwargs.pop("facecolor", None)
-    line, = ax.plot(x, y, **kwargs)
-    color = line.get_color()
-    line.remove()
-    kwargs.pop("color", None)
-    facecolor = color if facecolor is None else facecolor
-
-    # Draw the KDE plot and, optionally, shade
-    ax.plot(x, y, color=color, label=label, **kwargs)
-    shade_kws = dict(
-        facecolor=facecolor,
-        alpha=alpha_shade,
-        clip_on=kwargs.get("clip_on", True),
-        zorder=kwargs.get("zorder", 1),)
-    if shade:
-        if vertical:
-            ax.fill_betweenx(y, 0, x, **shade_kws)
-        else:
-            ax.fill_between(x, 0, y, **shade_kws)
+        return evolve_angles_forwards(*args, **kwargs)
 
-    # Set the density axis minimum to 0
-    if vertical:
-        ax.set_xlim(0, auto=None)
+
+@array_input(
+    ignore_kwargs=[
+        "final_velocity", "tolerance", "dt", "multi_process",
+        "evolution_approximant"
+    ], force_return_array=True
+)
+def evolve_angles_forwards(
+    mass_1, mass_2, a_1, a_2, tilt_1, tilt_2, phi_12, f_low, f_ref,
+    approximant, final_velocity="ISCO", tolerance=1e-3,
+    dt=0.1, multi_process=1, evolution_approximant="SpinTaylorT5"
+):
+    """Evolve the BBH spin angles forwards to a specified value using
+    lalsimulation.SimInspiralSpinTaylorPNEvolveOrbit. By default this is
+    the Schwarzchild ISCO velocity.
+
+    Parameters
+    ----------
+    mass_1: float/np.ndarray
+        float/array of primary mass samples of the binary
+    mass_2: float/np.ndarray
+        float/array of secondary mass samples of the binary
+    a_1: float/np.ndarray
+        float/array of primary spin magnitudes
+    a_2: float/np.ndarray
+        float/array of secondary spin magnitudes
+    tilt_1: float/np.ndarray
+        float/array of primary spin tilt angle from the orbital angular momentum
+    tilt_2: float/np.ndarray
+        float/array of secondary spin tilt angle from the orbital angular momentum
+    phi_12: float/np.ndarray
+        float/array of samples for the angle between the in-plane spin
+        components
+    f_low: float
+        low frequency cutoff used in the analysis
+    f_ref: float
+        reference frequency where spins are defined
+    approximant: str
+        Approximant used to generate the posterior samples
+    final_velocity: str, float
+        final orbital velocity for the evolution. This can either be the
+        Schwarzschild ISCO velocity 6**-0.5 ~= 0.408 ('ISCO') or a
+        fraction of the speed of light
+    tolerance: float
+        Only evolve spins if at least one spin's magnitude is greater than
+        tolerance
+    dt: float
+        steps in time for the integration, in terms of the mass of the binary
+    multi_process: int, optional
+        number of cores to run on when evolving the spins. Default: 1
+    evolution_approximant: str
+        name of the approximant you wish to use to evolve the spins. Default
+        is SpinTaylorT5. Other choices are SpinTaylorT1 or SpinTaylorT4
+    """
+    if isinstance(final_velocity, str) and final_velocity.lower() == "isco":
+        final_velocity = 6. ** -0.5
     else:
-        ax.set_ylim(0, auto=None)
+        final_velocity = float(final_velocity)
 
-    # Draw the legend here
-    handles, labels = ax.get_legend_handles_labels()
-    if legend and handles:
-        ax.legend(loc="best")
+    spinfreq_enum = SimInspiralGetSpinFreqFromApproximant(
+        getattr(lalsimulation, approximant)
+    )
+    if spinfreq_enum == SIM_INSPIRAL_SPINS_CASEBYCASE:
+        _msg = (
+            "Unable to evolve spins as '{}' does not have a set frequency "
+            "at which the spins are defined".format(approximant)
+        )
+        logger.warning(_msg)
+        raise EvolveSpinError(_msg)
+    f_start = float(np.where(
+        np.array(spinfreq_enum == SIM_INSPIRAL_SPINS_FLOW), f_low, f_ref
+    ))
+    with multiprocessing.Pool(multi_process) as pool:
+        args = np.array([
+            mass_1, mass_2, a_1, a_2, tilt_1, tilt_2, phi_12,
+            [f_start] * len(mass_1), [final_velocity] * len(mass_1),
+            [tolerance] * len(mass_1), [dt] * len(mass_1),
+            [evolution_approximant] * len(mass_1)
+        ], dtype="object").T
+        data = np.array(
+            list(
+                iterator(
+                    pool.imap(_wrapper_for_evolve_angles_forwards, args),
+                    tqdm=True, logger=logger, total=len(mass_1),
+                    desc="Evolving spins forward for remnant fits evaluation"
+                )
+            )
+        )
+    tilt_1_evol, tilt_2_evol, phi_12_evol = data.T
+    return tilt_1_evol, tilt_2_evol, phi_12_evol
 
-    return ax
 
+def _wrapper_for_evolve_angles_forwards(args):
+    """Wrapper function for _evolve_angles_forwards for a pool of workers
 
-def kdeplot(data, data2=None, shade=False, vertical=False, kernel="gau",
-            bw="scott", gridsize=100, cut=3, clip=None, legend=True,
-            cumulative=False, shade_lowest=True, cbar=False, cbar_ax=None,
-            cbar_kws=None, ax=None, xlow=None, xhigh=None, **kwargs):
-    """Fit and plot a univariate or bivariate kernel density estimate.
     Parameters
     ----------
-    data : 1d array-like
-        Input data.
-    data2: 1d array-like, optional
-        Second input data. If present, a bivariate KDE will be estimated.
-    shade : bool, optional
-        If True, shade in the area under the KDE curve (or draw with filled
-        contours when data is bivariate).
-    vertical : bool, optional
-        If True, density is on x-axis.
-    kernel : {'gau' | 'cos' | 'biw' | 'epa' | 'tri' | 'triw' }, optional
-        Code for shape of kernel to fit with. Bivariate KDE can only use
-        gaussian kernel.
-    bw : {'scott' | 'silverman' | scalar | pair of scalars }, optional
-        Name of reference method to determine kernel size, scalar factor,
-        or scalar for each dimension of the bivariate plot. Note that the
-        underlying computational libraries have different interperetations
-        for this parameter: ``statsmodels`` uses it directly, but ``scipy``
-        treats it as a scaling factor for the standard deviation of the
-        data.
-    gridsize : int, optional
-        Number of discrete points in the evaluation grid.
-    cut : scalar, optional
-        Draw the estimate to cut * bw from the extreme data points.
-    clip : pair of scalars, or pair of pair of scalars, optional
-        Lower and upper bounds for datapoints used to fit KDE. Can provide
-        a pair of (low, high) bounds for bivariate plots.
-    legend : bool, optional
-        If True, add a legend or label the axes when possible.
-    cumulative : bool, optional
-        If True, draw the cumulative distribution estimated by the kde.
-    shade_lowest : bool, optional
-        If True, shade the lowest contour of a bivariate KDE plot. Not
-        relevant when drawing a univariate plot or when ``shade=False``.
-        Setting this to ``False`` can be useful when you want multiple
-        densities on the same Axes.
-    cbar : bool, optional
-        If True and drawing a bivariate KDE plot, add a colorbar.
-    cbar_ax : matplotlib axes, optional
-        Existing axes to draw the colorbar onto, otherwise space is taken
-        from the main axes.
-    cbar_kws : dict, optional
-        Keyword arguments for ``fig.colorbar()``.
-    ax : matplotlib axes, optional
-        Axes to plot on, otherwise uses current axes.
-    kwargs : key, value pairings
-        Other keyword arguments are passed to ``plt.plot()`` or
-        ``plt.contour{f}`` depending on whether a univariate or bivariate
-        plot is being drawn.
-    xlow: float, optional
-        Optional lower bound for the kde
-    xhigh: float, optional
-        Optional upper bound for the kde
-    Returns
-    -------
-    ax : matplotlib Axes
-        Axes with plot.
-    See Also
-    --------
-    distplot: Flexibly plot a univariate distribution of observations.
-    jointplot: Plot a joint dataset with bivariate and marginal distributions.
-    Examples
-    --------
-    Plot a basic univariate density:
-    .. plot::
-        :context: close-figs
-        >>> import numpy as np; np.random.seed(10)
-        >>> import seaborn as sns; sns.set(color_codes=True)
-        >>> mean, cov = [0, 2], [(1, .5), (.5, 1)]
-        >>> x, y = np.random.multivariate_normal(mean, cov, size=50).T
-        >>> ax = sns.kdeplot(x)
-    Shade under the density curve and use a different color:
-    .. plot::
-        :context: close-figs
-        >>> ax = sns.kdeplot(x, shade=True, color="r")
-    Plot a bivariate density:
-    .. plot::
-        :context: close-figs
-        >>> ax = sns.kdeplot(x, y)
-    Use filled contours:
-    .. plot::
-        :context: close-figs
-        >>> ax = sns.kdeplot(x, y, shade=True)
-    Use more contour levels and a different color palette:
-    .. plot::
-        :context: close-figs
-        >>> ax = sns.kdeplot(x, y, n_levels=30, cmap="Purples_d")
-    Use a narrower bandwith:
-    .. plot::
-        :context: close-figs
-        >>> ax = sns.kdeplot(x, bw=.15)
-    Plot the density on the vertical axis:
-    .. plot::
-        :context: close-figs
-        >>> ax = sns.kdeplot(y, vertical=True)
-    Limit the density curve within the range of the data:
-    .. plot::
-        :context: close-figs
-        >>> ax = sns.kdeplot(x, cut=0)
-    Add a colorbar for the contours:
-    .. plot::
-        :context: close-figs
-        >>> ax = sns.kdeplot(x, y, cbar=True)
-    Plot two shaded bivariate densities:
-    .. plot::
-        :context: close-figs
-        >>> iris = sns.load_dataset("iris")
-        >>> setosa = iris.loc[iris.species == "setosa"]
-        >>> virginica = iris.loc[iris.species == "virginica"]
-        >>> ax = sns.kdeplot(setosa.sepal_width, setosa.sepal_length,
-        ...                  cmap="Reds", shade=True, shade_lowest=False)
-        >>> ax = sns.kdeplot(virginica.sepal_width, virginica.sepal_length,
-        ...                  cmap="Blues", shade=True, shade_lowest=False)
+    args: tuple
+        All args passed to _evolve_angles_forwards
     """
-    if ax is None:
-        ax = plt.gca()
+    return _evolve_angles_forwards(*args)
 
-    if isinstance(data, list):
-        data = np.asarray(data)
 
-    if len(data) == 0:
-        return ax
+def _evolve_angles_forwards(
+    mass_1, mass_2, a_1, a_2, tilt_1, tilt_2, phi_12, f_start, final_velocity,
+    tolerance, dt, evolution_approximant
+):
+    """Wrapper function for the SimInspiralSpinTaylorPNEvolveOrbit function
 
-    data = data.astype(np.float64)
-    if data2 is not None:
-        if isinstance(data2, list):
-            data2 = np.asarray(data2)
-        data2 = data2.astype(np.float64)
-
-    warn = False
-    bivariate = False
-    if isinstance(data, np.ndarray) and np.ndim(data) > 1:
-        warn = True
-        bivariate = True
-        x, y = data.T
-    elif isinstance(data, pd.DataFrame) and np.ndim(data) > 1:
-        warn = True
-        bivariate = True
-        x = data.iloc[:, 0].values
-        y = data.iloc[:, 1].values
-    elif data2 is not None:
-        bivariate = True
-        x = data
-        y = data2
-
-    if warn:
-        warn_msg = ("Passing a 2D dataset for a bivariate plot is deprecated "
-                    "in favor of kdeplot(x, y), and it will cause an error in "
-                    "future versions. Please update your code.")
-        warnings.warn(warn_msg, UserWarning)
-
-    if bivariate and cumulative:
-        raise TypeError("Cumulative distribution plots are not"
-                        "supported for bivariate distributions.")
-    if bivariate:
-        ax = _bivariate_kdeplot(x, y, shade, shade_lowest,
-                                kernel, bw, gridsize, cut, clip, legend,
-                                cbar, cbar_ax, cbar_kws, ax, **kwargs)
+    Parameters
+    ----------
+    mass_1: float
+        primary mass of the binary
+    mass_2: float
+        secondary mass of the binary
+    a_1: float
+        primary spin magnitude
+    a_2: float
+        secondary spin magnitude
+    tilt_1: float
+        primary spin tilt angle from the orbital angular momentum
+    tilt_2: float
+        secondary spin tilt angle from the orbital angular momentum
+    phi_12: float
+        the angle between the in-plane spin components
+    f_start: float
+        frequency to start the evolution from
+    final_velocity: float
+        Final velocity to evolve the spins up to
+    tolerance: float
+        Only evolve spins if at least one spins magnitude is greater than
+        tolerance
+    dt: float
+        steps in time for the integration, in terms of the mass of the binary
+    evolution_approximant: str
+        name of the approximant you wish to use to evolve the spins.
+    """
+    from packaging import version
+    if np.logical_or(a_1 > tolerance, a_2 > tolerance):
+        # Total mass in seconds
+        total_mass = (mass_1 + mass_2) * MTSUN_SI
+        f_final = final_velocity ** 3 / (total_mass * np.pi)
+        _approx = getattr(lalsimulation, evolution_approximant)
+        if version.parse(lalsimulation.__version__) >= version.parse("2.5.2"):
+            spinO = 6
+        else:
+            spinO = 7
+        data = SimInspiralSpinTaylorPNEvolveOrbit(
+            deltaT=dt * total_mass, m1=mass_1 * MSUN_SI,
+            m2=mass_2 * MSUN_SI, fStart=f_start, fEnd=f_final,
+            s1x=a_1 * np.sin(tilt_1), s1y=0.,
+            s1z=a_1 * np.cos(tilt_1),
+            s2x=a_2 * np.sin(tilt_2) * np.cos(phi_12),
+            s2y=a_2 * np.sin(tilt_2) * np.sin(phi_12),
+            s2z=a_2 * np.cos(tilt_2), lnhatx=0., lnhaty=0., lnhatz=1.,
+            e1x=1., e1y=0., e1z=0., lambda1=0., lambda2=0., quadparam1=1.,
+            quadparam2=1., spinO=spinO, tideO=0, phaseO=7, lscorr=0,
+            approx=_approx
+        )
+        # Set index to take from array output by SimInspiralSpinTaylorPNEvolveOrbit:
+        # -1 for evolving forward in time and 0 for evolving backward in time
+        if f_start <= f_final:
+            idx_use = -1
+        else:
+            idx_use = 0
+        a_1_evolve = np.array(
+            [
+                data[2].data.data[idx_use], data[3].data.data[idx_use],
+                data[4].data.data[idx_use]
+            ]
+        )
+        a_2_evolve = np.array(
+            [
+                data[5].data.data[idx_use], data[6].data.data[idx_use],
+                data[7].data.data[idx_use]
+            ]
+        )
+        Ln_evolve = np.array(
+            [
+                data[8].data.data[idx_use], data[9].data.data[idx_use],
+                data[10].data.data[idx_use]
+            ]
+        )
+        tilt_1_evol, tilt_2_evol, phi_12_evol = \
+            tilt_angles_and_phi_12_from_spin_vectors_and_L(
+                a_1_evolve, a_2_evolve, Ln_evolve
+            )
     else:
-        ax = _univariate_kdeplot(data, shade, vertical, kernel, bw,
-                                 gridsize, cut, clip, legend, ax,
-                                 cumulative=cumulative, xlow=xlow,
-                                 xhigh=xhigh, **kwargs)
+        tilt_1_evol, tilt_2_evol, phi_12_evol = tilt_1, tilt_2, phi_12
+    return tilt_1_evol, tilt_2_evol, phi_12_evol
+
+
+def _wrapper_for_evolve_angles_backwards(args):
+    """Wrapper function for evolving tilts backwards for a pool of workers
+
+    Parameters
+    ----------
+    args: tuple
+        Zeroth arg is the function you wish to use when evolving the tilts.
+        1st to 8th args are arguments passed to function. All other arguments
+        are treated as kwargs passed to function
+    """
+    _function = args[0]
+    _args = args[1:9]
+    _kwargs = args[9:]
+    return _function(*_args, **_kwargs[0])
+
+
+@array_input(
+    ignore_kwargs=[
+        "method", "multi_process", "return_fits_used", "version"
+    ], force_return_array=True
+)
+def evolve_angles_backwards(
+    mass_1, mass_2, a_1, a_2, tilt_1, tilt_2, phi_12, f_ref,
+    method="precession_averaged", multi_process=1, return_fits_used=False,
+    version="v2", approx="SpinTaylorT5", **kwargs
+):
+    """Evolve BBH tilt angles backwards to infinite separation
 
-    return ax
+    Parameters
+    ----------
+    mass_1: float/np.ndarray
+       float/array of primary mass samples of the binary
+    mass_2: float/np.ndarray
+        float/array of secondary mass samples of the binary
+    a_1: float/np.ndarray
+        float/array of primary spin magnitudes
+    a_2: float/np.ndarray
+        float/array of secondary spin magnitudes
+    tilt_1: float/np.ndarray
+        float/array of primary spin tilt angle from the orbital angular momentum
+    tilt_2: float/np.ndarray
+        float/array of secondary spin tilt angle from the orbital angular momentum
+    phi_12: float/np.ndarray
+        float/array of samples for the angle between the in-plane spin
+        components
+    f_ref: float
+        reference frequency where spins are defined
+    method: str
+        Method to use when evolving tilts to infinity. Possible options are
+        'precession_averaged' and 'hybrid_orbit_averaged'. 'precession_averaged'
+        computes tilt angles at infinite separation assuming that precession
+        averaged spin evolution from Gerosa et al. is valid starting from f_ref.
+        'hybrid_orbit_averaged' combines orbit-averaged evolution and
+        'precession_averaged' evolution as in Johnson-McDaniel et al. This is more
+        accurate but slower than the 'precession_averaged' method.
+    multi_process: int, optional
+        number of cores to run on when evolving the spins. Default: 1
+    return_fits_used: Bool, optional
+        return a dictionary of fits used. Default False
+    version: str, optional
+        version of the
+        tilts_at_infinity.hybrid_spin_evolution.calc_tilts_at_infty_hybrid_evolve
+        function to use within the lalsimulation library. Default 'v2'. If
+        an old version of lalsimulation is installed where 'v2' is not
+        available, fall back to 'v1'.
+    approx: str, optional
+        the approximant to use when evolving the spins. For allowed
+        approximants see
+        tilts_at_infinity.hybrid_spin_evolution.calc_tilts_at_infty_hybrid_evolve
+        Default 'SpinTaylorT5'
+    **kwargs: dict, optional
+        all kwargs passed to the
+        tilts_at_infinity.hybrid_spin_evolution.calc_tilts_at_infty_hybrid_evolve
+        function in the lalsimulation library
+    """
+    from lalsimulation.tilts_at_infinity import hybrid_spin_evolution
+    _mds = ["precession_averaged", "hybrid_orbit_averaged"]
+    if method.lower() not in _mds:
+        raise ValueError(
+            "Invalid method. Please choose either {}".format(", ".join(_mds))
+        )
+    # check to see if the provided version is available in lalsimulation. If not
+    # fall back to 'v1'
+    try:
+        _ = hybrid_spin_evolution.calc_tilts_at_infty_hybrid_evolve(
+            2., 1., 1., 1., 1., 1., 1., 1., version=version
+        )
+    except ValueError as e:
+        if "Only version" not in str(e):
+            raise
+        logger.warning(
+            "Unknown version '{}'. Falling back to 'v1'".format(version)
+        )
+        version = "v1"
+
+    kwargs.update(
+        {
+            "prec_only": method.lower() == "precession_averaged",
+            "version": version, "approx": approx
+        }
+    )
+
+    with multiprocessing.Pool(multi_process) as pool:
+        args = np.array(
+            [
+                [hybrid_spin_evolution.calc_tilts_at_infty_hybrid_evolve] * len(mass_1),
+                mass_1 * MSUN_SI, mass_2 * MSUN_SI, a_1, a_2, tilt_1, tilt_2, phi_12,
+                [f_ref] * len(mass_1), [kwargs] * len(mass_1)
+            ], dtype=object
+        ).T
+        data = np.array(
+            list(
+                iterator(
+                    pool.imap(_wrapper_for_evolve_angles_backwards, args),
+                    tqdm=True, desc="Evolving spins backwards to infinite separation",
+                    logger=logger, total=len(mass_1)
+                )
+            )
+        )
+    tilt_1_inf = np.array([l["tilt1_inf"] for l in data])
+    tilt_2_inf = np.array([l["tilt2_inf"] for l in data])
+    if return_fits_used:
+        fits_used = [
+            method.lower(), (
+                "lalsimulation.tilts_at_infinity.hybrid_spin_evolution."
+                "calc_tilts_at_infty_hybrid_evolve=={}".format(version)
+            )
+        ]
+        if method.lower() == "hybrid_orbit_averaged":
+            fits_used.append("approx={}".format(approx))
+        return [tilt_1_inf, tilt_2_inf, phi_12], fits_used
+    return tilt_1_inf, tilt_2_inf, phi_12
```

### Comparing `pesummary-0.9.1/pesummary/core/plots/figure.py` & `pesummary-1.0.0/pesummary/core/plots/figure.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,25 +1,14 @@
-# Copyright (C) 2019 Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from pesummary.utils.decorators import try_latex_plot
 from matplotlib.figure import Figure as MatplotlibFigure
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 def figure(*args, gca=True, **kwargs):
     """Extension of the matplotlib.pyplot.figure function
     """
     from matplotlib import pyplot
 
     try:
```

### Comparing `pesummary-0.9.1/pesummary/core/plots/interactive.py` & `pesummary-1.0.0/pesummary/core/plots/interactive.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,26 +1,14 @@
-# Copyright (C) 2019  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import plotly.graph_objects as go
-from plotly.colors import DEFAULT_PLOTLY_COLORS
 import plotly
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 def write_to_html(fig, filename):
     """Write a plotly.graph.objects.go.Figure to a html file
 
     Parameters
     ----------
     fig: plotly.graph.objects.go.Figure object
```

### Comparing `pesummary-0.9.1/pesummary/core/plots/main.py` & `pesummary-1.0.0/pesummary/core/plots/main.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,35 +1,28 @@
 #! /usr/bin/env python
 
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
+import numpy as np
 import os
 import importlib
+from multiprocessing import Pool
 
 from pesummary.core.plots.latex_labels import latex_labels
-from pesummary.utils.utils import logger, get_matplotlib_backend
+from pesummary.utils.utils import (
+    logger, get_matplotlib_backend, make_dir, get_matplotlib_style_file
+)
 from pesummary.core.plots import plot as core
 from pesummary.core.plots import interactive
 
 import matplotlib
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 matplotlib.use(get_matplotlib_backend(parallel=True))
-import numpy as np
+matplotlib.style.use(get_matplotlib_style_file())
 
 
 class _PlotGeneration(object):
     """Super class to handle the plot generation for a given set of result
     files
 
     Parameters
@@ -71,18 +64,20 @@
         self, savedir=None, webdir=None, labels=None, samples=None,
         kde_plot=False, existing_labels=None, existing_injection_data=None,
         existing_samples=None, existing_weights=None, same_parameters=None,
         injection_data=None, colors=None, custom_plotting=None,
         add_to_existing=False, priors={}, include_prior=False, weights=None,
         disable_comparison=False, linestyles=None, disable_interactive=False,
         multi_process=1, mcmc_samples=False, disable_corner=False,
-        corner_params=None
+        corner_params=None, expert_plots=True, checkpoint=False
     ):
         self.package = "core"
         self.webdir = webdir
+        make_dir(self.webdir)
+        make_dir(os.path.join(self.webdir, "plots", "corner"))
         self.savedir = savedir
         self.labels = labels
         self.mcmc_samples = mcmc_samples
         self.samples = samples
         self.kde_plot = kde_plot
         self.existing_labels = existing_labels
         self.existing_injection_data = existing_injection_data
@@ -95,16 +90,23 @@
         self.add_to_existing = add_to_existing
         self.priors = priors
         self.include_prior = include_prior
         self.linestyles = linestyles
         self.make_interactive = not disable_interactive
         self.make_corner = not disable_corner
         self.corner_params = corner_params
+        self.expert_plots = expert_plots
+        if self.mcmc_samples and self.expert_plots:
+            logger.warning("Unable to generate expert plots for mcmc samples")
+            self.expert_plots = False
+        self.checkpoint = checkpoint
         self.multi_process = multi_process
         self.pool = self.setup_pool()
+        self.preliminary_pages = {label: False for label in self.labels}
+        self.preliminary_comparison_pages = False
         self.make_comparison = (
             not disable_comparison and self._total_number_of_labels > 1
         )
         self.weights = (
             weights if weights is not None else {i: None for i in self.labels}
         )
 
@@ -114,25 +116,35 @@
                     key: item[param] for key, item in self.samples.items()
                 } for param in self.same_parameters
             }
         else:
             self.same_samples = None
 
         for i in self.samples.keys():
-            self.check_latex_labels(self.samples[i].keys())
+            try:
+                self.check_latex_labels(
+                    self.samples[i].keys(remove_debug=False)
+                )
+            except TypeError:
+                try:
+                    self.check_latex_labels(self.samples[i].keys())
+                except TypeError:
+                    pass
 
         self.plot_type_dictionary = {
             "oned_histogram": self.oned_histogram_plot,
             "sample_evolution": self.sample_evolution_plot,
             "autocorrelation": self.autocorrelation_plot,
             "oned_cdf": self.oned_cdf_plot,
             "custom": self.custom_plot
         }
         if self.make_corner:
             self.plot_type_dictionary.update({"corner": self.corner_plot})
+        if self.expert_plots:
+            self.plot_type_dictionary.update({"expert": self.expert_plot})
         if self.make_comparison:
             self.plot_type_dictionary.update(dict(
                 oned_histogram_comparison=self.oned_histogram_comparison_plot,
                 oned_cdf_comparison=self.oned_cdf_comparison_plot,
                 box_plot_comparison=self.box_plot_comparison_plot,
             ))
         if self.make_interactive:
@@ -145,15 +157,15 @@
                 self.plot_type_dictionary.update(
                     dict(
                         interactive_ridgeline=self.interactive_ridgeline_plot
                     )
                 )
 
     @staticmethod
-    def save(fig, name, close=True, format="png"):
+    def save(fig, name, preliminary=False, close=True, format="png"):
         """Save a figure to disk.
 
         Parameters
         ----------
         fig: matplotlib.pyplot.figure
             Matplotlib figure that you wish to save
         name: str
@@ -162,14 +174,20 @@
             Close the figure after it has been saved
         format: str, optional
             Format used to save the image
         """
         n = len(format)
         if ".%s" % (format) != name[-n - 1:]:
             name += ".%s" % (format)
+        if preliminary:
+            fig.text(
+                0.5, 0.5, 'Preliminary', fontsize=90, color='gray', alpha=0.1,
+                ha='center', va='center', rotation=30
+            )
+        fig.tight_layout()
         fig.savefig(name, format=format)
         if close:
             fig.close()
 
     @property
     def _total_number_of_labels(self):
         _number_of_labels = 0
@@ -201,16 +219,14 @@
         self._savedir = savedir
         if savedir is None:
             self._savedir = self.webdir + "/plots/"
 
     def setup_pool(self):
         """Setup a pool of processes to speed up plot generation
         """
-        from multiprocessing import Pool
-
         pool = Pool(processes=self.multi_process)
         return pool
 
     def generate_plots(self):
         """Generate all plots for all result files
         """
         for i in self.labels:
@@ -257,14 +273,16 @@
         """
         if self.make_corner:
             self.try_to_make_a_plot("corner", label=label)
         self.try_to_make_a_plot("oned_histogram", label=label)
         self.try_to_make_a_plot("sample_evolution", label=label)
         self.try_to_make_a_plot("autocorrelation", label=label)
         self.try_to_make_a_plot("oned_cdf", label=label)
+        if self.expert_plots:
+            self.try_to_make_a_plot("expert", label=label)
         if self.custom_plotting:
             self.try_to_make_a_plot("custom", label=label)
 
     def _generate_interactive_plots(self, label):
         """Generate all interactive plots and save them to an html file ready
         to be imported later
         """
@@ -337,48 +355,54 @@
         """
         if self.mcmc_samples:
             samples = self.samples[label].combine
         else:
             samples = self.samples[label]
         self._corner_plot(
             self.savedir, label, samples, latex_labels, self.webdir,
-            self.corner_params
+            self.corner_params, self.preliminary_pages[label], self.checkpoint
         )
 
     @staticmethod
-    def _corner_plot(savedir, label, samples, latex_labels, webdir, params):
+    def _corner_plot(
+        savedir, label, samples, latex_labels, webdir, params, preliminary=False,
+        checkpoint=False
+    ):
         """Generate a corner plot for a given set of samples
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         samples: dict
-            dictionary containing PESummary.utils.utils.Array objects that
+            dictionary containing PESummary.utils.array.Array objects that
             contain samples for each parameter
         latex_labels: str
             latex labels for each parameter in samples
         webdir: str
             the directory where the `js` directory is located
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
         import warnings
 
         with warnings.catch_warnings():
             warnings.simplefilter("ignore")
+            filename = os.path.join(
+                savedir, "corner", "{}_all_density_plots.png".format(label)
+            )
+            if os.path.isfile(filename) and checkpoint:
+                return
             fig, params, data = core._make_corner_plot(
                 samples, latex_labels, corner_parameters=params
             )
             _PlotGeneration.save(
-                fig, os.path.join(
-                    savedir, "corner", "{}_all_density_plots".format(
-                        label
-                    )
-                )
+                fig, filename, preliminary=preliminary
             )
             combine_corner = open(
                 os.path.join(webdir, "js", "combine_corner.js")
             )
             combine_corner = combine_corner.readlines()
             params = [str(i) for i in params]
             ind = [
@@ -428,16 +452,14 @@
         """Generate oned histogram plots for all parameters in the result file
 
         Parameters
         ----------
         label: str
             the label for the results file that you wish to plot
         """
-        import math
-
         error_message = (
             "Failed to generate oned_histogram plot for %s because {}"
         )
 
         iterator, samples, function = self._mcmc_iterator(
             label, "_oned_histogram_plot"
         )
@@ -448,15 +470,16 @@
 
         arguments = [
             (
                 [
                     self.savedir, label, param, samples[param],
                     latex_labels[param], self.injection_data[label][param],
                     self.kde_plot, prior(param), self.weights[label],
-                    self.package
+                    self.package, self.preliminary_pages[label],
+                    self.checkpoint
                 ], function, error_message % (param)
             ) for param in iterator
         ]
         self.pool.starmap(self._try_to_make_a_plot, arguments)
 
     def oned_histogram_comparison_plot(self, label):
         """Generate oned comparison histogram plots for all parameters that are
@@ -467,170 +490,207 @@
         label: str
             the label for the results file that you wish to plot
         """
         error_message = (
             "Failed to generate a comparison histogram plot for %s because {}"
         )
         for param in self.same_parameters:
+            injection = [
+                value[param] for value in self.injection_data.values()
+            ]
             arguments = [
                 self.savedir, param, self.same_samples[param],
-                latex_labels[param], self.colors, self.kde_plot,
-                self.linestyles, self.package
+                latex_labels[param], self.colors, injection, self.kde_plot,
+                self.linestyles, self.package,
+                self.preliminary_comparison_pages, self.checkpoint, None
             ]
             self._try_to_make_a_plot(
                 arguments, self._oned_histogram_comparison_plot,
                 error_message % (param)
             )
             continue
 
     @staticmethod
     def _oned_histogram_comparison_plot(
-        savedir, parameter, samples, latex_label, colors, kde=False,
-        linestyles=None, package="core"
+        savedir, parameter, samples, latex_label, colors, injection, kde=False,
+        linestyles=None, package="core", preliminary=False, checkpoint=False,
+        filename=None
     ):
         """Generate a oned comparison histogram plot for a given parameter
 
         Parameters
-        ----------
+        ----------i
         savedir: str
             the directory you wish to save the plot in
         parameter: str
             the name of the parameter that you wish to make a oned comparison
             histogram for
         samples: dict
-            dictionary of pesummary.utils.utils.Array objects containing the
+            dictionary of pesummary.utils.array.Array objects containing the
             samples that correspond to parameter for each result file. The key
             should be the corresponding label
         latex_label: str
             the latex label for parameter
         colors: list
             list of colors to be used to distinguish different result files
+        injection: list
+            list of injected values, one for each analysis
         kde: Bool, optional
             if True, kde plots will be generated rather than 1d histograms
         linestyles: list, optional
             list of linestyles used to distinguish different result files
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        import math
         module = importlib.import_module(
             "pesummary.{}.plots.plot".format(package)
         )
+        if filename is None:
+            filename = os.path.join(
+                savedir, "combined_1d_posterior_{}.png".format(parameter)
+            )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        hist = not kde
+        for num, inj in enumerate(injection):
+            if math.isnan(inj):
+                injection[num] = None
         same_samples = [val for key, val in samples.items()]
         fig = module._1d_comparison_histogram_plot(
             parameter, same_samples, colors, latex_label,
-            list(samples.keys()), kde=kde, linestyles=linestyles
+            list(samples.keys()), inj_value=injection, kde=kde,
+            linestyles=linestyles, hist=hist
         )
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "combined_1d_posterior_{}".format(parameter)
-            )
+            fig, filename, preliminary=preliminary
         )
 
     @staticmethod
     def _oned_histogram_plot(
         savedir, label, parameter, samples, latex_label, injection, kde=False,
-        prior=None, weights=None, package="core"
+        prior=None, weights=None, package="core", preliminary=False,
+        checkpoint=False
     ):
         """Generate a oned histogram plot for a given set of samples
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         parameter: str
             the name of the parameter that you wish to plot
-        samples: PESummary.utils.utils.Array
+        samples: PESummary.utils.array.Array
             array containing the samples corresponding to parameter
         latex_label: str
             the latex label corresponding to parameter
         injection: float
             the injected value
         kde: Bool, optional
             if True, kde plots will be generated rather than 1d histograms
-        prior: PESummary.utils.utils.Array, optional
+        prior: PESummary.utils.array.Array, optional
             the prior samples for param
-        weights: PESummary.utils.utils.Array, optional
+        weights: PESummary.utils.utilsrray, optional
             the weights for each samples. If None, assumed to be 1
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
         import math
         module = importlib.import_module(
             "pesummary.{}.plots.plot".format(package)
         )
 
         if math.isnan(injection):
             injection = None
+        hist = not kde
 
+        filename = os.path.join(
+            savedir, "{}_1d_posterior_{}.png".format(label, parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         fig = module._1d_histogram_plot(
-            parameter, samples, latex_label, injection, kde=kde, prior=prior,
-            weights=weights
+            parameter, samples, latex_label, inj_value=injection, kde=kde,
+            hist=hist, prior=prior, weights=weights
         )
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "{}_1d_posterior_{}".format(label, parameter)
-            )
+            fig, filename, preliminary=preliminary
         )
 
     @staticmethod
     def _oned_histogram_plot_mcmc(
         savedir, label, parameter, samples, latex_label, injection, kde=False,
-        prior=None, weights=None, package="core"
+        prior=None, weights=None, package="core", preliminary=False,
+        checkpoint=False
     ):
         """Generate a oned histogram plot for a given set of samples for
         multiple mcmc chains
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         parameter: str
             the name of the parameter that you wish to plot
         samples: dict
-            dictionary of PESummary.utils.utils.Array objects containing the
+            dictionary of PESummary.utils.array.Array objects containing the
             samples corresponding to parameter for multiple mcmc chains
         latex_label: str
             the latex label corresponding to parameter
         injection: float
             the injected value
         kde: Bool, optional
             if True, kde plots will be generated rather than 1d histograms
-        prior: PESummary.utils.utils.Array, optional
+        prior: PESummary.utils.array.Array, optional
             the prior samples for param
-        weights: PESummary.utils.utils.Array, optional
+        weights: PESummary.utils.array.Array, optional
             the weights for each samples. If None, assumed to be 1
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
         import math
-        from pesummary.utils.samples_dict import Array
+        from pesummary.utils.array import Array
 
         module = importlib.import_module(
             "pesummary.{}.plots.plot".format(package)
         )
 
         if math.isnan(injection):
             injection = None
         same_samples = [val for key, val in samples.items()]
-        fig = module._1d_histogram_plot_mcmc(
-            parameter, same_samples, latex_label, injection, kde=kde,
-            prior=prior, weights=weights
+        filename = os.path.join(
+            savedir, "{}_1d_posterior_{}.png".format(label, parameter)
         )
-        _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "{}_1d_posterior_{}".format(label, parameter)
+        if os.path.isfile(filename) and checkpoint:
+            pass
+        else:
+            fig = module._1d_histogram_plot_mcmc(
+                parameter, same_samples, latex_label, inj_value=injection,
+                kde=kde, prior=prior, weights=weights
             )
-        )
-        fig = module._1d_histogram_plot(
-            parameter, Array(np.concatenate(same_samples)), latex_label,
-            injection, kde=kde, prior=prior, weights=weights
-        )
-        _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "{}_1d_posterior_{}_combined".format(label, parameter)
+            _PlotGeneration.save(
+                fig, filename, preliminary=preliminary
             )
+        filename = os.path.join(
+            savedir, "{}_1d_posterior_{}_combined.png".format(label, parameter)
         )
+        if os.path.isfile(filename) and checkpoint:
+            pass
+        else:
+            fig = module._1d_histogram_plot(
+                parameter, Array(np.concatenate(same_samples)), latex_label,
+                inj_value=injection, kde=kde, prior=prior, weights=weights
+            )
+            _PlotGeneration.save(
+                fig, filename, preliminary=preliminary
+            )
 
     def sample_evolution_plot(self, label):
         """Generate sample evolution plots for all parameters in the result file
 
         Parameters
         ----------
         label: str
@@ -642,80 +702,323 @@
         iterator, samples, function = self._mcmc_iterator(
             label, "_sample_evolution_plot"
         )
         arguments = [
             (
                 [
                     self.savedir, label, param, samples[param],
-                    latex_labels[param], self.injection_data[label][param]
+                    latex_labels[param], self.injection_data[label][param],
+                    self.preliminary_pages[label], self.checkpoint
+                ], function, error_message % (param)
+            ) for param in iterator
+        ]
+        self.pool.starmap(self._try_to_make_a_plot, arguments)
+
+    def expert_plot(self, label):
+        """Generate expert plots for diagnostics
+
+        Parameters
+        ----------
+        label: str
+            the label for the results file that you wish to plot
+        """
+        error_message = (
+            "Failed to generate log_likelihood-%s 2d contour plot because {}"
+        )
+        iterator, samples, function = self._mcmc_iterator(
+            label, "_2d_contour_plot"
+        )
+        _debug = self.samples[label].debug_keys()
+        arguments = [
+            (
+                [
+                    self.savedir, label, param, "log_likelihood", samples[param],
+                    samples["log_likelihood"], latex_labels[param],
+                    latex_labels["log_likelihood"],
+                    self.preliminary_pages[label], [
+                        samples[param][np.argmax(samples["log_likelihood"])],
+                        np.max(samples["log_likelihood"]),
+                    ], self.checkpoint
+                ], function, error_message % (param)
+            ) for param in iterator + _debug
+        ]
+        self.pool.starmap(self._try_to_make_a_plot, arguments)
+        _reweight_keys = [
+            param for param in self.samples[label].debug_keys() if
+            "_non_reweighted" in param
+        ]
+        if len(_reweight_keys):
+            error_message = (
+                "Failed to generate %s-%s 2d contour plot because {}"
+            )
+            _base_param = lambda p: p.split("_non_reweighted")[0][1:]
+            arguments = [
+                (
+                    [
+                        self.savedir, label, _base_param(param), param,
+                        samples[_base_param(param)], samples[param],
+                        latex_labels[_base_param(param)], latex_labels[param],
+                        self.preliminary_pages[label], None, self.checkpoint
+                    ], function, error_message % (_base_param(param), param)
+                ) for param in _reweight_keys
+            ]
+            self.pool.starmap(self._try_to_make_a_plot, arguments)
+            error_message = (
+                "Failed to generate a histogram plot comparing %s and %s "
+                "because {}"
+            )
+            arguments = [
+                (
+                    [
+                        self.savedir, _base_param(param), {
+                            "reweighted": samples[_base_param(param)],
+                            "non-reweighted": samples[param]
+                        }, latex_labels[_base_param(param)], ['b', 'r'],
+                        [np.nan, np.nan], True, None, self.package,
+                        self.preliminary_comparison_pages, self.checkpoint,
+                        os.path.join(
+                            self.savedir, "{}_1d_posterior_{}_{}.png".format(
+                                label, _base_param(param), param
+                            )
+                        )
+                    ], self._oned_histogram_comparison_plot,
+                    error_message % (_base_param(param), param)
+                ) for param in _reweight_keys
+            ]
+            self.pool.starmap(self._try_to_make_a_plot, arguments)
+
+        error_message = (
+            "Failed to generate log_likelihood-%s sample_evolution plot "
+            "because {}"
+        )
+        iterator, samples, function = self._mcmc_iterator(
+            label, "_colored_sample_evolution_plot"
+        )
+        arguments = [
+            (
+                [
+                    self.savedir, label, param, "log_likelihood", samples[param],
+                    samples["log_likelihood"], latex_labels[param],
+                    latex_labels["log_likelihood"],
+                    self.preliminary_pages[label], self.checkpoint
+                ], function, error_message % (param)
+            ) for param in iterator
+        ]
+        self.pool.starmap(self._try_to_make_a_plot, arguments)
+        error_message = (
+            "Failed to generate bootstrapped oned_histogram plot for %s "
+            "because {}"
+        )
+        iterator, samples, function = self._mcmc_iterator(
+            label, "_oned_histogram_bootstrap_plot"
+        )
+        arguments = [
+            (
+                [
+                    self.savedir, label, param, samples[param],
+                    latex_labels[param], self.preliminary_pages[label],
+                    self.package, self.checkpoint
                 ], function, error_message % (param)
             ) for param in iterator
         ]
         self.pool.starmap(self._try_to_make_a_plot, arguments)
 
     @staticmethod
+    def _oned_histogram_bootstrap_plot(
+        savedir, label, parameter, samples, latex_label, preliminary=False,
+        package="core", checkpoint=False, nsamples=1000, ntests=100, **kwargs
+    ):
+        """Generate a bootstrapped oned histogram plot for a given set of
+        samples
+
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        label: str
+            the label corresponding to the results file
+        parameter: str
+            the name of the parameter that you wish to plot
+        samples: PESummary.utils.array.Array
+            array containing the samples corresponding to parameter
+        latex_label: str
+            the latex label corresponding to parameter
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        module = importlib.import_module(
+            "pesummary.{}.plots.plot".format(package)
+        )
+
+        filename = os.path.join(
+            savedir, "{}_1d_posterior_{}_bootstrap.png".format(label, parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        fig = module._1d_histogram_plot_bootstrap(
+            parameter, samples, latex_label, nsamples=nsamples, ntests=ntests,
+            **kwargs
+        )
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
+        )
+
+    @staticmethod
+    def _2d_contour_plot(
+        savedir, label, parameter_x, parameter_y, samples_x, samples_y,
+        latex_label_x, latex_label_y, preliminary=False, truth=None,
+        checkpoint=False
+    ):
+        """Generate a 2d contour plot for a given set of samples
+
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        label: str
+            the label corresponding to the results file
+        samples_x: PESummary.utils.array.Array
+            array containing the samples for the x axis
+        samples_y: PESummary.utils.array.Array
+            array containing the samples for the y axis
+        latex_label_x: str
+            the latex label for the x axis
+        latex_label_y: str
+            the latex label for the y axis
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        from pesummary.core.plots.publication import twod_contour_plot
+
+        filename = os.path.join(
+            savedir, "{}_2d_contour_{}_{}.png".format(
+                label, parameter_x, parameter_y
+            )
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        fig = twod_contour_plot(
+            samples_x, samples_y, levels=[0.9, 0.5], xlabel=latex_label_x,
+            ylabel=latex_label_y, bins=50, truth=truth
+        )
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
+        )
+
+    @staticmethod
+    def _colored_sample_evolution_plot(
+        savedir, label, parameter_x, parameter_y, samples_x, samples_y,
+        latex_label_x, latex_label_y, preliminary=False, checkpoint=False
+    ):
+        """Generate a 2d contour plot for a given set of samples
+
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        label: str
+            the label corresponding to the results file
+        samples_x: PESummary.utils.array.Array
+            array containing the samples for the x axis
+        samples_y: PESummary.utils.array.Array
+            array containing the samples for the y axis
+        latex_label_x: str
+            the latex label for the x axis
+        latex_label_y: str
+            the latex label for the y axis
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        filename = os.path.join(
+            savedir, "{}_sample_evolution_{}_{}_colored.png".format(
+                label, parameter_x, parameter_y
+            )
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        fig = core._sample_evolution_plot(
+            parameter_x, samples_x, latex_label_x, z=samples_y,
+            z_label=latex_label_y
+        )
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
+        )
+
+    @staticmethod
     def _sample_evolution_plot(
-        savedir, label, parameter, samples, latex_label, injection
+        savedir, label, parameter, samples, latex_label, injection,
+        preliminary=False, checkpoint=False
     ):
         """Generate a sample evolution plot for a given set of samples
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         parameter: str
             the name of the parameter that you wish to plot
-        samples: PESummary.utils.utils.Array
+        samples: PESummary.utils.array.Array
             array containing the samples corresponding to parameter
         latex_label: str
             the latex label corresponding to parameter
         injection: float
             the injected value
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        filename = os.path.join(
+            savedir, "{}_sample_evolution_{}.png".format(label, parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         fig = core._sample_evolution_plot(
             parameter, samples, latex_label, injection
         )
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "{}_sample_evolution_{}".format(label, parameter)
-            )
+            fig, filename, preliminary=preliminary
         )
 
     @staticmethod
     def _sample_evolution_plot_mcmc(
-        savedir, label, parameter, samples, latex_label, injection
+        savedir, label, parameter, samples, latex_label, injection,
+        preliminary=False, checkpoint=False
     ):
         """Generate a sample evolution plot for a given set of mcmc chains
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         parameter: str
             the name of the parameter that you wish to plot
         samples: dict
-            dictionary containing pesummary.utils.utils.Array objects containing
+            dictionary containing pesummary.utils.array.Array objects containing
             the samples corresponding to parameter for each chain
         latex_label: str
             the latex label corresponding to parameter
         injection: float
             the injected value
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        filename = os.path.join(
+            savedir, "{}_sample_evolution_{}.png".format(label, parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         same_samples = [val for key, val in samples.items()]
         fig = core._sample_evolution_plot_mcmc(
             parameter, same_samples, latex_label, injection
         )
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "{}_sample_evolution_{}".format(label, parameter)
-            )
+            fig, filename, preliminary=preliminary
         )
 
     def autocorrelation_plot(self, label):
         """Generate autocorrelation plots for all parameters in the result file
 
         Parameters
         ----------
@@ -726,69 +1029,81 @@
             "Failed to generate an autocorrelation plot for %s because {}"
         )
         iterator, samples, function = self._mcmc_iterator(
             label, "_autocorrelation_plot"
         )
         arguments = [
             (
-                [self.savedir, label, param, samples[param]],
-                function, error_message % (param)
+                [
+                    self.savedir, label, param, samples[param],
+                    self.preliminary_pages[label], self.checkpoint
+                ], function, error_message % (param)
             ) for param in iterator
         ]
         self.pool.starmap(self._try_to_make_a_plot, arguments)
 
     @staticmethod
-    def _autocorrelation_plot(savedir, label, parameter, samples):
+    def _autocorrelation_plot(
+        savedir, label, parameter, samples, preliminary=False, checkpoint=False
+    ):
         """Generate an autocorrelation plot for a given set of samples
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         parameter: str
             the name of the parameter that you wish to plot
-        samples: PESummary.utils.utils.Array
+        samples: PESummary.utils.array.Array
             array containing the samples corresponding to parameter
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        filename = os.path.join(
+            savedir, "{}_autocorrelation_{}.png".format(label, parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         fig = core._autocorrelation_plot(parameter, samples)
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "{}_autocorrelation_{}".format(
-                    label, parameter
-                )
-            )
+            fig, filename, preliminary=preliminary
         )
 
     @staticmethod
-    def _autocorrelation_plot_mcmc(savedir, label, parameter, samples):
+    def _autocorrelation_plot_mcmc(
+        savedir, label, parameter, samples, preliminary=False, checkpoint=False
+    ):
         """Generate an autocorrelation plot for a list of samples, one for each
         mcmc chain
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         parameter: str
             the name of the parameter that you wish to plot
         samples: dict
-            dictioanry of PESummary.utils.utils.Array objects containing the
+            dictioanry of PESummary.utils.array.Array objects containing the
             samples corresponding to parameter for each mcmc chain
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        filename = os.path.join(
+            savedir, "{}_autocorrelation_{}.png".format(label, parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         same_samples = [val for key, val in samples.items()]
         fig = core._autocorrelation_plot_mcmc(parameter, same_samples)
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "{}_autocorrelation_{}".format(
-                    label, parameter
-                )
-            )
+            fig, filename, preliminary=preliminary
         )
 
     def oned_cdf_plot(self, label):
         """Generate oned CDF plots for all parameters in the result file
 
         Parameters
         ----------
@@ -801,138 +1116,164 @@
         iterator, samples, function = self._mcmc_iterator(
             label, "_oned_cdf_plot"
         )
         arguments = [
             (
                 [
                     self.savedir, label, param, samples[param],
-                    latex_labels[param]
+                    latex_labels[param], self.preliminary_pages[label],
+                    self.checkpoint
                 ], function, error_message % (param)
             ) for param in iterator
         ]
         self.pool.starmap(self._try_to_make_a_plot, arguments)
 
     @staticmethod
-    def _oned_cdf_plot(savedir, label, parameter, samples, latex_label):
+    def _oned_cdf_plot(
+        savedir, label, parameter, samples, latex_label, preliminary=False,
+        checkpoint=False
+    ):
         """Generate a oned CDF plot for a given set of samples
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         parameter: str
             the name of the parameter that you wish to plot
-        samples: PESummary.utils.utils.Array
+        samples: PESummary.utils.array.Array
             array containing the samples corresponding to parameter
         latex_label: str
             the latex label corresponding to parameter
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        filename = os.path.join(
+            savedir + "{}_cdf_{}.png".format(label, parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         fig = core._1d_cdf_plot(parameter, samples, latex_label)
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir + "{}_cdf_{}".format(label, parameter)
-            )
+            fig, filename, preliminary=preliminary
         )
 
     @staticmethod
-    def _oned_cdf_plot_mcmc(savedir, label, parameter, samples, latex_label):
+    def _oned_cdf_plot_mcmc(
+        savedir, label, parameter, samples, latex_label, preliminary=False,
+        checkpoint=False
+    ):
         """Generate a oned CDF plot for a given set of samples, one for each
         mcmc chain
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         parameter: str
             the name of the parameter that you wish to plot
         samples: dict
-            dictionary of PESummary.utils.utils.Array objects containing the
+            dictionary of PESummary.utils.array.Array objects containing the
             samples corresponding to parameter for each mcmc chain
         latex_label: str
             the latex label corresponding to parameter
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        filename = os.path.join(
+            savedir + "{}_cdf_{}.png".format(label, parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         same_samples = [val for key, val in samples.items()]
         fig = core._1d_cdf_plot_mcmc(parameter, same_samples, latex_label)
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir + "{}_cdf_{}".format(label, parameter)
-            )
+            fig, filename, preliminary=preliminary
         )
 
     def interactive_ridgeline_plot(self, label):
         """Generate an interactive ridgeline plot for all paramaters that are
         common to all result files
         """
         error_message = (
             "Failed to generate an interactive ridgeline plot for %s because {}"
         )
         for param in self.same_parameters:
             arguments = [
                 self.savedir, param, self.same_samples[param],
-                latex_labels[param], self.colors
+                latex_labels[param], self.colors, self.checkpoint
             ]
             self._try_to_make_a_plot(
                 arguments, self._interactive_ridgeline_plot,
                 error_message % (param)
             )
             continue
 
     @staticmethod
     def _interactive_ridgeline_plot(
-        savedir, parameter, samples, latex_label, colors
+        savedir, parameter, samples, latex_label, colors, checkpoint=False
     ):
         """Generate an interactive ridgeline plot for
         """
+        filename = os.path.join(
+            savedir, "interactive_ridgeline_{}.html".format(parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         same_samples = [val for key, val in samples.items()]
         _ = interactive.ridgeline(
             same_samples, list(samples.keys()), xlabel=latex_label,
-            colors=colors, write_to_html_file=os.path.join(
-                savedir, "interactive_ridgeline_{}.html".format(parameter)
-            )
+            colors=colors, write_to_html_file=filename
         )
 
     def interactive_corner_plot(self, label):
         """Generate an interactive corner plot for a given result file
 
         Parameters
         ----------
         label: str
             the label for the results file that you wish to plot
         """
         self._interactive_corner_plot(
-            self.savedir, label, self.samples[label], latex_labels
+            self.savedir, label, self.samples[label], latex_labels,
+            self.checkpoint
         )
 
     @staticmethod
-    def _interactive_corner_plot(savedir, label, samples, latex_labels):
+    def _interactive_corner_plot(
+        savedir, label, samples, latex_labels, checkpoint=False
+    ):
         """Generate an interactive corner plot for a given set of samples
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         label: str
             the label corresponding to the results file
         samples: dict
-            dictionary containing PESummary.utils.utils.Array objects that
+            dictionary containing PESummary.utils.array.Array objects that
             contain samples for each parameter
         latex_labels: str
             latex labels for each parameter in samples
         """
+        filename = os.path.join(
+            savedir, "corner", "{}_interactive.html".format(label)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         parameters = samples.keys()
         data = [samples[parameter] for parameter in parameters]
         latex_labels = [latex_labels[parameter] for parameter in parameters]
         _ = interactive.corner(
-            data, latex_labels, write_to_html_file=os.path.join(
-                savedir, "corner", "{}_interactive.html".format(label)
-            )
+            data, latex_labels, write_to_html_file=filename
         )
 
     def oned_cdf_comparison_plot(self, label):
         """Generate oned comparison CDF plots for all parameters that are
         common to all result files
 
         Parameters
@@ -942,55 +1283,62 @@
         """
         error_message = (
             "Failed to generate a comparison CDF plot for %s because {}"
         )
         for param in self.same_parameters:
             arguments = [
                 self.savedir, param, self.same_samples[param],
-                latex_labels[param], self.colors, self.linestyles
+                latex_labels[param], self.colors, self.linestyles,
+                self.preliminary_comparison_pages, self.checkpoint
             ]
             self._try_to_make_a_plot(
                 arguments, self._oned_cdf_comparison_plot,
                 error_message % (param)
             )
             continue
 
     @staticmethod
     def _oned_cdf_comparison_plot(
-        savedir, parameter, samples, latex_label, colors, linestyles=None
+        savedir, parameter, samples, latex_label, colors, linestyles=None,
+        preliminary=False, checkpoint=False
     ):
         """Generate a oned comparison CDF plot for a given parameter
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         parameter: str
             the name of the parameter that you wish to make a oned comparison
             histogram for
         samples: dict
-            dictionary of pesummary.utils.utils.Array objects containing the
+            dictionary of pesummary.utils.array.Array objects containing the
             samples that correspond to parameter for each result file. The key
             should be the corresponding label
         latex_label: str
             the latex label for parameter
         colors: list
             list of colors to be used to distinguish different result files
         linestyles: list, optional
             list of linestyles used to distinguish different result files
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        filename = os.path.join(
+            savedir, "combined_cdf_{}.png".format(parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         keys = list(samples.keys())
         same_samples = [samples[key] for key in keys]
         fig = core._1d_cdf_comparison_plot(
             parameter, same_samples, colors, latex_label, keys, linestyles
         )
         _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "combined_cdf_{}".format(parameter)
-            )
+            fig, filename, preliminary=preliminary
         )
 
     def box_plot_comparison_plot(self, label):
         """Generate comparison box plots for all parameters that are
         common to all result files
 
         Parameters
@@ -1000,51 +1348,60 @@
         """
         error_message = (
             "Failed to generate a comparison box plot for %s because {}"
         )
         for param in self.same_parameters:
             arguments = [
                 self.savedir, param, self.same_samples[param],
-                latex_labels[param], self.colors
+                latex_labels[param], self.colors,
+                self.preliminary_comparison_pages, self.checkpoint
             ]
             self._try_to_make_a_plot(
                 arguments, self._box_plot_comparison_plot,
                 error_message % (param)
             )
             continue
 
     @staticmethod
     def _box_plot_comparison_plot(
-        savedir, parameter, samples, latex_label, colors
+        savedir, parameter, samples, latex_label, colors, preliminary=False,
+        checkpoint=False
     ):
         """Generate a comparison box plot for a given parameter
 
         Parameters
         ----------
         savedir: str
             the directory you wish to save the plot in
         parameter: str
             the name of the parameter that you wish to make a oned comparison
             histogram for
         samples: dict
-            dictionary of pesummary.utils.utils.Array objects containing the
+            dictionary of pesummary.utils.array.Array objects containing the
             samples that correspond to parameter for each result file. The key
             should be the corresponding label
         latex_label: str
             the latex label for parameter
         colors: list
             list of colors to be used to distinguish different result files
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
+        filename = os.path.join(
+            savedir, "combined_boxplot_{}.png".format(parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
         same_samples = [val for key, val in samples.items()]
         fig = core._comparison_box_plot(
             parameter, same_samples, colors, latex_label,
             list(samples.keys())
         )
         _PlotGeneration.save(
-            fig, os.path.join(savedir, "combined_boxplot_{}".format(parameter))
+            fig, filename, preliminary=preliminary
         )
 
     def custom_plot(self, label):
         """Generate custom plots according to the passed python file
 
         Parameters
         ----------
@@ -1063,9 +1420,9 @@
         for num, i in enumerate(methods):
             fig = i(
                 list(self.samples[label].keys()), self.samples[label]
             )
             _PlotGeneration.save(
                 fig, os.path.join(
                     self.savedir, "{}_custom_plotting_{}".format(label, num)
-                )
+                ), preliminary=self.preliminary_pages[label]
             )
```

### Comparing `pesummary-0.9.1/pesummary/core/webpage/webpage.py` & `pesummary-1.0.0/pesummary/core/webpage/webpage.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,33 +1,23 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-#                     Edward Fauchon-Jones <edward.fauchon-jones@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import pesummary
 from pesummary.core.webpage import tables
 from pesummary.core.webpage.base import Base
 
-import sys
 from pygments import highlight
 from pygments.lexers import get_lexer_by_name
 from pygments.formatters import HtmlFormatter
 import time
 
+__author__ = [
+    "Charlie Hoy <charlie.hoy@ligo.org>",
+    "Edward Fauchon-Jones <edward.fauchon-jones@ligo.org>"
+]
+
 BOOTSTRAP = """<!DOCTYPE html>
 
 <!--
                                     Made by
             ____  ___________
            / __ \/ ____/ ___/__  ______ ___  ____ ___  ____ ________  __
           / /_/ / __/  \__ \/ / / / __ `__ \/ __ `__ \/ __ `/ ___/ / / /
@@ -36,15 +26,15 @@
                                                                /____/
 
                                    MIT License
 
        PESummary was developed by Hoy et al. and source code can be seen
        here: git.ligo.org/lscsoft/pesummary. If you wish to use PESummary
    for your own work, please cite PESummary. The following page gives details
-   https://lscsoft.docs.ligo.org/pesummary/stable_docs/citing_pesummary.html.
+   https://lscsoft.docs.ligo.org/pesummary/stable/citing_pesummary.html.
                                      Thanks!
   -->
 <html lang='en'>
     <title>title</title>
     <meta charset='utf-8'>
     <meta name='viewport' content='width=device-width, initial-scale=1'>
     <link rel='stylesheet' href='https://maxcdn.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css'>
@@ -62,19 +52,22 @@
     <script src='./js/multi_dropbar.js'></script>
     <script src='./js/multiple_posteriors.js'></script>
     <script src='./js/search.js'></script>
     <script src='./js/side_bar.js'></script>
     <script src='./js/html_to_csv.js'></script>
     <script src='./js/html_to_json.js'></script>
     <script src='./js/html_to_shell.js'></script>
+    <script src='./js/expert.js'></script>
     <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
     <link rel="stylesheet" href="./css/navbar.css">
     <link rel="stylesheet" href="./css/font.css">
     <link rel="stylesheet" href="./css/table.css">
     <link rel="stylesheet" href="./css/image_styles.css">
+    <link rel="stylesheet" href="./css/watermark.css">
+    <link rel="stylesheet" href="./css/toggle.css">
 """
 
 OTHER_SCRIPTS = """    <script src='https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js'></script>
     <script src='https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js'></script>
     <script src='https://maxcdn.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js'></script>
     <script src='../js/combine_corner.js'></script>
     <script src='../js/grab.js'></script>
@@ -82,19 +75,22 @@
     <script src='../js/multi_dropbar.js'></script>
     <script src='../js/multiple_posteriors.js'></script>
     <script src='../js/search.js'></script>
     <script src='../js/side_bar.js'></script>
     <script src='../js/html_to_csv.js'></script>
     <script src='../js/html_to_json.js'></script>
     <script src='../js/html_to_shell.js'></script>
+    <script src='../js/expert.js'></script>
     <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
     <link rel="stylesheet" href="../css/navbar.css">
     <link rel="stylesheet" href="../css/font.css">
     <link rel="stylesheet" href="../css/table.css">
     <link rel="stylesheet" href="../css/image_styles.css">
+    <link rel="stylesheet" href="../css/watermark.css">
+    <link rel="stylesheet" href="../css/toggle.css">
 """
 
 
 def make_html(web_dir, title="Summary Pages", pages=None, stylesheets=[],
               label=None):
     """Make the initial html page.
 
@@ -175,26 +171,37 @@
 
     def _header(self, approximant):
         """
         """
         self.add_content("<h7 hidden>{}</h7>".format(self.label))
         self.add_content("<h7 hidden>{}</h7>".format(approximant))
 
-    def _footer(self, user, rundir):
+    def _footer(self, user, rundir, fix_bottom=False):
         """
         """
         self.add_content("<script>")
         self.add_content("$(document).ready(function(){", indent=2)
         self.add_content("$('[data-toggle=\"tooltip\"]').tooltip();", indent=4)
         self.add_content("});", indent=2)
         self.add_content("</script>")
-        self.make_div(
-            _class='jumbotron', _style='margin-bottom:0; line-height: 0.5;'
-            + 'background-color:#989898; bottom:0; position:bottom;'
-            + 'width:100%')
+        if fix_bottom:
+            self.make_div(
+                _class='container', _style='bottom: 0px; '
+                + 'top: 0px; min-height: 100%; left: 0; right: 0;'
+                + 'min-width: 100%; padding-left: 0px; padding-right: 0px'
+            )
+            self.make_div(
+                _class='jumbotron', _style='margin-bottom:0; line-height: 0.5;'
+                + 'background-color:#989898; bottom:0; position:absolute;'
+                + 'width:100%')
+        else:
+            self.make_div(
+                _class='jumbotron', _style='margin-bottom:0; line-height: 0.5;'
+                + 'background-color:#989898; bottom:0; position:bottom;'
+                + 'width:100%')
         self.add_content("<div class='container'>")
         self.add_content("<div class='row'>", indent=2)
         self.add_content("<div class='col-sm-4 icon-bar'>", indent=4)
         self.add_content("<div class='icon'>", indent=6)
         self.add_content(
             "<a href='https://git.ligo.org/lscsoft/pesummary/tree/v{}' "
             "data-toggle='tooltip' title='View PESummary-v{}'>"
@@ -212,15 +219,15 @@
         self.add_content(
             "<a href='https://lscsoft.docs.ligo.org/pesummary/' "
             "data-toggle='tooltip' title='View the docs!'>"
             "<i class='fa fa-book' style='font-size: 30px; color: #E8E8E8; "
             "font-weight: 900; padding-right:10px'></i></a>"
         )
         link = (
-            "https://lscsoft.docs.ligo.org/pesummary/stable_docs/tutorials/"
+            "https://lscsoft.docs.ligo.org/pesummary/stable/tutorials/"
             "make_your_own_page_from_metafile.html"
         )
         self.add_content(
             "<a href='{}' data-toggle='tooltip' title='Make your own page'>"
             "<i class='fa fa-window-restore' style='font-size: 30px; "
             "color: #E8E8E8; font-weight: 900; padding-right:10px'></i>"
             "</a>".format(link)
@@ -234,14 +241,16 @@
             "by {} at {} on {}</p>".format(
                 user, time.strftime("%H:%M"), time.strftime("%B %d %Y")
             )
         )
         self.add_content("</div>", indent=4)
         self.add_content("</div>", indent=2)
         self.add_content("</div>")
+        if fix_bottom:
+            self.add_content("</div>")
         self.end_div()
 
     def _setup_navbar(self, background_colour):
         if background_colour == "navbar-dark" or background_colour is None:
             self.add_content("<nav class='navbar navbar-expand-sm navbar-dark "
                              "bg-dark fixed-top'>\n")
         else:
@@ -263,35 +272,47 @@
         title: str, optional
             header title of html page
         approximant: str, optional
             the approximant that you are analysing
         """
         self._header(approximant)
 
-    def make_footer(self, user=None, rundir=None):
+    def make_footer(self, user=None, rundir=None, fix_bottom=False):
         """Make footer for document in bootstrap format.
         """
-        self._footer(user, rundir)
+        self._footer(user, rundir, fix_bottom=fix_bottom)
 
-    def make_banner(self, approximant=None, key="Summary", _style=None, link=None):
+    def make_banner(
+        self, approximant=None, key="Summary", _style=None, link=None,
+        custom=""
+    ):
         """Make a banner for the document.
         """
         self.make_div(indent=2, _class='banner', _style=_style)
         self.add_content("%s" % (approximant))
         self.end_div()
         self.make_div(indent=2, _class='paragraph')
         if key == "Summary":
             self.add_content(
                 "The figures below show the summary plots for the run")
         elif key == "config":
             self.add_content(
                 "Below is the config file for %s" % (approximant))
+        elif key == "prior":
+            self.add_content(
+                "Below is the prior file for %s" % (approximant))
         elif key == "corner":
             self.add_content(
                 "Below is the custom corner plotter for %s" % (approximant))
+        elif key == "additional":
+            self.add_content(
+                "Below we show plots which have been generated previously "
+                "and passed to pesummary via the `--add_existing_plot` "
+                "command line argument"
+            )
         elif key == "interactive_corner":
             self.add_content(
                 "Below are interative corner plots for %s. Simply use the "
                 "Box Select to select the points you wish to look at" % (
                     approximant
                 )
             )
@@ -365,23 +386,25 @@
             self.add_content(
                 "Below are links to download all relevant information"
             )
         elif key == "About":
             self.add_content(
                 "Below is information about how these pages were generated"
             )
+        elif key == "custom":
+            self.add_content(custom)
         else:
             self.add_content(
                 "The figures below show the plots for %s" % (approximant))
         self.end_div()
 
     def make_navbar(self, links=None, samples_path="./samples", search=True,
                     histogram_download=None,
                     background_color="navbar-dark",
-                    hdf5=False, about=True):
+                    hdf5=False, about=True, toggle=False):
         """Make a navigation bar in boostrap format.
 
         Parameters
         ----------
         links: list, optional
             list giving links that you want your navbar to include. If a
             dropdown option is required, give a 2d list showing the main link
@@ -426,76 +449,148 @@
                                                  "aria-haspopup='true' "
                                                  "aria-expanded='false'>{}</a>\n".format(j[0], j[0]), indent=16)
                                 self.add_content("<ul class='dropdown-menu' "
                                                  "aria-labelledby='{}'>\n".format(j[0]), indent=16)
                                 for k in j[1]:
                                     if type(k) == dict:
                                         key = list(k.keys())[0]
-                                        self.add_content(
-                                            "<li class='dropdown-item' href='#' "
-                                            "onclick='grab_html(\"{}\", label=\"{}\")'>"
-                                            "<a>{}</a></li>\n".format(key, k[key], key), indent=18)
+                                        if "external:" in k[key]:
+                                            self.add_content(
+                                                "<li class='dropdown-item' "
+                                                "href='#' onclick='window.location"
+                                                "=\"{}\"'><a>{}</a></li>\n".format(
+                                                    k[key].split("external:")[1],
+                                                    key
+                                                ), indent=18
+                                            )
+                                        else:
+                                            self.add_content(
+                                                "<li class='dropdown-item' "
+                                                "href='#' onclick='grab_html"
+                                                "(\"{}\", label=\"{}\")'>"
+                                                "<a>{}</a></li>\n".format(
+                                                    key, k[key], key
+                                                ), indent=18
+                                            )
                                     else:
                                         self.add_content(
                                             "<li class='dropdown-item' href='#' "
                                             "onclick='grab_html(\"{}\")'>"
                                             "<a>{}</a></li>\n".format(k, k), indent=18)
 
                                 self.add_content("</ul>", indent=16)
                                 self.add_content("</li>", indent=14)
                             else:
                                 for k in j:
                                     if type(k) == dict:
                                         key = list(k.keys())[0]
-                                        self.add_content(
-                                            "<li class='dropdown-item' href='#' "
-                                            "onclick='grab_html(\"{}\", label=\"{}\")'>"
-                                            "<a>{}</a></li>\n".format(key, k[key], key), indent=14)
+                                        if "external:" in k[key]:
+                                            self.add_content(
+                                                "<li class='dropdown-item' "
+                                                "href='#' onclick='window.location"
+                                                "=\"{}\"'><a>{}</a></li>\n".format(
+                                                    k[key].split("external:")[1],
+                                                    key
+                                                ), indent=18
+                                            )
+                                        else:
+                                            self.add_content(
+                                                "<li class='dropdown-item' "
+                                                "href='#' onclick='grab_html"
+                                                "(\"{}\", label=\"{}\")'>"
+                                                "<a>{}</a></li>\n".format(
+                                                    key, k[key], key
+                                                ), indent=14
+                                            )
 
                                     else:
                                         self.add_content(
                                             "<li class='dropdown-item' href='#' "
                                             "onclick='grab_html(\"{}\")'>"
                                             "<a>{}</a></li>\n".format(k, k), indent=14)
 
                         else:
                             if type(j[0]) == dict:
                                 key = list(j[0].keys())[0]
-                                self.add_content(
-                                    "<li class='dropdown-item' href='#' "
-                                    "onclick='grab_html(\"{}\", label=\"{}\")'>"
-                                    "<a>{}</a></li>\n".format(key, j[0][key], key), indent=14)
+                                if 'external:' in j[0][key]:
+                                    self.add_content(
+                                        "<li class='dropdown-item' href='#' "
+                                        "onclick='window.location=\"{}\"'>"
+                                        "<a>{}</a></li>\n".format(
+                                            j[0][key].split("external:")[1], key
+                                        ), indent=14
+                                    )
+                                else:
+                                    self.add_content(
+                                        "<li class='dropdown-item' href='#' "
+                                        "onclick='grab_html(\"{}\", label=\"{}\")'>"
+                                        "<a>{}</a></li>\n".format(key, j[0][key], key),
+                                        indent=14
+                                    )
 
                             else:
-                                self.add_content(
-                                    "<li class='dropdown-item' href='#' "
-                                    "onclick='grab_html(\"{}\")'>"
-                                    "<a>{}</a></li>\n".format(j[0], j[0]), indent=14)
+                                if "external:" in j[0]:
+                                    self.add_content(
+                                        "<li class='dropdown-item' href='#' "
+                                        "onclick='window.location=\"{}\"'>"
+                                        "<a>{}</a></li>\n".format(
+                                            j[0].split("external:")[1], j[0]
+                                        ), indent=14
+                                    )
+                                else:
+                                    self.add_content(
+                                        "<li class='dropdown-item' href='#' "
+                                        "onclick='grab_html(\"{}\")'>"
+                                        "<a>{}</a></li>\n".format(j[0], j[0]),
+                                        indent=14
+                                    )
 
                 self.add_content("</ul>\n", indent=12)
                 self.add_content("</li>\n", indent=10)
             else:
                 self.add_content("<li class='nav-item'>\n", indent=8)
                 if i == "home":
-                    self.add_content("<a class='nav-link' "
-                                     "href='#' onclick='grab_html(\"{}\")'"
-                                     ">{}</a>\n".format(i, i), indent=10)
+                    if "external:" in i:
+                        self.add_content(
+                            "<a class='nav-link' href='#' onclick='window.location"
+                            "=\"{}\"'>{}</a>\n".format(i.split("external:")[1], i),
+                            indent=10)
+                    else:
+                        self.add_content("<a class='nav-link' "
+                                         "href='#' onclick='grab_html(\"{}\")'"
+                                         ">{}</a>\n".format(i, i), indent=10)
                 else:
                     if type(i) == dict:
                         key = list(i.keys())[0]
-                        self.add_content(
-                            "<a class='nav-link' "
-                            "href='#' onclick='grab_html(\"{}\", label=\"{}\")'"
-                            ">{}</a>\n".format(key, i[key], key), indent=10)
+                        if "external:" in i[key]:
+                            self.add_content(
+                                "<a class='nav-link' "
+                                "href='#' onclick='window.location=\"{}\"'"
+                                ">{}</a>\n".format(
+                                    i[key].split("external:")[1], key
+                                ), indent=10)
+                        else:
+                            self.add_content(
+                                "<a class='nav-link' "
+                                "href='#' onclick='grab_html(\"{}\", label=\"{}\")'"
+                                ">{}</a>\n".format(key, i[key], key), indent=10)
 
                     else:
-                        self.add_content(
-                            "<a class='nav-link' "
-                            "href='#' onclick='grab_html(\"{}\")'"
-                            ">{}</a>\n".format(i, i), indent=10)
+                        if "external:" in i:
+                            self.add_content(
+                                "<a class='nav-link' "
+                                "href='#' onclick='window.location=\"{}\"'"
+                                ">{}</a>\n".format(
+                                    i.split("external:")[1], i
+                                ), indent=10)
+                        else:
+                            self.add_content(
+                                "<a class='nav-link' "
+                                "href='#' onclick='grab_html(\"{}\")'"
+                                ">{}</a>\n".format(i, i), indent=10)
 
                 self.add_content("</li>\n", indent=8)
         self.add_content("</ul>\n", indent=6)
         self.add_content("</div>\n", indent=4)
         if histogram_download:
             self.add_content("<a href='%s' download>" % (histogram_download),
                              indent=4)
@@ -504,14 +599,26 @@
                 "<i class='fa fa-download'></i> Histogram Data</button>", indent=6)
             self.add_content("</a>", indent=4)
 
         self.add_content("<div class='collapse navbar-collapse' id='collapsibleNavbar'>\n", indent=4)
         self.add_content(
             "<ul class='navbar-nav flex-row ml-md-auto d-none d-md-flex'"
             "style='margin-right:1em;'>\n", indent=6)
+        if toggle:
+            self.add_content(
+                "<div style='margin-top:0.5em; margin-right: 1em;' "
+                "data-toggle='tooltip' title='Activate expert mode'>", indent=6
+            )
+            self.add_content("<label class='switch'>", indent=8)
+            self.add_content(
+                "<input type='checkbox' onchange='show_expert_div()'>", indent=10
+            )
+            self.add_content("<span class='slider round'></span>", indent=10)
+            self.add_content("</label>", indent=8)
+            self.add_content("</div>", indent=6)
         self.add_content(
             "<a class='nav-link' href='#', onclick='grab_html(\"{}\")'"
             ">{}</a>\n".format("Downloads", "Downloads"), indent=2
         )
         if about:
             self.add_content(
                 "<a class='nav-link' href='#', onclick='grab_html(\"{}\")'"
@@ -697,15 +804,16 @@
         styles = formatter.get_style_defs('.highlight')
         styles += ".highlight {margin: 5px; padding: 10px; background: #FFFFFF}"
         return styles
 
     def make_table_of_images(self, contents=None, rows=None, columns=None,
                              code="modal", cli=None, autoscale=False,
                              unique_id=None, captions=None, extra_div=False,
-                             mcmc_samples=False):
+                             mcmc_samples=False, margin_left=None, display=None,
+                             container_id=None, **kwargs):
         """Generate a table of images in bootstrap format.
 
         Parameters
         ----------
         headings: list, optional
             list of headings
         contents: list, optional
@@ -717,16 +825,18 @@
             width of the images in the table
         container: bool, optional
             if True, the table of images is placed inside a container
         """
         table = tables.table_of_images(contents, rows, columns, self.html_file,
                                        code=code, cli=cli, autoscale=autoscale,
                                        unique_id=unique_id, captions=captions,
-                                       extra_div=extra_div,
-                                       mcmc_samples=mcmc_samples)
+                                       extra_div=extra_div, display=display,
+                                       mcmc_samples=mcmc_samples,
+                                       margin_left=margin_left,
+                                       container_id=container_id, **kwargs)
         table.make()
 
     def insert_image(self, path, justify="center", code=None):
         """Generate an image in bootstrap format.
 
         Parameters
         ----------
@@ -934,14 +1044,29 @@
         self.add_content("<button value='test' class='btn btn-info btn-xs' "
                          "style='cursor: pointer' "
                          "data-toggle='popover' data-placement='top' "
                          "data-content='%s'>Command Line</button>" % (cli),
                          indent=12)
         self.end_div(0)
 
+    def make_watermark(self, text="Preliminary"):
+        """Add a watermark to the html page
+
+        Parameters
+        ----------
+        text: str
+            work you wish to use as a watermark
+        """
+        self.add_content("<div id='background'>")
+        for _ in range(3):
+            self.add_content(
+                "<p id='bg-text'>{} {}</p>".format(text, text)
+            )
+        self.end_div()
+
     def export(
         self, filename, csv=True, json=False, shell=False, histogram_dat=None,
         requirements=False, conda=False, margin_top="-4em", margin_bottom="5em",
     ):
         """Make a button which to export a html table to csv
 
         Parameters
```

### Comparing `pesummary-0.9.1/pesummary/core/webpage/tables.py` & `pesummary-1.0.0/pesummary/core/webpage/tables.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,30 +1,21 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from pesummary.core.webpage.base import Base
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class table_of_images(Base):
 
     def __init__(self, content, rows, columns, html_file, code, cli,
                  autoscale=False, unique_id=None, captions=None, extra_div=False,
-                 mcmc_samples=False):
+                 mcmc_samples=False, margin_left=None, display=None,
+                 container_id=None, close_container=True,
+                 add_to_open_container=False):
         """
 
         Parameters
         ----------
         content: nd list
             nd list containing the image paths that you want to include in your
             table. Sorted by columns [[column1], [column2]]
@@ -36,14 +27,19 @@
         self.code = code
         self.cli = cli
         self.captions = captions
         self.autoscale = autoscale
         self.unique_id = unique_id
         self.extra_div = extra_div
         self.mcmc_samples = mcmc_samples
+        self.margin_left = margin_left
+        self.display = display
+        self.container_id = container_id
+        self.close_container = close_container
+        self.add_to_open_container = add_to_open_container
         if self.unique_id is not None:
             self.modal_id = "Modal_{}".format(self.unique_id)
             self.demo_id = "demo_{}".format(self.unique_id)
         else:
             self.modal_id = "MyModal"
             self.demo_id = "demo"
         self._add_scripts()
@@ -59,34 +55,41 @@
             string += ", \"{}\"".format(self.modal_id)
         if self.mcmc_samples:
             string += ", mcmc_samples=\"{}\"".format(self.mcmc_samples)
         string += ")'>\n"
         self.add_content(string, indent=indent)
 
     def make(self):
-        self.add_content("<script>")
-        self.add_content("$(document).ready(function(){", indent=2)
-        self.add_content("$('[data-toggle=\"popover\"]').popover();", indent=4)
-        self.add_content("});", indent=2)
-        self.add_content("</script>")
-        self.add_content("<style>")
-        self.add_content(".popover {", indent=2)
-        self.add_content("max-width: 550px;", indent=4)
-        self.add_content("width: 550px;", indent=4)
-        self.add_content("}", indent=2)
-        self.add_content("</style>")
-        self.make_container()
+        if not self.add_to_open_container:
+            self.add_content("<script>")
+            self.add_content("$(document).ready(function(){", indent=2)
+            self.add_content("$('[data-toggle=\"popover\"]').popover();", indent=4)
+            self.add_content("});", indent=2)
+            self.add_content("</script>")
+            self.add_content("<style>")
+            self.add_content(".popover {", indent=2)
+            self.add_content("max-width: 550px;", indent=4)
+            self.add_content("width: 550px;", indent=4)
+            self.add_content("}", indent=2)
+            self.add_content("</style>")
+            self.make_container(
+                display=self.display, container_id=self.container_id
+            )
         self.make_div(2, _class="mx-auto d-block", _style=None)
         if self.rows == 1:
             _id = self.content[0][0].split("/")[-1][:-4]
             self._insert_image(self.content[0][0], 900, 4, _id, justify="left")
             self.make_div(2, _class=None, _style="float: right;")
             for num, i in enumerate(self.content[1]):
                 _id = i.split("/")[-1][:-4]
-                self.make_div(2, _class="row")
+                if self.margin_left is not None:
+                    _style = "margin-left: {}".format(self.margin_left)
+                else:
+                    _style = None
+                self.make_div(2, _class="row", _style=_style)
                 self.make_div(4, _class="column")
                 self.add_content("<a>", 6)
                 self._insert_image(i, 415, 8, _id, justify=None)
                 self.add_content("</a>", 6)
                 if self.captions:
                     self.add_content("<div class='row justify-content-center'>", indent=6)
                     self.make_div(6, _class="col-sm-4", _style=None)
@@ -103,15 +106,19 @@
             self.end_div(2)
         else:
             ind = 0
             width = "450"
             captions_margin_left = "-70"
             _class = "row justify-content-center"
             self.make_div(4, _class=_class, _style=None)
-            self.make_div(6, _class="row", _style=None)
+            if self.margin_left is not None:
+                _style = "margin-left: {}".format(self.margin_left)
+            else:
+                _style = None
+            self.make_div(6, _class="row", _style=_style)
             for idx, i in enumerate(self.content):
                 if self.autoscale:
                     width = str(1350 / len(i))
                     captions_margin_left = str(-1350. / (9.5 * len(i)))
                 self.make_div(8, _class="column", _style="padding-left: 1em;")
                 for num, j in enumerate(i):
                     _id = j.split("/")[-1][:-4]
@@ -161,8 +168,9 @@
                     if self.cli or self.captions:
                         self.end_div(10)
                     ind += 1
                 self.end_div(8)
             self.end_div(6)
             self.end_div(4)
         self.end_div(2)
-        self.end_container()
+        if self.close_container:
+            self.end_container()
```

### Comparing `pesummary-0.9.1/pesummary/core/webpage/main.py` & `pesummary-1.0.0/pesummary/core/webpage/main.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,38 +1,28 @@
-# Copyright (C) 2019 Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
 import sys
 import uuid
 from glob import glob
-from operator import itemgetter
 from pathlib import Path
-from shutil import which
+import shutil
 
-from scipy import stats
 import numpy as np
+import math
 
 import pesummary
-from pesummary import conf
-from pesummary.utils.utils import logger, LOG_FILE, jensen_shannon_divergence
+from pesummary import conf, __version_string__
+from pesummary.utils.utils import (
+    logger, LOG_FILE, jensen_shannon_divergence_from_pdfs, safe_round, make_dir
+)
 from pesummary.core.webpage import webpage
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class PlotCaption(object):
     """Class to handle the generation of a plot caption
 
     Parameters
     ----------
     plot: str
@@ -115,43 +105,88 @@
         file_versions=None, hdf5=None, colors=None, custom_plotting=None,
         existing_labels=None, existing_config=None, existing_file_version=None,
         existing_injection_data=None, existing_samples=None,
         existing_metafile=None, existing_file_kwargs=None,
         existing_weights=None, add_to_existing=False, notes=None,
         disable_comparison=False, disable_interactive=False,
         package_information={"packages": [], "manager": "pypi"},
-        mcmc_samples=False, external_hdf5_links=False
+        mcmc_samples=False, external_hdf5_links=False, key_data=None,
+        existing_plot=None, disable_expert=False, analytic_priors=None,
     ):
         self.webdir = webdir
+        make_dir(self.webdir)
+        make_dir(os.path.join(self.webdir, "html"))
+        make_dir(os.path.join(self.webdir, "css"))
         self.samples = samples
         self.labels = labels
         self.publication = publication
         self.user = user
         self.config = config
         self.same_parameters = same_parameters
         self.base_url = base_url
         self.file_versions = file_versions
+        if self.file_versions is None:
+            self.file_versions = {
+                label: "No version information found" for label in self.labels
+            }
         self.hdf5 = hdf5
         self.colors = colors
+        if self.colors is None:
+            self.colors = list(conf.colorcycle)
         self.custom_plotting = custom_plotting
         self.existing_labels = existing_labels
         self.existing_config = existing_config
         self.existing_file_version = existing_file_version
         self.existing_samples = existing_samples
         self.existing_metafile = existing_metafile
         self.existing_file_kwargs = existing_file_kwargs
         self.add_to_existing = add_to_existing
+        self.analytic_priors = analytic_priors
+        if self.analytic_priors is None:
+            self.analytic_priors = {label: None for label in self.samples.keys()}
+        self.key_data = key_data
+        _label = self.labels[0]
+        if self.samples is not None:
+            if key_data is None:
+                self.key_data = {
+                    label: _samples.key_data for label, _samples in
+                    self.samples.items()
+                }
+            self.key_data_headings = sorted(
+                list(self.key_data[_label][list(self.samples[_label].keys())[0]])
+            )
+            self.key_data_table = {
+                label: {
+                    param: [
+                        safe_round(self.key_data[label][param][key], 3) for key
+                        in self.key_data_headings
+                    ] for param in self.samples[label].keys()
+                } for label in self.labels
+            }
         self.notes = notes
         self.make_interactive = not disable_interactive
         self.package_information = package_information
         self.mcmc_samples = mcmc_samples
         self.external_hdf5_links = external_hdf5_links
+        self.existing_plot = existing_plot
+        self.expert_plots = not disable_expert
         self.make_comparison = (
             not disable_comparison and self._total_number_of_labels > 1
         )
+        self.preliminary_pages = {label: False for label in self.labels}
+        self.all_pages_preliminary = False
+        self._additional_1d_pages = {label: [] for label in self.labels}
+        if self.additional_1d_pages is not None:
+            for j, _parameters in self.additional_1d_pages.items():
+                for i in self.labels:
+                    if all(
+                            param in self.samples[i].keys() for param in
+                            _parameters
+                    ):
+                        self._additional_1d_pages[i].append(j)
         self.categories = self.default_categories()
         self.popular_options = self.default_popular_options()
         self.navbar = {
             "home": self.make_navbar_for_homepage(),
             "result_page": self.make_navbar_for_result_page(),
             "comparison": self.make_navbar_for_comparison_page()
         }
@@ -172,21 +207,80 @@
                 self.comparison_stats = None
                 logger.info(
                     "Failed to generate comparison statistics because {}. As a "
                     "result they will not be added to the webpages".format(e)
                 )
 
     @property
+    def _metafile(self):
+        return (
+            "posterior_samples.json" if not self.hdf5 else
+            "posterior_samples.h5"
+        )
+
+    @property
     def _total_number_of_labels(self):
         _number_of_labels = 0
         for item in [self.labels, self.existing_labels]:
             if isinstance(item, list):
                 _number_of_labels += len(item)
         return _number_of_labels
 
+    def copy_css_and_js_scripts(self):
+        """Copy css and js scripts from the package to the web directory
+        """
+        import pkg_resources
+        import shutil
+        files_to_copy = []
+        path = pkg_resources.resource_filename("pesummary", "core")
+        scripts = glob(os.path.join(path, "js", "*.js"))
+        for i in scripts:
+            files_to_copy.append(
+                [i, os.path.join(self.webdir, "js", os.path.basename(i))]
+            )
+        scripts = glob(os.path.join(path, "css", "*.css"))
+        for i in scripts:
+            files_to_copy.append(
+                [i, os.path.join(self.webdir, "css", os.path.basename(i))]
+            )
+        for _dir in ["js", "css"]:
+            try:
+                os.mkdir(os.path.join(self.webdir, _dir))
+            except FileExistsError:
+                pass
+        for ff in files_to_copy:
+            shutil.copy(ff[0], ff[1])
+
+    def make_modal_carousel(
+        self, html_file, image_contents, unique_id=False, **kwargs
+    ):
+        """Make a modal carousel for a table of images
+
+        Parameters
+        ----------
+        html_file: pesummary.core.webpage.webpage.page
+            open html file
+        image_contents: list
+            list of images to place in a table
+        unique_id: Bool, optional
+            if True, assign a unique identifer to the modal
+        **kwargs: dict, optional
+            all additional kwargs passed to `make_table_of_images` function
+        """
+        if unique_id:
+            unique_id = '{}'.format(uuid.uuid4().hex.upper()[:6])
+        else:
+            unique_id = None
+        html_file.make_table_of_images(
+            contents=image_contents, unique_id=unique_id, **kwargs
+        )
+        images = [y for x in image_contents for y in x]
+        html_file.make_modal_carousel(images=images, unique_id=unique_id)
+        return html_file
+
     def generate_comparison_statistics(self):
         """Generate comparison statistics for all parameters that are common to
         all result files
         """
         data = {
             i: self._generate_comparison_statistics(
                 i, [self.samples[j][i] for j in self.labels]
@@ -198,56 +292,59 @@
         """Generate comparison statistics for a set of samples
 
         Parameters
         ----------
         samples: list
             list of samples for each result file
         """
-        from scipy.stats import gaussian_kde
         from pesummary.utils.utils import kolmogorov_smirnov_test
 
         rows = range(len(samples))
         columns = range(len(samples))
         ks = [
             [
                 kolmogorov_smirnov_test([samples[i], samples[j]]) for i in
                 rows
             ] for j in columns
         ]
-        js = [
-            [
-                self._jensen_shannon_divergence(param, [samples[i], samples[j]])
-                for i in rows
-            ] for j in columns
-        ]
-        return [ks, js]
+        # JS divergence is symmetric and therefore we just need to loop over
+        # one triangle
+        js = np.zeros((len(samples), len(samples)))
+        pdfs = self._kde_from_same_samples(param, samples)
+        for num, i in enumerate(range(1, len(js))):
+            for idx, j in enumerate(range(0, i)):
+                js[i][idx] += jensen_shannon_divergence_from_pdfs(
+                    [pdfs[i], pdfs[j]]
+                )
+        js = js + js.T
+        return [ks, js.tolist()]
 
-    def _jensen_shannon_divergence(self, param, samples):
-        """Return the Jensen Shannon divergence between two sets of samples
+    def _kde_from_same_samples(self, param, samples, **kwargs):
+        """Generate KDEs for a set of samples
 
         Parameters
         ----------
         param: str
             The parameter that the samples belong to
         samples: list
-            2d list containing the samples you wish to calculate the Jensen
-            Shannon divergence between
+            list of samples for each result file
         """
-        return jensen_shannon_divergence([samples[0], samples[1]])
+        from pesummary.utils.utils import samples_to_kde
+        return samples_to_kde(samples, **kwargs)
 
     @staticmethod
     def get_executable(executable):
         """Return the path to an executable
 
         Parameters
         ----------
         executable: str
             the name of the executable you wish to find
         """
-        return which(
+        return shutil.which(
             executable,
             path=os.pathsep.join((
                 os.getenv("PATH", ""),
                 str(Path(sys.executable).parent),
             )),
         )
 
@@ -275,102 +372,135 @@
         """Make a navbar for the result page homepage
         """
         links = {
             i: ["1d Histograms", [{"Custom": i}, {"All": i}]] for i in
             self.labels
         }
         for num, label in enumerate(self.labels):
-            for j in self.categorize_parameters(self.samples[label].keys()):
+            _params = list(self.samples[label].keys())
+            if len(self._additional_1d_pages[label]):
+                _params += self._additional_1d_pages[label]
+            for j in self.categorize_parameters(_params):
                 j = [j[0], [{k: label} for k in j[1]]]
                 links[label].append(j)
 
         final_links = {
             i: [
                 "home", ["Result Pages", self._result_page_links()],
                 {"Corner": i}, {"Config": i}, links[i]
             ] for i in self.labels
         }
         if self.make_comparison:
             for label in self.labels:
                 final_links[label][1][1] += ["Comparison"]
+        _dummy_label = self.labels[0]
+        if len(final_links[_dummy_label][1][1]) > 1:
+            for label in self.labels:
+                _options = [{l: "switch"} for l in self.labels if l != label]
+                if self.make_comparison:
+                    _options.append({"Comparison": "switch"})
+                final_links[label].append(["Switch", _options])
         if self.make_interactive:
             for label in self.labels:
                 final_links[label].append(
                     ["Interactive", [{"Interactive_Corner": label}]]
                 )
+        if self.existing_plot is not None:
+            for _label in self.labels:
+                if _label in self.existing_plot.keys():
+                    final_links[_label].append(
+                        {"Additional": _label}
+                    )
         return final_links
 
     def make_navbar_for_comparison_page(self):
         """Make a navbar for the comparison homepage
         """
         if self.same_parameters is not None:
             links = ["1d Histograms", ["Custom", "All"]]
             for i in self.categorize_parameters(self.same_parameters):
                 links.append(i)
             final_links = [
                 "home", ["Result Pages", self._result_page_links()], links
             ]
             final_links[1][1] += ["Comparison"]
+            final_links.append(
+                ["Switch", [{l: "switch"} for l in self.labels]]
+            )
             if self.make_interactive:
                 final_links.append(
                     ["Interactive", ["Interactive_Ridgeline"]]
                 )
             return final_links
         return None
 
-    def categorize_parameters(self, parameters):
+    def categorize_parameters(
+        self, parameters, starting_letter=True, heading_all=True
+    ):
         """Categorize the parameters into common headings
 
         Parameters
         ----------
         parameters: list
             list of parameters that you would like to sort
         """
         params = []
         for heading, category in self.categories.items():
             if any(
                 any(i[0] in j for j in category["accept"]) for i in parameters
             ):
-                cond = self._condition(category["accept"], category["reject"])
-                params.append(
-                    [heading, self._partition(cond, parameters)]
+                cond = self._condition(
+                    category["accept"], category["reject"],
+                    starting_letter=starting_letter
                 )
+                part = self._partition(cond, parameters)
+                if heading_all and len(part):
+                    part = ["{}_all".format(heading)] + part
+                params.append([heading, part])
         used_headings = [i[0] for i in params]
         other_index = \
             used_headings.index("others") if "others" in used_headings else None
         other_params = []
         for pp in parameters:
             if not any(pp in j[1] for j in params):
                 if other_index is not None:
                     params[other_index][1].append(pp)
                 else:
                     other_params.append(pp)
         if other_index is None:
             params.append(["others", other_params])
         return params
 
-    def _condition(self, true, false):
+    def _condition(self, true, false, starting_letter=False):
         """Setup a condition
 
         Parameters
         ----------
         true: list
             list of strings that you would like to include
         false: list
             list of strings that you would like to neglect
         """
+        def _starting_letter(param, condition):
+            if condition:
+                return param[0]
+            return param
         if len(true) != 0 and len(false) == 0:
-            condition = lambda j: True if any(i in j for i in true) else \
-                False
+            condition = lambda j: True if any(
+                i in _starting_letter(j, starting_letter) for i in true
+            ) else False
         elif len(true) == 0 and len(false) != 0:
-            condition = lambda j: True if any(i not in j for i in false) \
-                else False
+            condition = lambda j: True if any(
+                i not in _starting_letter(j, starting_letter) for i in false
+            ) else False
         elif len(true) and len(false) != 0:
             condition = lambda j: True if any(
-                i in j and all(k not in j for k in false) for i in true
+                i in _starting_letter(j, starting_letter) and all(
+                    k not in _starting_letter(j, starting_letter) for k in false
+                ) for i in true
             ) else False
         return condition
 
     def _partition(self, condition, array):
         """Filter the list according to a condition
 
         Parameters
@@ -392,22 +522,27 @@
         self.make_1d_histogram_pages()
         self.make_corner_pages()
         self.make_config_pages()
         if self.make_comparison:
             self.make_comparison_pages()
         if self.make_interactive:
             self.make_interactive_pages()
+        if self.existing_plot is not None:
+            self.make_additional_plots_pages()
         self.make_error_page()
         self.make_version_page()
         self.make_logging_page()
         if self.notes is not None:
             self.make_notes_page()
         self.make_downloads_page()
         self.make_about_page()
-        self.generate_specific_javascript()
+        try:
+            self.generate_specific_javascript()
+        except Exception:
+            pass
 
     def create_blank_html_pages(self, pages, stylesheets=[]):
         """Create blank html pages
 
         Parameters
         ----------
         pages: list
@@ -415,15 +550,15 @@
         """
         webpage.make_html(
             web_dir=self.webdir, pages=pages, stylesheets=stylesheets
         )
 
     def setup_page(
         self, html_page, links, label=None, title=None, approximant=None,
-        background_colour=None, histogram_download=False
+        background_colour=None, histogram_download=False, toggle=False
     ):
         """Set up each webpage with a header and navigation bar.
 
         Parameters
         ----------
         html_page: str
             String containing the html page that you would like to set up
@@ -441,107 +576,244 @@
             If true, a download link for the each histogram is displayed in
             the navbar
         """
         html_file = webpage.open_html(
             web_dir=self.webdir, base_url=self.base_url, html_page=html_page,
             label=label
         )
+        _preliminary_keys = self.preliminary_pages.keys()
+        if self.all_pages_preliminary:
+            html_file.make_watermark()
+        elif approximant is not None and approximant in _preliminary_keys:
+            if self.preliminary_pages[approximant]:
+                html_file.make_watermark()
+
         html_file.make_header(approximant=approximant)
         if html_page == "home" or html_page == "home.html":
             html_file.make_navbar(
                 links=links, samples_path=self.results_path["home"],
                 background_color=background_colour,
-                hdf5=self.hdf5
+                hdf5=self.hdf5, toggle=toggle
             )
         elif histogram_download:
             html_file.make_navbar(
                 links=links, samples_path=self.results_path["other"],
-                histogram_download=os.path.join(
+                toggle=toggle, histogram_download=os.path.join(
                     "..", "samples", "dat", label, "{}_{}_samples.dat".format(
                         label, html_page
                     )
                 ), background_color=background_colour, hdf5=self.hdf5
             )
         else:
             html_file.make_navbar(
                 links=links, samples_path=self.results_path["home"],
-                background_color=background_colour, hdf5=self.hdf5
+                background_color=background_colour, hdf5=self.hdf5,
+                toggle=toggle
             )
         return html_file
 
     def make_home_pages(self):
         """Wrapper function for _make_home_pages()
         """
         pages = ["{}_{}".format(i, i) for i in self.labels]
         pages.append("home")
         self.create_blank_html_pages(pages)
         self._make_home_pages(pages)
 
-    def _make_home_pages(self, pages):
+    def _make_home_pages(
+        self, pages, title=None, banner="Summary", make_home=True,
+        make_result=True, return_html=False
+    ):
         """Make the home pages
 
         Parameters
         ----------
         pages: list
             list of pages that you wish to create
         """
-        html_file = self.setup_page("home", self.navbar["home"])
-        html_file.make_banner(approximant="Summary", key="Summary")
+        if make_home:
+            html_file = self.setup_page("home", self.navbar["home"], title=title)
+            html_file.make_banner(approximant=banner, key="Summary")
+            if return_html:
+                return html_file
+        if not make_result:
+            return
 
         for num, i in enumerate(self.labels):
             html_file = self.setup_page(
-                i, self.navbar["result_page"][i], i,
+                i, self.navbar["result_page"][i], i, approximant=i,
                 title="{} Summary page".format(i),
-                background_colour=self.colors[num], approximant=i
+                background_colour=self.colors[num]
             )
-            html_file.make_banner(approximant=i, key=i)
+            images, cli, captions = self.default_images_for_result_page(i)
+            _images_available = [
+                item for sublist in images for item in sublist if os.path.isfile(
+                    item.replace(
+                        self.image_path["other"], "{}/plots/".format(self.webdir)
+                    )
+                )
+            ]
+            if len(_images_available) > 2:
+                html_file.make_banner(approximant=i, key=i)
+                html_file = self.make_modal_carousel(
+                    html_file, images, cli=cli, unique_id=True,
+                    captions=captions, autoscale=True
+                )
+            else:
+                html_file.make_banner(approximant=i, key="custom", custom="")
+
             if self.custom_plotting:
                 custom_plots = glob(
                     "{}/plots/{}_custom_plotting_*".format(self.webdir, i)
                 )
                 path = self.image_path["other"]
                 for num, i in enumerate(custom_plots):
                     custom_plots[num] = path + i.split("/")[-1]
                 image_contents = [
-                    custom_plots[i:4 + i] for i in range(0, len(custom_plots), 4)]
-                html_file.make_table_of_images(contents=image_contents)
-                images = [y for x in image_contents for y in x]
-                html_file.make_modal_carousel(images=images)
+                    custom_plots[i:4 + i] for i in range(
+                        0, len(custom_plots), 4
+                    )
+                ]
+                html_file = self.make_modal_carousel(
+                    html_file, image_contents, unique_id=True
+                )
+
+            html_file.make_banner(
+                approximant="Summary Table", key="summary_table",
+                _style="font-size: 26px;"
+            )
+            _style = "margin-top:3em; margin-bottom:5em; max-width:1400px"
+            _class = "row justify-content-center"
+            html_file.make_container(style=_style)
+            html_file.make_div(4, _class=_class, _style=None)
+
+            key_data = self.key_data
+            contents = []
+            headings = [" "] + self.key_data_headings.copy()
+            _injection = False
+            if "injected" in headings:
+                _injection = not all(
+                    math.isnan(_data["injected"]) for _data in
+                    self.key_data[i].values()
+                )
+            if _injection:
+                headings.append("injected")
+            for j in self.samples[i].keys():
+                row = []
+                row.append(j)
+                row += self.key_data_table[i][j]
+                if _injection:
+                    row.append(safe_round(self.key_data[i][j]["injected"], 3))
+                contents.append(row)
+
+            html_file.make_table(
+                headings=headings, contents=contents, heading_span=1,
+                accordian=False, format="table-hover header-fixed",
+                sticky_header=True
+            )
+            html_file.end_div(4)
+            html_file.end_container()
+            html_file.export(
+                "summary_information_{}.csv".format(i)
+            )
             html_file.make_footer(user=self.user, rundir=self.webdir)
             html_file.close()
 
     def make_1d_histogram_pages(self):
         """Wrapper function for _make_1d_histogram pages
         """
         pages = [
             "{}_{}_{}".format(i, i, j) for i in self.labels for j in
             self.samples[i].keys()
         ]
         pages += ["{}_{}_Custom".format(i, i) for i in self.labels]
         pages += ["{}_{}_All".format(i, i) for i in self.labels]
+        for i in self.labels:
+            if len(self._additional_1d_pages[i]):
+                pages += [
+                    "{}_{}_{}".format(i, i, j) for j in
+                    self._additional_1d_pages[i]
+                ]
+        pages += [
+            "{}_{}_{}_all".format(i, i, j[0]) for i in self.labels for j in
+            self.categorize_parameters(self.samples[i].keys()) if len(j[1])
+        ]
         self.create_blank_html_pages(pages)
         self._make_1d_histogram_pages(pages)
 
     def _make_1d_histogram_pages(self, pages):
         """Make the 1d histogram pages
 
         Parameters
         ----------
         pages: list
             list of pages that you wish to create
         """
         for num, i in enumerate(self.labels):
+            if len(self._additional_1d_pages[i]):
+                for j in self._additional_1d_pages[i]:
+                    _parameters = self.additional_1d_pages[j]
+                    html_file = self.setup_page(
+                        "{}_{}".format(i, j), self.navbar["result_page"][i],
+                        i, title="{} Posterior PDFs describing {}".format(i, j),
+                        approximant=i, background_colour=self.colors[num],
+                        histogram_download=False, toggle=self.expert_plots
+                    )
+                    html_file.make_banner(approximant=i, key=i)
+                    path = self.image_path["other"]
+                    _plots = [
+                        path + "{}_1d_posterior_{}.png".format(i, param) for
+                        param in _parameters
+                    ]
+                    contents = [
+                        _plots[i:2 + i] for i in range(0, len(_plots), 2)
+                    ]
+                    html_file.make_table_of_images(
+                        contents=contents, code="changeimage",
+                        mcmc_samples=self.mcmc_samples, autoscale=True
+                    )
+                    key_data = self.key_data
+                    contents = []
+                    headings = [" "] + self.key_data_headings.copy()
+                    _injection = False
+                    rows = []
+                    for param in _parameters:
+                        _row = [param]
+                        _row += self.key_data_table[i][param]
+                        rows.append(_row)
+                    _style = "margin-top:3em; margin-bottom:5em; max-width:1400px"
+                    _class = "row justify-content-center"
+                    html_file.make_container(style=_style)
+                    html_file.make_div(4, _class=_class, _style=None)
+                    html_file.make_table(
+                        headings=headings, contents=rows, heading_span=1,
+                        accordian=False, format="table-hover"
+                    )
+                    html_file.end_div(4)
+                    html_file.end_container()
+                    html_file.export("summary_information_{}.csv".format(i))
+                    html_file.make_footer(user=self.user, rundir=self.webdir)
+                    html_file.close()
             for j in self.samples[i].keys():
                 html_file = self.setup_page(
                     "{}_{}".format(i, j), self.navbar["result_page"][i],
                     i, title="{} Posterior PDF for {}".format(i, j),
                     approximant=i, background_colour=self.colors[num],
-                    histogram_download=False
+                    histogram_download=False, toggle=self.expert_plots
                 )
-                html_file.make_banner(approximant=i, key=i)
+                if j.description != "Unknown parameter description":
+                    _custom = (
+                        "The figures below show the plots for {}: {}"
+                    )
+                    html_file.make_banner(
+                        approximant=i, key="custom",
+                        custom=_custom.format(j, j.description)
+                    )
+                else:
+                    html_file.make_banner(approximant=i, key=i)
                 path = self.image_path["other"]
                 contents = [
                     [path + "{}_1d_posterior_{}.png".format(i, j)],
                     [
                         path + "{}_sample_evolution_{}.png".format(i, j),
                         path + "{}_autocorrelation_{}.png".format(i, j)
                     ]
@@ -553,20 +825,76 @@
                         PlotCaption("autocorrelation").format(j)
                     ]
                 ]
                 html_file.make_table_of_images(
                     contents=contents, rows=1, columns=2, code="changeimage",
                     captions=captions, mcmc_samples=self.mcmc_samples
                 )
+                contents = [
+                    [path + "{}_2d_contour_{}_log_likelihood.png".format(i, j)],
+                    [
+                        path + "{}_sample_evolution_{}_{}_colored.png".format(
+                            i, j, "log_likelihood"
+                        ), path + "{}_1d_posterior_{}_bootstrap.png".format(i, j)
+                    ]
+                ]
+                captions = [
+                    [PlotCaption("2d_contour").format(j, "log_likelihood")],
+                    [
+                        PlotCaption("sample_evolution_colored").format(
+                            j, "log_likelihood"
+                        ),
+                        PlotCaption("1d_histogram_bootstrap").format(100, j, 1000)
+                    ],
+                ]
+                if self.expert_plots:
+                    html_file.make_table_of_images(
+                        contents=contents, rows=1, columns=2, code="changeimage",
+                        captions=captions, mcmc_samples=self.mcmc_samples,
+                        display='none', container_id='expert_div',
+                        close_container=False
+                    )
+                    _additional = self.add_to_expert_pages(path, i)
+                    if _additional is not None and j in _additional.keys():
+                        html_file.make_table_of_images(
+                            contents=_additional[j], code="changeimage",
+                            mcmc_samples=self.mcmc_samples,
+                            autoscale=True, display='none',
+                            add_to_open_container=True,
+                        )
+                    html_file.end_div()
                 html_file.export(
                     "", csv=False, json=False, shell=False, margin_bottom="1em",
                     histogram_dat=os.path.join(
                         self.results_path["other"], i, "{}_{}.dat".format(i, j)
                     )
                 )
+                key_data = self.key_data
+                contents = []
+                headings = self.key_data_headings.copy()
+                _injection = False
+                if "injected" in headings:
+                    _injection = not math.isnan(self.key_data[i][j]["injected"])
+                row = self.key_data_table[i][j]
+                if _injection:
+                    headings.append("injected")
+                    row.append(safe_round(self.key_data[i][j]["injected"], 3))
+                _style = "margin-top:3em; margin-bottom:5em; max-width:1400px"
+                _class = "row justify-content-center"
+                html_file.make_container(style=_style)
+                html_file.make_div(4, _class=_class, _style=None)
+                html_file.make_table(
+                    headings=headings, contents=[row], heading_span=1,
+                    accordian=False, format="table-hover"
+                )
+                html_file.end_div(4)
+                html_file.end_container()
+                html_file.export(
+                    "summary_information_{}.csv".format(i)
+                )
                 html_file.make_footer(user=self.user, rundir=self.webdir)
                 html_file.close()
             html_file = self.setup_page(
                 "{}_Custom".format(i), self.navbar["result_page"][i],
                 i, title="{} Posteriors for multiple".format(i),
                 approximant=i, background_colour=self.colors[num]
             )
@@ -601,14 +929,136 @@
                         path + "{}_sample_evolution_{}.png".format(i, j),
                         path + "{}_autocorrelation_{}.png".format(i, j)
                     ]
                 ]
                 html_file.make_table_of_images(
                     contents=contents, rows=1, columns=2, code="changeimage")
             html_file.close()
+            for j in self.categorize_parameters(self.samples[i].keys()):
+                if not len(j[1]):
+                    continue
+                html_file = self.setup_page(
+                    "{}_{}_all".format(i, j[0]), self.navbar["result_page"][i],
+                    i, title="All posteriors for describing {}".format(j[0]),
+                    approximant=i, background_colour=self.colors[num]
+                )
+                for k in j[1][1:]:
+                    html_file.make_banner(
+                        approximant=k, _style="font-size: 26px;"
+                    )
+                    contents = [
+                        [path + "{}_1d_posterior_{}.png".format(i, k)],
+                        [
+                            path + "{}_sample_evolution_{}.png".format(i, k),
+                            path + "{}_autocorrelation_{}.png".format(i, k)
+                        ]
+                    ]
+                    html_file.make_table_of_images(
+                        contents=contents, rows=1, columns=2, code="changeimage"
+                    )
+                html_file.make_banner(
+                    approximant="Summary Table", key="summary_table",
+                    _style="font-size: 26px;"
+                )
+                _style = "margin-top:3em; margin-bottom:5em; max-width:1400px"
+                _class = "row justify-content-center"
+                html_file.make_container(style=_style)
+                html_file.make_div(4, _class=_class, _style=None)
+                headings = [" "] + self.key_data_headings.copy()
+                contents = []
+                for k in j[1][1:]:
+                    row = [k]
+                    row += self.key_data_table[i][k]
+                    contents.append(row)
+                html_file.make_table(
+                    headings=headings, contents=contents, heading_span=1,
+                    accordian=False, format="table-hover",
+                    sticky_header=True
+                )
+                html_file.end_div(4)
+                html_file.end_container()
+                html_file.export("{}_summary_{}.csv".format(j[0], i))
+                html_file.make_footer(user=self.user, rundir=self.webdir)
+                html_file.close()
+
+    def make_additional_plots_pages(self):
+        """Wrapper function for _make_additional_plots_pages
+        """
+        pages = [
+            "{}_{}_Additional".format(i, i) for i in self.labels if i in
+            self.existing_plot.keys()
+        ]
+        self.create_blank_html_pages(pages)
+        self._make_additional_plots_pages(pages)
+
+    def _make_additional_plots_pages(self, pages):
+        """Make the additional plots pages
+
+        Parameters
+        ----------
+        pages: list
+            list of pages that you wish to create
+        """
+        from PIL import Image
+        for num, i in enumerate(self.labels):
+            if i not in self.existing_plot.keys():
+                continue
+            html_file = self.setup_page(
+                "{}_Additional".format(i), self.navbar["result_page"][i], i,
+                title="Additional plots for {}".format(i), approximant=i,
+                background_colour=self.colors[num]
+            )
+            html_file.make_banner(approximant=i, key="additional")
+            if isinstance(self.existing_plot[i], list):
+                images = sorted(
+                    self.existing_plot[i],
+                    key=lambda _path: Image.open(_path).size[1]
+                )
+                tol = 2.
+                # if all images are of similar height, then grid them uniformly, else
+                # grid by heights
+                cond = all(
+                    Image.open(_path).size[1] / tol < Image.open(images[0]).size[1]
+                    for _path in images
+                )
+                if cond:
+                    image_contents = [
+                        [
+                            self.image_path["other"] + Path(_path).name for
+                            _path in images[j:4 + j]
+                        ] for j in range(0, len(self.existing_plot[i]), 4)
+                    ]
+                else:
+                    heights = [Image.open(_path).size[1] for _path in images]
+                    image_contents = [
+                        [self.image_path["other"] + Path(images[0]).name]
+                    ]
+                    row = 0
+                    for num, _height in enumerate(heights[1:]):
+                        if _height / tol < heights[num]:
+                            image_contents[row].append(
+                                self.image_path["other"] + Path(images[num + 1]).name
+                            )
+                        else:
+                            row += 1
+                            image_contents.append(
+                                [self.image_path["other"] + Path(images[num + 1]).name]
+                            )
+                margin_left = None
+            else:
+                image_contents = [
+                    [self.image_path["other"] + Path(self.existing_plot[i]).name]
+                ]
+                margin_left = "-250px"
+            html_file = self.make_modal_carousel(
+                html_file, image_contents, unique_id=True, autoscale=True,
+                margin_left=margin_left
+            )
+            html_file.make_footer(user=self.user, rundir=self.webdir)
+            html_file.close()
 
     def make_corner_pages(self):
         """Wrapper function for _make_corner_pages
         """
         pages = ["{}_{}_Corner".format(i, i) for i in self.labels]
         self.create_blank_html_pages(pages)
         self._make_corner_pages(pages)
@@ -678,31 +1128,52 @@
                 html_file.end_container()
                 with open(
                     "{0:s}/css/{1:s}_{2:s}_Config.css".format(
                         self.webdir, i, i
                     ), "w"
                 ) as f:
                     f.write(styles)
+                _fix = False
             else:
                 html_file.add_content(
                     "<div class='row justify-content-center'; "
                     "style='font-family: Arial-body; font-size: 14px'>"
                     "<p style='margin-top:2.5em'> No configuration file was "
                     "provided </p></div>"
                 )
-            html_file.make_footer(user=self.user, rundir=self.webdir)
+                _fix = True
+            if i in self.analytic_priors.keys():
+                if self.analytic_priors[i] is not None:
+                    html_file.make_div(indent=2, _class='paragraph')
+                    html_file.add_content(
+                        "Below is the prior file for %s" % (i)
+                    )
+                    html_file.end_div()
+                    html_file.make_container()
+                    styles = html_file.make_code_block(
+                        language='ini', contents=self.analytic_priors[i]
+                    )
+                    html_file.end_container()
+                    _fix = False
+            html_file.make_footer(
+                user=self.user, rundir=self.webdir, fix_bottom=_fix
+            )
             html_file.close()
 
     def make_comparison_pages(self):
         """Wrapper function for _make_comparison_pages
         """
         pages = ["Comparison_{}".format(i) for i in self.same_parameters]
         pages += ["Comparison_Custom"]
         pages += ["Comparison_All"]
         pages += ["Comparison"]
+        pages += [
+            "Comparison_{}_all".format(j[0]) for j in
+            self.categorize_parameters(self.same_parameters) if len(j[1])
+        ]
         self.create_blank_html_pages(pages)
         self._make_comparison_pages(pages)
 
     def _make_comparison_pages(self, pages):
         """Make pages to compare all result files
 
         Parameters
@@ -714,38 +1185,35 @@
             "Comparison", self.navbar["comparison"], approximant="Comparison",
             title="Comparison Summary Page"
         )
         html_file.make_banner(approximant="Comparison", key="Comparison")
         path = self.image_path["other"]
         if len(self.default_comparison_homepage_plots()):
             contents = self.default_comparison_homepage_plots()
-            unique_id = '{}'.format(uuid.uuid4().hex.upper()[:6])
-            html_file.make_table_of_images(contents=contents, unique_id=unique_id)
-            images = [y for x in contents for y in x]
-            html_file.make_modal_carousel(images=images, unique_id=unique_id)
+            html_file = self.make_modal_carousel(
+                html_file, contents, unique_id=True
+            )
         if self.custom_plotting:
             from glob import glob
 
             custom_plots = glob(
                 os.path.join(
                     self.webdir, "plots", "combined_custom_plotting_*"
                 )
             )
             for num, i in enumerate(custom_plots):
                 custom_plots[num] = path + i.split("/")[-1]
             image_contents = [
                 custom_plots[i:4 + i] for i in range(0, len(custom_plots), 4)
             ]
-            unique_id = '{}'.format(uuid.uuid4().hex.upper()[:6])
-            html_file.make_table_of_images(
-                contents=image_contents, unique_id=unique_id
+            html_file = self.make_modal_carousel(
+                html_file, image_contents, unique_id=True
             )
-            images = [y for x in image_contents for y in x]
-            html_file.make_modal_carousel(images=images, unique_id=unique_id)
         path = self.image_path["other"]
+
         if self.comparison_stats is not None:
             for _num, _key in enumerate(["KS_test", "JS_test"]):
                 if _key == "KS_test":
                     html_file.make_banner(
                         approximant="KS test", key="ks_test",
                         _style="font-size: 26px;"
                     )
@@ -791,14 +1259,37 @@
                     path + "combined_cdf_{}.png".format(i),
                     path + "combined_boxplot_{}.png".format(i)
                 ]
             ]
             html_file.make_table_of_images(
                 contents=contents, rows=1, columns=2, code="changeimage"
             )
+            html_file.make_banner(
+                approximant="Summary Table", key="summary_table",
+                _style="font-size: 26px;"
+            )
+            _style = "margin-top:3em; margin-bottom:5em; max-width:1400px"
+            _class = "row justify-content-center"
+            html_file.make_container(style=_style)
+            html_file.make_div(4, _class=_class, _style=None)
+            headings = [" "] + self.key_data_headings.copy()
+            contents = []
+            for label in self.labels:
+                row = [label]
+                row += self.key_data_table[label][i]
+                contents.append(row)
+            html_file.make_table(
+                headings=headings, contents=contents, heading_span=1,
+                accordian=False, format="table-hover",
+                sticky_header=True
+            )
+            html_file.end_div(4)
+            html_file.end_container()
+            html_file.export("comparison_summary_{}.csv".format(i))
+
             if self.comparison_stats is not None:
                 for _num, _key in enumerate(["KS_test", "JS_test"]):
                     if _key == "KS_test":
                         html_file.make_banner(
                             approximant="KS test", key="ks_test",
                             _style="font-size: 26px;"
                         )
@@ -854,14 +1345,38 @@
                     path + "combined_cdf_{}.png".format(j),
                     path + "combined_boxplot_{}.png".format(j)
                 ]
             ]
             html_file.make_table_of_images(
                 contents=contents, rows=1, columns=2, code="changeimage")
         html_file.close()
+        for j in self.categorize_parameters(self.same_parameters):
+            if not len(j[1]):
+                continue
+            html_file = self.setup_page(
+                "Comparison_{}_all".format(j[0]), self.navbar["comparison"],
+                title="All posteriors for describing {}".format(j[0]),
+                approximant="Comparison"
+            )
+            for k in j[1][1:]:
+                html_file.make_banner(
+                    approximant=k, _style="font-size: 26px;"
+                )
+                contents = [
+                    [path + "combined_1d_posterior_{}.png".format(k)],
+                    [
+                        path + "combined_cdf_{}.png".format(k),
+                        path + "combined_boxplot_{}.png".format(k)
+                    ]
+                ]
+                html_file.make_table_of_images(
+                    contents=contents, rows=1, columns=2, code="changeimage"
+                )
+            html_file.make_footer(user=self.user, rundir=self.webdir)
+            html_file.close()
 
     def make_interactive_pages(self):
         """Wrapper function for _make_interactive_pages
         """
         pages = ["{}_{}_Interactive_Corner".format(i, i) for i in self.labels]
         if self.make_comparison:
             pages += ["Comparison_Interactive_Ridgeline"]
@@ -973,57 +1488,54 @@
         """Make a page to display the version information
 
         Parameters
         ----------
         pages: list
             list of pages that you wish to create
         """
-        from pesummary._version_helper import (
-            PackageInformation, install_path
-        )
+        from pesummary._version_helper import install_path
 
         html_file = webpage.open_html(
             web_dir=self.webdir, base_url=self.base_url, html_page="Version"
         )
         html_file = self.setup_page(
             "Version", self.navbar["home"], title="Version Information"
         )
         html_file.make_banner(approximant="Version", key="Version")
-        path = pesummary.__file__[:-12]
-        with open(path + "/.version", 'r') as f:
-            contents = f.read()
+        contents = __version_string__
         contents += install_path(return_string=True)
         for i in self.labels:
             contents = (
                 "# {} version information\n\n{}_version={}\n\n".format(
                     i, i, self.file_versions[i]
                 )
             ) + contents
         html_file.make_container()
         styles = html_file.make_code_block(language='shell', contents=contents)
         with open('{0:s}/css/Version.css'.format(self.webdir), 'w') as f:
             f.write(styles)
         html_file.end_container()
         packages = self.package_information["packages"]
         style = "margin-top:{}; margin-bottom:{};"
-        html_file.make_table(
-            headings=[x.title().replace('_', ' ') for x in packages.dtype.names],
-            contents=[[pp.decode("utf-8") for pp in pkg] for pkg in packages],
-            accordian=False, style=style.format("1em", "1em")
-        )
-        if self.package_information["manager"] == "conda":
-            html_file.export(
-                "environment.yml", margin_top="1em", csv=False,
-                conda=True
-            )
-        else:
-            html_file.export(
-                "requirements.txt", margin_top="1em", csv=False,
-                requirements=True
+        if len(packages):
+            html_file.make_table(
+                headings=[x.title().replace('_', ' ') for x in packages.dtype.names],
+                contents=[[pp.decode("utf-8") for pp in pkg] for pkg in packages],
+                accordian=False, style=style.format("1em", "1em")
             )
+            if self.package_information["manager"] == "conda":
+                html_file.export(
+                    "environment.yml", margin_top="1em", csv=False,
+                    conda=True
+                )
+            else:
+                html_file.export(
+                    "requirements.txt", margin_top="1em", csv=False,
+                    requirements=True
+                )
         html_file.make_footer(user=self.user, rundir=self.webdir)
         html_file.close()
 
     def make_logging_page(self):
         """Wrapper function for _make_logging_page
         """
         pages = ["Logging"]
@@ -1100,25 +1612,27 @@
         with open('{0:s}/css/About.css'.format(self.webdir), 'w') as g:
             g.write(styles)
         html_file.end_container()
         html_file.export(
             "pesummary.sh", csv=False, json=False, shell=True,
             margin_top="-4em"
         )
-        html_file.make_footer(user=self.user, rundir=self.webdir)
+        html_file.make_footer(
+            user=self.user, rundir=self.webdir, fix_bottom=True
+        )
         html_file.close()
 
     def make_downloads_page(self):
         """Wrapper function for _make_downloads_page
         """
         pages = ["Downloads"]
         self.create_blank_html_pages(pages)
         self._make_downloads_page(pages)
 
-    def _make_downloads_page(self, pages):
+    def _make_downloads_page(self, pages, fix_bottom=False):
         """Make a page with links to files which can be downloaded
 
         Parameters
         ----------
         pages: list
             list of pages you wish to create
         """
@@ -1129,24 +1643,20 @@
             "Downloads", self.navbar["home"], title="Downloads"
         )
         html_file.make_banner(approximant="Downloads", key="Downloads")
         html_file.make_container()
         base_string = "{} can be downloaded <a href={} download>here</a>"
         style = "margin-top:{}; margin-bottom:{};"
         headings = ["Description"]
-        metafile = (
-            "posterior_samples.json" if not self.hdf5 else "posterior_samples.h5"
-        )
-
         metafile_row = [
             [
                 base_string.format(
                     "The complete metafile containing all information "
                     "about the analysis",
-                    self.results_path["other"] + metafile
+                    self.results_path["other"] + self._metafile
                 )
             ]
         ]
         if self.external_hdf5_links:
             string = ""
             for label in self.labels:
                 string += (
@@ -1166,57 +1676,85 @@
             ]
         metafile_row += [
             [
                 (
                     "Information about reading this metafile can be seen "
                     " <a href={}>here</a>".format(
                         "https://lscsoft.docs.ligo.org/pesummary/"
-                        "stable_docs/data/reading_the_metafile.html"
+                        "stable/data/reading_the_metafile.html"
                     )
                 )
             ]
         ]
         html_file.make_table(
             headings=headings, contents=metafile_row,
             accordian=False, style=style.format("1em", "1em")
         )
         for num, i in enumerate(self.labels):
             table_contents = self._make_entry_in_downloads_table(
                 html_file, i, num, base_string
             )
-            html_file.make_table(
-                headings=headings, contents=table_contents, accordian=False
-            )
+            if table_contents is not None:
+                html_file.make_table(
+                    headings=headings, contents=table_contents, accordian=False
+                )
         html_file.end_container()
-        html_file.make_footer(user=self.user, rundir=self.webdir)
+        html_file.make_footer(
+            user=self.user, rundir=self.webdir, fix_bottom=fix_bottom
+        )
         html_file.close()
 
     def _make_entry_in_downloads_table(self, html_file, label, num, base_string):
         """Make a label specific entry into the downloads table
 
         Parameters
         ----------
         label: str
             the label you wish to add to the downloads table
         base_string: str
             the download string
         """
+        import glob
+        identifier = label if not self.mcmc_samples else "chain"
+        _original = glob.glob(
+            os.path.join(self.webdir, "samples", "{}_*".format(identifier))
+        )
         html_file.add_content(
             "<div class='banner', style='margin-left:-4em'>{}</div>".format(
                 label
             )
         )
-        table_contents = [
+        if not self.mcmc_samples and len(_original) > 0:
+            table_contents = [
+                [
+                    base_string.format(
+                        "Original file provided to PESummary",
+                        self.results_path["other"] + Path(_original[0]).name
+                    )
+                ]
+            ]
+        elif self.mcmc_samples and len(_original) > 0:
+            table_contents = [
+                [
+                    base_string.format(
+                        "Original chain {} provided to PESummary".format(num),
+                        self.results_path["other"] + Path(_original[num]).name
+                    )
+                ] for num in range(len(_original))
+            ]
+        else:
+            table_contents = []
+        table_contents.append(
             [
                 base_string.format(
                     "Dat file containing posterior samples",
                     self.results_path["other"] + "%s_pesummary.dat" % (label)
                 )
             ]
-        ]
+        )
         if self.config is not None and self.config[num] is not None:
             table_contents.append(
                 [
                     base_string.format(
                         "Config file used for this analysis",
                         self.config_path["other"] + "%s_config.ini" % (label)
                     )
@@ -1258,20 +1796,20 @@
 
     def generate_specific_javascript(self):
         """Tailor the javascript to the specific situation.
         """
         path = self.webdir + "/js/grab.js"
         existing = open(path)
         existing = existing.readlines()
-        ind = existing.index("    if ( param == approximant ) {\n")
+        ind = existing.index("        if ( param == approximant ) {\n")
         content = existing[:ind]
         for i in [list(j.keys())[0] for j in self._result_page_links()]:
-            content.append("    if ( param == \"%s\" ) {\n" % (i))
-            content.append("        approx = \"None\" \n")
-            content.append("    }\n")
+            content.append("        if ( param == \"%s\" ) {\n" % (i))
+            content.append("            approx = \"None\" \n")
+            content.append("        }\n")
         for i in existing[ind + 1:]:
             content.append(i)
         new_file = open(path, "w")
         new_file.writelines(content)
         new_file.close()
 
     def default_categories(self):
@@ -1298,14 +1836,19 @@
             },
             "Y-Z": {
                 "accept": ["y", "Y", "z", "Z"], "reject": []
             }
         }
         return categories
 
+    def default_images_for_result_page(self, label):
+        """Return the default images that will be displayed on the result page
+        """
+        return [], [], []
+
     def default_popular_options(self):
         """Return a list of default options
         """
         return []
 
     def default_comparison_homepage_plots(self):
         """Return a list of default plots for the comparison homepage
@@ -1320,7 +1863,45 @@
 
     def add_existing_data(self):
         """
         """
         from pesummary.utils.utils import _add_existing_data
 
         self = _add_existing_data(self)
+
+    def add_to_expert_pages(self, path, label):
+        """Additional expert plots to add beyond the default. This returns a
+        dictionary keyed by the parameter, with values providing the path
+        to the additional plots you wish to add. The plots are a 2d list
+        where each sublist represents a row in the table of images.
+
+        Parameters
+        ----------
+        path: str
+            path to the image directory
+        label: str
+            label of the plot you wish to add
+        """
+        mydict = {}
+        contour_base = path + "{}_2d_contour_{}_{}.png"
+        histogram_base = path + "{}_1d_posterior_{}_{}.png"
+        if not hasattr(self.samples[label], "debug_keys"):
+            return mydict
+        for param in self.samples[label].debug_keys():
+            if "_non_reweighted" in param:
+                base_param = param.split("_non_reweighted")[0][1:]
+                if base_param in self.samples[label].keys():
+                    mydict[base_param] = [[
+                        histogram_base.format(label, base_param, param),
+                        contour_base.format(label, base_param, param)
+                    ]]
+        return mydict
+
+    @property
+    def additional_1d_pages(self):
+        """Additional 1d histogram pages beyond one for each parameter. You may,
+        for instance, want a 1d histogram page which combines multiple
+        parameters. This returns a dictionary, keyed by the new 1d histogram
+        page, with values indicating the parameters you wish to include on this
+        page. Only the 1d marginalized histograms are shown.
+        """
+        return None
```

### Comparing `pesummary-0.9.1/pesummary/io/__init__.py` & `pesummary-1.0.0/pesummary/tests/executables.sh`

 * *Files 22% similar despite different names*

```diff
@@ -1,17 +1,27 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
+# Copyright (C) 2019 Charlie Hoy <charlie.hoy@ligo.org> This program is free
+# software; you can redistribute it and/or modify it under the terms of the GNU
+# General Public License as published by the Free Software Foundation; either
+# version 3 of the License, or (at your option) any later version.
 #
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
+# This program is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
+# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
+# details.
 #
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# You should have received a copy of the GNU General Public License along with
+# this program; if not, write to the Free Software Foundation, Inc., 51
+# Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
 
-from .read import read
-from .write import write
+set -e
+
+SCRIPT_DIR=`dirname "${BASH_SOURCE[0]}"`
+
+_executables=($(ls ${SCRIPT_DIR}/../cli/summary*))
+executables=()
+for i in ${_executables[@]}; do
+    executables+=(`python -c "print('${i}'.split('/')[-1].split('.py')[0])"`);
+done
+echo ${executables[@]}
+for i in ${executables[@]}; do
+    eval '${i} --help';
+done
```

### Comparing `pesummary-0.9.1/pesummary/tests/meta_file_test.py` & `pesummary-1.0.0/pesummary/tests/meta_file_test.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,40 +1,35 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import json
 import os
 import shutil
 
 import h5py
 import numpy as np
 
+from pesummary.core.file import meta_file as core_meta_file
 from pesummary.gw.file import meta_file
 from pesummary.gw.file.meta_file import _GWMetaFile
-from pesummary.gw.inputs import GWInput
-from pesummary.utils.samples_dict import SamplesDict, Array
+from pesummary.gw.cli.inputs import MetaFileInput
+from pesummary.utils.samples_dict import SamplesDict
+from pesummary.utils.array import Array
 from .base import data_dir
+import tempfile
+
+tmpdir_main = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 def test_recursively_save_dictionary_to_hdf5_file():
-    if os.path.isdir("./.outdir"):
-        shutil.rmtree("./.outdir")
-    os.makedirs("./.outdir")
+    tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+    if os.path.isdir(tmpdir):
+        shutil.rmtree(tmpdir)
+    os.makedirs(tmpdir)
 
     data = {
                "H1_L1_IMRPhenomPv2": {
                    "posterior_samples": {
                        "parameters": ["mass_1", "mass_2"],
                        "samples": [[10, 2], [50, 5], [100, 90]]
                        },
@@ -49,19 +44,19 @@
                    "posterior_samples": {
                        "parameters": ["psi", "phi"],
                        "samples": [[1.2, 0.2], [3.14, 0.1], [0.5, 0.3]]
                    }
                },
           }
 
-    with h5py.File("./.outdir/test.h5") as f:
-        meta_file.recursively_save_dictionary_to_hdf5_file(
+    with h5py.File("{}/test.h5".format(tmpdir), "w") as f:
+        core_meta_file.recursively_save_dictionary_to_hdf5_file(
             f, data, extra_keys=list(data.keys()))
 
-    f = h5py.File("./.outdir/test.h5", "r")
+    f = h5py.File("{}/test.h5".format(tmpdir), "r")
     assert sorted(list(f.keys())) == sorted(list(data.keys()))
     assert sorted(
         list(f["H1_L1_IMRPhenomPv2/posterior_samples"].keys())) == sorted(
             ["parameters", "samples"]
     )
     assert f["H1_L1_IMRPhenomPv2/posterior_samples/parameters"][0].decode("utf-8") == "mass_1"
     assert f["H1_L1_IMRPhenomPv2/posterior_samples/parameters"][1].decode("utf-8") == "mass_2"
@@ -116,17 +111,18 @@
         )
     )
 
 
 def test_softlinks():
     """
     """
-    if os.path.isdir("./.outdir"):
-        shutil.rmtree("./.outdir")
-    os.makedirs("./.outdir")
+    tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+    if os.path.isdir(tmpdir):
+        shutil.rmtree(tmpdir)
+    os.makedirs(tmpdir)
 
     data = {
         "label1": {
             "psds": {
                 "H1": [[10, 20], [30, 40]],
                 "L1": [[10, 20], [30, 40]]
             },
@@ -197,27 +193,27 @@
         assert \
             repeat[keys[0]][1] == "softlink:/{}".format(keys[1]) and \
             repeat[keys[1]][1] == repeat[keys[1]][0] or \
             repeat[keys[1]][1] == "softlink:/{}".format(keys[0]) and \
             repeat[keys[0]][1] == repeat[keys[0]][0]
 
     print(simlinked_dict)
-    with h5py.File("./.outdir/test.h5") as f:
-        meta_file.recursively_save_dictionary_to_hdf5_file(
+    with h5py.File("{}/test.h5".format(tmpdir), "w") as f:
+        core_meta_file.recursively_save_dictionary_to_hdf5_file(
             f, simlinked_dict, extra_keys=meta_file.DEFAULT_HDF5_KEYS + ["label1", "label2"])
 
-    with h5py.File("./.outdir/no_softlink.h5") as f:
-        meta_file.recursively_save_dictionary_to_hdf5_file(
+    with h5py.File("{}/no_softlink.h5".format(tmpdir), "w") as f:
+        core_meta_file.recursively_save_dictionary_to_hdf5_file(
             f, data, extra_keys=meta_file.DEFAULT_HDF5_KEYS + ["label1", "label2"])
 
-    softlink_size = os.stat("./.outdir/test.h5").st_size
-    no_softlink_size = os.stat('./.outdir/no_softlink.h5').st_size
+    softlink_size = os.stat("{}/test.h5".format(tmpdir)).st_size
+    no_softlink_size = os.stat('{}/no_softlink.h5'.format(tmpdir)).st_size
     assert softlink_size < no_softlink_size
 
-    with h5py.File("./.outdir/test.h5", "r") as f:
+    with h5py.File("{}/test.h5".format(tmpdir), "r") as f:
         assert \
             f["label2"]["config_file"]["condor"]["executable"][0] == \
             f["label1"]["config_file"]["condor"]["executable"][0]
         assert \
             all(
                 i == j for i, j in zip(
                     f["label1"]["psds"]["H1"][0], f["label1"]["psds"]["L1"][0]
@@ -233,16 +229,16 @@
 
 class TestMetaFile(object):
     """Class the test the pesummary.gw.file.meta_file._GWMetaFile class
     """
     def setup(self):
         """Setup the Test class
         """
-        if not os.path.isdir(".outdir/samples"):
-            os.makedirs(".outdir/samples")
+        if not os.path.isdir("{}/samples".format(tmpdir_main)):
+            os.makedirs("{}/samples".format(tmpdir_main))
 
         self.samples = np.array([np.random.random(10) for i in range(15)])
         self.input_parameters = [
             "mass_1", "mass_2", "a_1", "a_2", "tilt_1", "tilt_2", "phi_jl",
             "phi_12", "psi", "theta_jn", "ra", "dec", "luminosity_distance",
             "geocent_time", "log_likelihood"]
         self.input_data = {"EXP1": SamplesDict(self.input_parameters, self.samples)}
@@ -253,46 +249,48 @@
         self.input_injection_data = np.random.random(15)
         self.input_injection = {"EXP1": {
             i: j for i, j in zip(self.input_parameters, self.input_injection_data)}}
         self.input_file_kwargs = {"EXP1": {
             "sampler": {"flow": 10}, "meta_data": {"samplerate": 10}
         }}
         self.input_config = [data_dir + "/config_lalinference.ini"]
-        psd_data = GWInput.extract_psd_data_from_file(data_dir + "/psd_file.txt")
+        psd_data = MetaFileInput.extract_psd_data_from_file(data_dir + "/psd_file.txt")
         self.psds = {"EXP1": {"H1": psd_data}}
-        calibration_data = GWInput.extract_calibration_data_from_file(
+        calibration_data = MetaFileInput.extract_calibration_data_from_file(
             data_dir + "/calibration_envelope.txt")
         self.calibration = {"EXP1": {"H1": calibration_data}}
 
         object = _GWMetaFile(
             self.input_data, self.input_labels, self.input_config,
             self.input_injection, self.input_file_version, self.input_file_kwargs,
-            webdir=".outdir", psd=self.psds, calibration=self.calibration)
+            webdir=tmpdir_main, psd=self.psds, calibration=self.calibration)
         object.make_dictionary()
         object.save_to_json(object.data, object.meta_file)
         object = _GWMetaFile(
             self.input_data, self.input_labels, self.input_config,
             self.input_injection, self.input_file_version, self.input_file_kwargs,
-            webdir=".outdir", psd=self.psds, calibration=self.calibration,
+            webdir=tmpdir_main, psd=self.psds, calibration=self.calibration,
             hdf5=True)
         object.make_dictionary()
         object.save_to_hdf5(
             object.data, object.labels, object.samples, object.meta_file
         )
 
-        with open(".outdir/samples/posterior_samples.json", "r") as f:
+        with open("{}/samples/posterior_samples.json".format(tmpdir_main), "r") as f:
             self.json_file = json.load(f)
-        self.hdf5_file = h5py.File(".outdir/samples/posterior_samples.h5", "r")
+        self.hdf5_file = h5py.File(
+            "{}/samples/posterior_samples.h5".format(tmpdir_main), "r"
+        )
 
     def teardown(self):
         """Remove all files and directories created from this class
         """
         self.hdf5_file.close()
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir_main):
+            shutil.rmtree(tmpdir_main)
 
     def test_parameters(self):
         """Test the parameters stored in the metafile
         """
         for num, data in enumerate([self.json_file, self.hdf5_file]):
             assert sorted(list(data.keys())) == sorted(
                 self.input_labels + ["version", "history"]
```

### Comparing `pesummary-0.9.1/pesummary/tests/summaryplots_test.py` & `pesummary-1.0.0/pesummary/tests/summaryplots_test.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,36 +1,24 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
 import shutil
 from glob import glob
 
-from pesummary.core.command_line import command_line
-from pesummary.gw.command_line import insert_gwspecific_option_group
-from pesummary.gw.inputs import GWInput
+from pesummary.gw.cli.parser import ArgumentParser
+from pesummary.gw.cli.inputs import PlottingInput, WebpagePlusPlottingPlusMetaFileInput
 from pesummary.cli.summaryplots import _GWPlotGeneration as GWPlotGeneration
 from pesummary.gw.file.meta_file import GWMetaFile
 from pesummary.cli.summarypages import _GWWebpageGeneration as GWWebpageGeneration
 from .base import make_result_file, get_list_of_plots, data_dir
 
 import pytest
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class TestPlotGeneration(object):
 
     def setup(self):
         directories = ["./.outdir_bilby", "./.outdir_lalinference",
                        "./.outdir_comparison", "./.outdir_add_to_existing2",
                        ".outdir_comparison_no_comparison",
@@ -45,235 +33,237 @@
             f.writelines(["1.00 3.44\n"])
             f.writelines(["100.00 4.00\n"])
             f.writelines(["1000.00 5.00\n"])
             f.writelines(["2000.00 6.00\n"])
         with open("./.outdir_bilby/calibration.dat", "w") as f:
             f.writelines(["1.0 2.0 3.0 4.0 5.0 6.0 7.0\n"])
             f.writelines(["2000.0 2.0 3.0 4.0 5.0 6.0 7.0"])
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
         make_result_file(
-            gw=True, extension="hdf5", bilby=True, outdir="./.outdir_bilby/"
+            gw=True, extension="hdf5", bilby=True, outdir="./.outdir_bilby/",
+            n_samples=10
         )
         os.rename("./.outdir_bilby/test.h5", "./.outdir_bilby/bilby_example.h5")
         default_arguments = [
             "--approximant", "IMRPhenomPv2",
             "--webdir", "./.outdir_bilby",
             "--samples", "./.outdir_bilby/bilby_example.h5",
             "--config", data_dir + "/config_bilby.ini",
             "--psd", "./.outdir_bilby/psd.dat",
             "--calibration", "./.outdir_bilby/calibration.dat",
-            "--labels", "H10", "--no_ligo_skymap"]
+            "--labels", "H10", "--no_ligo_skymap", "--disable_expert"]
         opts = parser.parse_args(default_arguments)
-        inputs = GWInput(opts)
+        inputs = PlottingInput(opts)
         webpage = GWPlotGeneration(inputs)
         webpage.generate_plots()
         plots = sorted(glob("./.outdir_bilby/plots/*.png"))
         expected_plots = get_list_of_plots(
-            gw=True, label="H1", outdir=".outdir_bilby", psd=True,
+            gw=True, label="H1", outdir="./.outdir_bilby", psd=True,
             calibration=False
         )
         for i, j in zip(expected_plots, plots):
             print(i, j)
         assert all(i == j for i,j in zip(sorted(expected_plots), sorted(plots)))
 
     def test_plot_generation_for_lalinference_structure(self):
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
         make_result_file(
             gw=True, extension="hdf5", lalinference=True,
-            outdir="./.outdir_lalinference/"
+            outdir="./.outdir_lalinference/", n_samples=10
         )
         os.rename(
             "./.outdir_lalinference/test.hdf5",
             "./.outdir_lalinference/lalinference_example.h5"
         )
         default_arguments = [
             "--approximant", "IMRPhenomPv2",
             "--webdir", "./.outdir_lalinference",
             "--samples", "./.outdir_lalinference/lalinference_example.h5",
             "--config", data_dir + "/config_lalinference.ini",
-            "--labels", "H10", "--no_ligo_skymap"]
+            "--labels", "H10", "--no_ligo_skymap", "--disable_expert"]
         opts = parser.parse_args(default_arguments)
-        inputs = GWInput(opts)
+        inputs = PlottingInput(opts)
         webpage = GWPlotGeneration(inputs)
         webpage.generate_plots()
         plots = sorted(glob("./.outdir_lalinference/plots/*.png"))
         expected_plots = get_list_of_plots(
-            gw=True, label="H1", outdir=".outdir_lalinference"
+            gw=True, label="H1", outdir="./.outdir_lalinference"
         )
         assert all(i == j for i,j in zip(sorted(expected_plots), sorted(plots)))
 
     def test_plot_generation_for_comparison(self):
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
         make_result_file(
             gw=True, extension="hdf5", lalinference=True,
-            outdir="./.outdir_comparison/"
+            outdir="./.outdir_comparison/", n_samples=10
         )
         os.rename(
             "./.outdir_comparison/test.hdf5",
             "./.outdir_comparison/lalinference_example.h5"
         )
         make_result_file(
-            gw=True, extension="hdf5", bilby=True, outdir="./.outdir_comparison/"
+            gw=True, extension="hdf5", bilby=True, outdir="./.outdir_comparison/",
+            n_samples=10
         )
         os.rename(
             "./.outdir_comparison/test.h5",
             "./.outdir_comparison/bilby_example.h5"
         )
         default_arguments = [
             "--approximant", "IMRPhenomPv2", "IMRPhenomP",
             "--webdir", "./.outdir_comparison",
             "--samples", "./.outdir_comparison/bilby_example.h5",
             "./.outdir_comparison/lalinference_example.h5",
-            "--labels", "H10", "H11", "--no_ligo_skymap"]
+            "--labels", "H10", "H11", "--no_ligo_skymap", "--disable_expert"]
         opts = parser.parse_args(default_arguments)
-        inputs = GWInput(opts)
+        inputs = PlottingInput(opts)
         webpage = GWPlotGeneration(inputs)
         webpage.generate_plots()
         plots = sorted(glob("./.outdir_comparison/plots/*.png"))
         expected_plots = get_list_of_plots(
-            gw=True, label="H1", number=2, outdir=".outdir_comparison"
+            gw=True, label="H1", number=2, outdir="./.outdir_comparison"
         )
         for i,j in zip(sorted(plots), sorted(expected_plots)):
             print(i, j)
         assert all(i == j for i,j in zip(sorted(plots), sorted(expected_plots)))
 
     def test_plot_generation_for_add_to_existing(self):
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
         make_result_file(
             gw=True, extension="hdf5", lalinference=True,
-            outdir="./.outdir_add_to_existing2/"
+            outdir="./.outdir_add_to_existing2/", n_samples=10
         )
         os.rename(
             "./.outdir_add_to_existing2/test.hdf5",
             "./.outdir_add_to_existing2/lalinference_example.h5"
         )
         make_result_file(
             gw=True, extension="hdf5", bilby=True,
-            outdir="./.outdir_add_to_existing2/"
+            outdir="./.outdir_add_to_existing2/", n_samples=10
         )
         os.rename(
             "./.outdir_add_to_existing2/test.h5",
             "./.outdir_add_to_existing2/bilby_example.h5"
         )
         default_arguments = [
             "--approximant", "IMRPhenomPv2",
             "--webdir", "./.outdir_add_to_existing2",
             "--samples", "./.outdir_add_to_existing2/bilby_example.h5",
-            "--labels", "H10", "--no_ligo_skymap"]
+            "--labels", "H10", "--no_ligo_skymap", "--disable_expert"]
         opts = parser.parse_args(default_arguments)
-        inputs = GWInput(opts)
+        inputs = WebpagePlusPlottingPlusMetaFileInput(opts)
         webpage = GWPlotGeneration(inputs)
         webpage.generate_plots()
         webpage = GWWebpageGeneration(inputs)
         webpage.generate_webpages()
         meta_file = GWMetaFile(inputs)
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
         default_arguments = [
             "--approximant", "IMRPhenomP",
             "--existing_webdir", "./.outdir_add_to_existing2",
             "--samples", "./.outdir_add_to_existing2/lalinference_example.h5",
-            "--labels", "H11", "--no_ligo_skymap"]
+            "--labels", "H11", "--no_ligo_skymap", "--disable_expert"]
         opts = parser.parse_args(default_arguments)
-        inputs = GWInput(opts)
+        inputs = PlottingInput(opts)
         webpage = GWPlotGeneration(inputs) 
         webpage.generate_plots()
         plots = sorted(glob("./.outdir_add_to_existing2/plots/*.png"))
         expected_plots = get_list_of_plots(
-            gw=True, label="H1", number=2, outdir=".outdir_add_to_existing2"
+            gw=True, label="H1", number=2, outdir="./.outdir_add_to_existing2"
         )
         assert all(i == j for i, j in zip(sorted(plots), sorted(expected_plots)))
 
     def test_plot_generation_for_multiple_without_comparison(self):
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
         make_result_file(
             gw=True, extension="hdf5", lalinference=True,
-            outdir="./.outdir_comparison_no_comparison/"
+            outdir="./.outdir_comparison_no_comparison/", n_samples=10
         )
         os.rename(
             "./.outdir_comparison_no_comparison/test.hdf5",
             "./.outdir_comparison_no_comparison/lalinference_example.h5"
         )
         make_result_file(
             gw=True, extension="hdf5", bilby=True,
-            outdir="./.outdir_comparison_no_comparison/"
+            outdir="./.outdir_comparison_no_comparison/", n_samples=10
         )
         os.rename(
             "./.outdir_comparison_no_comparison/test.h5",
             "./.outdir_comparison_no_comparison/bilby_example.h5"
         )
         default_arguments = [
             "--approximant", "IMRPhenomPv2", "IMRPhenomP",
             "--webdir", "./.outdir_comparison_no_comparison",
             "--samples", "./.outdir_comparison_no_comparison/bilby_example.h5",
             "./.outdir_comparison_no_comparison/lalinference_example.h5",
             "--labels", "H10", "H11", "--no_ligo_skymap",
-            "--disable_comparison"
+            "--disable_comparison", "--disable_expert"
         ]
         opts = parser.parse_args(default_arguments)
-        inputs = GWInput(opts)
+        inputs = PlottingInput(opts)
         webpage = GWPlotGeneration(inputs)
         webpage.generate_plots()
         plots = sorted(glob("./.outdir_comparison/plots/*.png"))
         expected_plots = get_list_of_plots(
             gw=True, label="H1", number=2, outdir=".outdir_comparison_no_comparison",
             comparison=False
         )
         for i,j in zip(sorted(plots), sorted(expected_plots)):
             print(i, j)
         assert all(i == j for i,j in zip(sorted(plots), sorted(expected_plots)))
 
     def test_plot_generation_for_add_to_existing_without_comparison(self):
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
         make_result_file(
             gw=True, extension="hdf5", lalinference=True,
-            outdir="./.outdir_add_to_existing_no_comparison/"
+            outdir="./.outdir_add_to_existing_no_comparison/", n_samples=10
         )
         os.rename(
             "./.outdir_add_to_existing_no_comparison/test.hdf5",
             "./.outdir_add_to_existing_no_comparison/lalinference_example.h5"
         )
         make_result_file(
             gw=True, extension="hdf5", bilby=True,
-            outdir="./.outdir_add_to_existing_no_comparison/"
+            outdir="./.outdir_add_to_existing_no_comparison/", n_samples=10
         )
         os.rename(
             "./.outdir_add_to_existing_no_comparison/test.h5",
             "./.outdir_add_to_existing_no_comparison/bilby_example.h5"
         )
         default_arguments = [
             "--approximant", "IMRPhenomPv2",
             "--webdir", "./.outdir_add_to_existing_no_comparison",
             "--samples", "./.outdir_add_to_existing_no_comparison/bilby_example.h5",
-            "--labels", "H10", "--no_ligo_skymap"]
+            "--labels", "H10", "--no_ligo_skymap", "--disable_expert"]
         opts = parser.parse_args(default_arguments)
-        inputs = GWInput(opts)
+        inputs = WebpagePlusPlottingPlusMetaFileInput(opts)
         webpage = GWPlotGeneration(inputs)
         webpage.generate_plots()
         webpage = GWWebpageGeneration(inputs)
         webpage.generate_webpages()
         meta_file = GWMetaFile(inputs)
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
         default_arguments = [
             "--approximant", "IMRPhenomP",
             "--existing_webdir", "./.outdir_add_to_existing_no_comparison",
             "--samples", "./.outdir_add_to_existing_no_comparison/lalinference_example.h5",
             "--labels", "H11", "--no_ligo_skymap",
-            "--disable_comparison"
+            "--disable_comparison", "--disable_expert"
         ]
         opts = parser.parse_args(default_arguments)
-        inputs = GWInput(opts)
+        inputs = PlottingInput(opts)
         webpage = GWPlotGeneration(inputs)
         webpage.generate_plots()
         plots = sorted(glob("./.outdir_add_to_existing_no_comparison/plots/*.png"))
         expected_plots = get_list_of_plots(
-            gw=True, label="H1", number=2, outdir=".outdir_add_to_existing_no_comparison",
+            gw=True, label="H1", number=2, outdir="./.outdir_add_to_existing_no_comparison",
             comparison=False
         )
         assert all(i == j for i, j in zip(sorted(plots), sorted(expected_plots)))
```

### Comparing `pesummary-0.9.1/pesummary/tests/read_test.py` & `pesummary-1.0.0/pesummary/tests/read_test.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,15 +1,23 @@
+# Licensed under an MIT style license -- see LICENSE.md
+
 import os
 import shutil
 import numpy as np
 
 from .base import make_result_file, testing_dir
 import pesummary
 from pesummary.gw.file.read import read as GWRead
 from pesummary.core.file.read import read as Read
+from pesummary.io import read, write
+import glob
+import tempfile
+
+tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 class BaseRead(object):
     """Base class to test the core functions in the Read and GWRead functions
     """
     def test_parameters(self, true, pesummary=False):
         """Test the parameter property
@@ -23,49 +31,57 @@
 
     def test_samples(self, true, pesummary=False):
         """Test the samples property
         """
         if pesummary:
             assert len(self.result.samples[0]) == 1000
             assert len(self.result.samples[0][0]) == 18
-            true_flat = [item for sublist in true for item in sublist]
-            flat = [item for sublist in self.result.samples[0] for item in sublist]
-            assert all(i in true_flat for i in flat)
-            assert all(i in flat for i in true_flat)
+            samples = self.result.samples[0]
+            parameters = self.result.parameters[0]
         else:
             assert len(self.result.samples) == 1000
             assert len(self.result.samples[0]) == 18
-            true_flat = [item for sublist in true for item in sublist]
-            flat = [item for sublist in self.result.samples for item in sublist]
-            assert all(i in true_flat for i in flat)
-            assert all(i in flat for i in true_flat)
+            samples = self.result.samples
+            parameters = self.result.parameters
+
+        idxs = [self.parameters.index(i) for i in parameters]
+        np.testing.assert_almost_equal(
+            np.array(samples), np.array(self.samples)[:, idxs]
+        )
+        for ind, param in enumerate(parameters):
+            samp = np.array(samples).T[ind]
+            idx = self.parameters.index(param)
+            np.testing.assert_almost_equal(samp, np.array(self.samples).T[idx])
 
     def test_samples_dict(self, true):
         """Test the samples_dict property
         """
         parameters = true[0]
         samples = true[1]
+
         for num, param in enumerate(parameters):
             specific_samples = [i[num] for i in samples]
             drawn_samples = self.result.samples_dict[param]
-            assert all(i == j for i, j in zip(drawn_samples, specific_samples))
+            np.testing.assert_almost_equal(drawn_samples, specific_samples)
 
     def test_version(self, true=None):
         """Test the version property
         """
         if true is None:
             assert self.result.input_version == "No version information found"
         else:
             assert self.result.input_version == true
 
     def test_extra_kwargs(self, true=None):
         """Test the extra_kwargs property
         """
         if true is None:
-            assert self.result.extra_kwargs == {"sampler": {"nsamples": 1000}, "meta_data": {}}
+            assert self.result.extra_kwargs == {
+                "sampler": {"nsamples": 1000}, "meta_data": {}
+            }
         else:
             assert sorted(self.result.extra_kwargs) == sorted(true)
 
     def test_injection_parameters(self, true, pesummary=False):
         """Test the injection_parameters property
         """
         if true is None:
@@ -82,18 +98,18 @@
                         assert math.isnan(self.result.injection_parameters[i])
                     else:
                         assert true[i] == self.result.injection_parameters[i]
 
     def test_to_dat(self):
         """Test the to_dat method
         """
-        self.result.to_dat(outdir=".outdir", label="label")
-        assert os.path.isfile(os.path.join(".outdir", "pesummary_label.dat"))
+        self.result.to_dat(outdir=tmpdir, label="label")
+        assert os.path.isfile(os.path.join(tmpdir, "pesummary_label.dat"))
         data = np.genfromtxt(
-            os.path.join(".outdir", "pesummary_label.dat"), names=True)
+            os.path.join(tmpdir, "pesummary_label.dat"), names=True)
         assert all(i in self.parameters for i in list(data.dtype.names))
         assert all(i in list(data.dtype.names) for i in self.parameters)
         for param in self.parameters:
             assert np.testing.assert_almost_equal(
                 data[param], self.result.samples_dict[param], 8
             ) is None
 
@@ -108,24 +124,25 @@
         .downsample method downsamples to the specified number of samples,
         that it only takes samples that are currently in the posterior
         table and that it maintains concurrent samples.
         """
         old_samples_dict = self.result.samples_dict
         nsamples = 50
         self.result.downsample(nsamples)
-        assert self.result.samples_dict.number_of_samples == nsamples
+        new_samples_dict = self.result.samples_dict
+        assert new_samples_dict.number_of_samples == nsamples
         for param in self.parameters:
             assert all(
                 samp in old_samples_dict[param] for samp in
-                self.result.samples_dict[param]
+                new_samples_dict[param]
             )
         for num in range(nsamples):
             samp_inds = [
                 old_samples_dict[param].tolist().index(
-                    self.result.samples_dict[param][num]
+                    new_samples_dict[param][num]
                 ) for param in self.parameters
             ]
             assert len(set(samp_inds)) == 1
 
 
 class GWBaseRead(BaseRead):
     """Base class to test the GWRead specific functions
@@ -186,19 +203,19 @@
         pass
 
     def test_to_lalinference_dat(self):
         """Test the to_lalinference dat=True method
         """
         from pesummary.gw.file.standard_names import lalinference_map
 
-        self.result.to_lalinference(dat=True, outdir=".outdir",
+        self.result.to_lalinference(dat=True, outdir=tmpdir,
                                     filename="lalinference_label.dat")
-        assert os.path.isfile(os.path.join(".outdir", "lalinference_label.dat"))
+        assert os.path.isfile(os.path.join(tmpdir, "lalinference_label.dat"))
         data = np.genfromtxt(
-            os.path.join(".outdir", "lalinference_label.dat"), names=True)
+            os.path.join(tmpdir, "lalinference_label.dat"), names=True)
         for param in data.dtype.names:
             if param not in self.result.parameters:
                 pesummary_param = lalinference_map[param]
             else:
                 pesummary_param = param
             assert np.testing.assert_almost_equal(
                 data[param], self.result.samples_dict[pesummary_param], 8
@@ -214,30 +231,34 @@
 
 class TestCoreJsonFile(BaseRead):
     """Class to test loading in a JSON file with the core Read function
     """
     def setup(self):
         """Setup the TestCoreJsonFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        self.parameters, self.samples = make_result_file(extension="json", gw=False)
-        self.path = os.path.join(".outdir", "test.json")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="json", gw=False
+        )
+        self.path = os.path.join(tmpdir, "test.json")
         self.result = Read(self.path)
 
     def teardown(self):
         """Remove all files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
-        assert isinstance(self.result, pesummary.core.file.formats.default.Default)
+        assert isinstance(
+            self.result, pesummary.core.file.formats.default.SingleAnalysisDefault
+        )
 
     def test_parameters(self):
         """Test the parameter property of the default class
         """
         super(TestCoreJsonFile, self).test_parameters(self.parameters)
 
     def test_samples(self):
@@ -269,46 +290,52 @@
         """Test the to_dat method
         """
         super(TestCoreJsonFile, self).test_to_dat()
 
     def test_file_format_read(self):
         """Test that when the file_format is specified, that correct class is used
         """
-        from pesummary.core.file.formats.default import Default
+        from pesummary.core.file.formats.default import SingleAnalysisDefault
 
-        super(TestCoreJsonFile, self).test_file_format_read(self.path, "json", Default)
+        super(TestCoreJsonFile, self).test_file_format_read(
+            self.path, "json", SingleAnalysisDefault
+        )
 
     def test_downsample(self):
         """Test that the posterior table is correctly downsampled
         """
         super(TestCoreJsonFile, self).test_downsample()
 
 
 class TestCoreHDF5File(BaseRead):
     """Class to test loading in an HDF5 file with the core Read function
     """
     def setup(self):
         """Setup the TestCoreHDF5File class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        self.parameters, self.samples = make_result_file(extension="hdf5", gw=False)
-        self.path = os.path.join(".outdir", "test.h5")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="hdf5", gw=False
+        )
+        self.path = os.path.join(tmpdir, "test.h5")
         self.result = Read(self.path)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
-        assert isinstance(self.result, pesummary.core.file.formats.default.Default)
+        assert isinstance(
+            self.result, pesummary.core.file.formats.default.SingleAnalysisDefault
+        )
 
     def test_parameters(self):
         """Test the parameter property of the default class
         """
         super(TestCoreHDF5File, self).test_parameters(self.parameters)
 
     def test_samples(self):
@@ -342,46 +369,204 @@
         """Test the to_dat method
         """
         super(TestCoreHDF5File, self).test_to_dat()
 
     def test_file_format_read(self):
         """Test that when the file_format is specified, that correct class is used
         """
-        from pesummary.core.file.formats.default import Default
+        from pesummary.core.file.formats.default import SingleAnalysisDefault
 
-        super(TestCoreHDF5File, self).test_file_format_read(self.path, "hdf5", Default)
+        super(TestCoreHDF5File, self).test_file_format_read(self.path, "hdf5", SingleAnalysisDefault)
 
     def test_downsample(self):
         """Test that the posterior table is correctly downsampled
         """
         super(TestCoreHDF5File, self).test_downsample()
 
 
+class TestCoreCSVFile(BaseRead):
+    """Class to test loading in a csv file with the core Read function
+    """
+    def setup(self):
+        """Setup the TestCoreCSVFile class
+        """
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            extension="csv", outdir=tmpdir, gw=False
+        )
+        self.path = os.path.join(tmpdir, "test.csv")
+        self.result = Read(self.path)
+
+    def teardown(self):
+        """Remove the files and directories created from this class
+        """
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def test_class_name(self):
+        """Test the class used to load in this file
+        """
+        assert isinstance(
+            self.result, pesummary.core.file.formats.default.SingleAnalysisDefault
+        )
+
+    def test_parameters(self):
+        """Test the parameter property of the default class
+        """
+        super(TestCoreCSVFile, self).test_parameters(self.parameters)
+
+    def test_samples(self):
+        """Test the samples property of the default class
+        """
+        super(TestCoreCSVFile, self).test_samples(self.samples)
+
+    def test_samples_dict(self):
+        """Test the samples_dict property of the default class
+        """
+        true = [self.parameters, self.samples]
+        super(TestCoreCSVFile, self).test_samples_dict(true)
+
+    def test_version(self):
+        """Test the version property of the default class
+        """
+        super(TestCoreCSVFile, self).test_version()
+
+    def test_extra_kwargs(self):
+        """Test the extra_kwargs property of the default class
+        """
+        super(TestCoreCSVFile, self).test_extra_kwargs()
+
+    def test_injection_parameters(self):
+        """Test the injection_parameters property
+        """
+        true = {par: float("nan") for par in self.parameters}
+        super(TestCoreCSVFile, self).test_injection_parameters(true)
+
+    def test_to_dat(self):
+        """Test the to_dat method
+        """
+        super(TestCoreCSVFile, self).test_to_dat()
+
+    def test_file_format_read(self):
+        """Test that when the file_format is specified, that correct class is used
+        """
+        from pesummary.core.file.formats.default import SingleAnalysisDefault
+
+        super(TestCoreCSVFile, self).test_file_format_read(self.path, "csv", SingleAnalysisDefault)
+
+    def test_downsample(self):
+        """Test that the posterior table is correctly downsampled
+        """
+        super(TestCoreCSVFile, self).test_downsample()
+
+
+class TestCoreNumpyFile(BaseRead):
+    """Class to test loading in a numpy file with the core Read function
+    """
+    def setup(self):
+        """Setup the TestCoreNumpyFile class
+        """
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="npy", gw=False
+        )
+        self.path = os.path.join(tmpdir, "test.npy")
+        self.result = Read(self.path)
+
+    def teardown(self):
+        """Remove the files and directories created from this class
+        """
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def test_class_name(self):
+        """Test the class used to load in this file
+        """
+        assert isinstance(
+            self.result, pesummary.core.file.formats.default.SingleAnalysisDefault
+        )
+
+    def test_parameters(self):
+        """Test the parameter property of the default class
+        """
+        super(TestCoreNumpyFile, self).test_parameters(self.parameters)
+
+    def test_samples(self):
+        """Test the samples property of the default class
+        """
+        super(TestCoreNumpyFile, self).test_samples(self.samples)
+
+    def test_samples_dict(self):
+        """Test the samples_dict property of the default class
+        """
+        true = [self.parameters, self.samples]
+        super(TestCoreNumpyFile, self).test_samples_dict(true)
+
+    def test_version(self):
+        """Test the version property of the default class
+        """
+        super(TestCoreNumpyFile, self).test_version()
+
+    def test_extra_kwargs(self):
+        """Test the extra_kwargs property of the default class
+        """
+        super(TestCoreNumpyFile, self).test_extra_kwargs()
+
+    def test_injection_parameters(self):
+        """Test the injection_parameters property
+        """
+        true = {par: float("nan") for par in self.parameters}
+        super(TestCoreNumpyFile, self).test_injection_parameters(true)
+
+    def test_to_dat(self):
+        """Test the to_dat method
+        """
+        super(TestCoreNumpyFile, self).test_to_dat()
+
+    def test_file_format_read(self):
+        """Test that when the file_format is specified, that correct class is used
+        """
+        from pesummary.core.file.formats.default import SingleAnalysisDefault
+
+        super(TestCoreNumpyFile, self).test_file_format_read(self.path, "numpy", SingleAnalysisDefault)
+
+    def test_downsample(self):
+        """Test that the posterior table is correctly downsampled
+        """
+        super(TestCoreNumpyFile, self).test_downsample()
+
+
 class TestCoreDatFile(BaseRead):
     """Class to test loading in an dat file with the core Read function
     """
     def setup(self):
         """Setup the TestCoreDatFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        self.parameters, self.samples = make_result_file(extension="dat", gw=False)
-        self.path = os.path.join(".outdir", "test.dat")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="dat", gw=False
+        )
+        self.path = os.path.join(tmpdir, "test.dat")
         self.result = Read(self.path)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
-        assert isinstance(self.result, pesummary.core.file.formats.default.Default)
+        assert isinstance(
+            self.result, pesummary.core.file.formats.default.SingleAnalysisDefault
+        )
 
     def test_parameters(self):
         """Test the parameter property of the default class
         """
         super(TestCoreDatFile, self).test_parameters(self.parameters)
 
     def test_samples(self):
@@ -415,17 +600,17 @@
         """Test the to_dat method
         """
         super(TestCoreDatFile, self).test_to_dat()
 
     def test_file_format_read(self):
         """Test that when the file_format is specified, that correct class is used
         """
-        from pesummary.core.file.formats.default import Default
+        from pesummary.core.file.formats.default import SingleAnalysisDefault
 
-        super(TestCoreDatFile, self).test_file_format_read(self.path, "dat", Default)
+        super(TestCoreDatFile, self).test_file_format_read(self.path, "dat", SingleAnalysisDefault)
 
     def test_downsample(self):
         """Test that the posterior table is correctly downsampled
         """
         super(TestCoreDatFile, self).test_downsample()
 
 
@@ -463,15 +648,17 @@
         """Test the extra_kwargs property of the default class
         """
         true = {"sampler": {
             "log_bayes_factor": 0.5,
             "log_noise_evidence": 0.1,
             "log_evidence": 0.2,
             "log_evidence_err": 0.1},
-            "meta_data": {'time_marginalization': True}}
+            "meta_data": {'time_marginalization': True},
+            "other": {"likelihood": {"time_marginalization": "True"}}
+        }
         super(BilbyFile, self).test_extra_kwargs(true)
 
     def test_injection_parameters(self, true):
         """Test the injection_parameters property
         """
         super(BilbyFile, self).test_injection_parameters(true)
 
@@ -485,36 +672,40 @@
     def test_priors(self, read_function=Read):
         """Test that the priors are correctly extracted from the bilby result
         file
         """
         for param, prior in self.result.priors["samples"].items():
             assert isinstance(prior, np.ndarray)
         f = read_function(self.path, disable_prior=True)
-        assert not hasattr(f, "priors")
+        assert not len(f.priors["samples"])
+        f = read_function(self.path, nsamples_for_prior=200)
+        params = list(f.priors["samples"].keys())
+        assert len(f.priors["samples"][params[0]]) == 200
         
 
 
 class TestCoreJsonBilbyFile(BilbyFile):
     """Class to test loading in a bilby json file with the core Read function
     """
     def setup(self):
         """Setup the TestCoreBilbyFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
         self.parameters, self.samples = make_result_file(
-            extension="json", gw=False, bilby=True)
-        self.path = os.path.join(".outdir", "test.json")
+            outdir=tmpdir, extension="json", gw=False, bilby=True
+        )
+        self.path = os.path.join(tmpdir, "test.json")
         self.result = Read(self.path)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
         super(TestCoreJsonBilbyFile, self).test_class_name()
 
     def test_parameters(self):
@@ -573,26 +764,27 @@
 
 class TestCoreHDF5BilbyFile(BilbyFile):
     """Class to test loading in a bilby hdf5 file with the core Read function
     """
     def setup(self):
         """Setup the TestCoreBilbyFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
         self.parameters, self.samples = make_result_file(
-            extension="hdf5", gw=False, bilby=True)
-        self.path = os.path.join(".outdir", "test.h5")
+            outdir=tmpdir, extension="hdf5", gw=False, bilby=True
+        )
+        self.path = os.path.join(tmpdir, "test.h5")
         self.result = Read(self.path)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
         super(TestCoreHDF5BilbyFile, self).test_class_name()
 
     def test_parameters(self):
@@ -687,35 +879,35 @@
         assert list(self.result.samples_dict.keys()) == ["label"]
 
         parameters = self.parameters
         samples = self.samples
         for num, param in enumerate(parameters):
             specific_samples = [i[num] for i in samples]
             drawn_samples = self.result.samples_dict["label"][param]
-            assert all(i == j for i, j in zip(drawn_samples, specific_samples))
+            np.testing.assert_almost_equal(drawn_samples, specific_samples)
 
     def test_to_bilby(self):
         """Test the to_bilby method
         """
         from pesummary.core.file.read import is_bilby_json_file
 
         bilby_object = self.result.to_bilby(save=False)["label"]
         bilby_object.save_to_file(
-            filename=os.path.join(".outdir", "bilby.json"))
-        assert is_bilby_json_file(os.path.join(".outdir", "bilby.json"))
+            filename=os.path.join(tmpdir, "bilby.json"))
+        assert is_bilby_json_file(os.path.join(tmpdir, "bilby.json"))
 
     def test_to_dat(self):
         """Test the to_dat method
         """
         self.result.to_dat(
-            outdir=".outdir", filenames={"label": "pesummary_label.dat"}
+            outdir=tmpdir, filenames={"label": "pesummary_label.dat"}
         )
-        assert os.path.isfile(os.path.join(".outdir", "pesummary_label.dat"))
+        assert os.path.isfile(os.path.join(tmpdir, "pesummary_label.dat"))
         data = np.genfromtxt(
-            os.path.join(".outdir", "pesummary_label.dat"), names=True)
+            os.path.join(tmpdir, "pesummary_label.dat"), names=True)
         assert all(i in self.parameters for i in list(data.dtype.names))
         assert all(i in list(data.dtype.names) for i in self.parameters)
 
     def test_downsample(self):
         """Test the .downsample method
         """
         old_samples_dict = self.result.samples_dict
@@ -741,25 +933,26 @@
 class TestCoreJsonPESummaryFile(PESummaryFile):
     """Class to test loading in a PESummary json file with the core Read
     function
     """
     def setup(self):
         """Setup the TestCorePESummaryFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
         self.parameters, self.samples = make_result_file(
-            extension="json", gw=False, pesummary=True)
-        self.result = Read(os.path.join(".outdir", "test.json"))
+            outdir=tmpdir, extension="json", gw=False, pesummary=True
+        )
+        self.result = Read(os.path.join(tmpdir, "test.json"))
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
         super(TestCoreJsonPESummaryFile, self).test_class_name()
 
     def test_parameters(self):
@@ -818,25 +1011,26 @@
 class TestCoreHDF5PESummaryFile(PESummaryFile):
     """Class to test loading in a PESummary hdf5 file with the core Read
     function
     """
     def setup(self):
         """Setup the TestCorePESummaryFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
         self.parameters, self.samples = make_result_file(
-            extension="hdf5", gw=False, pesummary=True)
-        self.result = Read(os.path.join(".outdir", "test.h5"))
+            outdir=tmpdir, extension="hdf5", gw=False, pesummary=True
+        )
+        self.result = Read(os.path.join(tmpdir, "test.h5"))
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
         super(TestCoreHDF5PESummaryFile, self).test_class_name()
 
     def test_parameters(self):
@@ -888,36 +1082,198 @@
 
     def test_downsample(self):
         """Test that the posterior table is correctly downsampled
         """
         super(TestCoreHDF5PESummaryFile, self).test_downsample()
 
 
+class TestGWCSVFile(GWBaseRead):
+    """Class to test loading in a csv file with the core Read function
+    """
+    def setup(self):
+        """Setup the TestGWCSVFile class
+        """
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="csv", gw=True
+        )
+        self.path = os.path.join(tmpdir, "test.csv")
+        self.result = GWRead(self.path)
+
+    def teardown(self):
+        """Remove the files and directories created from this class
+        """
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def test_class_name(self):
+        """Test the class used to load in this file
+        """
+        assert isinstance(
+            self.result, pesummary.gw.file.formats.default.SingleAnalysisDefault
+        )
+
+    def test_parameters(self):
+        """Test the parameter property of the default class
+        """
+        super(TestGWCSVFile, self).test_parameters(self.parameters)
+
+    def test_samples(self):
+        """Test the samples property of the default class
+        """
+        super(TestGWCSVFile, self).test_samples(self.samples)
+
+    def test_samples_dict(self):
+        """Test the samples_dict property of the default class
+        """
+        true = [self.parameters, self.samples]
+        super(TestGWCSVFile, self).test_samples_dict(true)
+
+    def test_version(self):
+        """Test the version property of the default class
+        """
+        super(TestGWCSVFile, self).test_version()
+
+    def test_extra_kwargs(self):
+        """Test the extra_kwargs property of the default class
+        """
+        super(TestGWCSVFile, self).test_extra_kwargs()
+
+    def test_injection_parameters(self):
+        """Test the injection_parameters property
+        """
+        true = {par: float("nan") for par in self.parameters}
+        super(TestGWCSVFile, self).test_injection_parameters(true)
+
+    def test_to_dat(self):
+        """Test the to_dat method
+        """
+        super(TestGWCSVFile, self).test_to_dat()
+
+    def test_to_lalinference_dat(self):
+        """Test the to_lalinference dat=True method
+        """
+        super(TestGWCSVFile, self).test_to_lalinference_dat()
+
+    def test_file_format_read(self):
+        """Test that when the file_format is specified, that correct class is used
+        """
+        from pesummary.gw.file.formats.default import SingleAnalysisDefault
+
+        super(TestGWCSVFile, self).test_file_format_read(
+            self.path, "csv", SingleAnalysisDefault
+        )
+
+
+class TestGWNumpyFile(GWBaseRead):
+    """Class to test loading in a npy file with the core Read function
+    """
+    def setup(self):
+        """Setup the TestGWNumpyFile class
+        """
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="npy", gw=True
+        )
+        self.path = os.path.join(tmpdir, "test.npy")
+        self.result = GWRead(self.path)
+
+    def teardown(self):
+        """Remove the files and directories created from this class
+        """
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def test_class_name(self):
+        """Test the class used to load in this file
+        """
+        assert isinstance(
+            self.result, pesummary.gw.file.formats.default.SingleAnalysisDefault
+        )
+
+    def test_parameters(self):
+        """Test the parameter property of the default class
+        """
+        super(TestGWNumpyFile, self).test_parameters(self.parameters)
+
+    def test_samples(self):
+        """Test the samples property of the default class
+        """
+        super(TestGWNumpyFile, self).test_samples(self.samples)
+
+    def test_samples_dict(self):
+        """Test the samples_dict property of the default class
+        """
+        true = [self.parameters, self.samples]
+        super(TestGWNumpyFile, self).test_samples_dict(true)
+
+    def test_version(self):
+        """Test the version property of the default class
+        """
+        super(TestGWNumpyFile, self).test_version()
+
+    def test_extra_kwargs(self):
+        """Test the extra_kwargs property of the default class
+        """
+        super(TestGWNumpyFile, self).test_extra_kwargs()
+
+    def test_injection_parameters(self):
+        """Test the injection_parameters property
+        """
+        true = {par: float("nan") for par in self.parameters}
+        super(TestGWNumpyFile, self).test_injection_parameters(true)
+
+    def test_to_dat(self):
+        """Test the to_dat method
+        """
+        super(TestGWNumpyFile, self).test_to_dat()
+
+    def test_to_lalinference_dat(self):
+        """Test the to_lalinference dat=True method
+        """
+        super(TestGWNumpyFile, self).test_to_lalinference_dat()
+
+    def test_file_format_read(self):
+        """Test that when the file_format is specified, that correct class is used
+        """
+        from pesummary.gw.file.formats.default import SingleAnalysisDefault
+
+        super(TestGWNumpyFile, self).test_file_format_read(
+            self.path, "numpy", SingleAnalysisDefault
+        )
+
+
 class TestGWDatFile(GWBaseRead):
     """Class to test loading in an dat file with the core Read function
     """
     def setup(self):
         """Setup the TestGWDatFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        self.parameters, self.samples = make_result_file(extension="dat", gw=True)
-        self.path = os.path.join(".outdir", "test.dat")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="dat", gw=True
+        )
+        self.path = os.path.join(tmpdir, "test.dat")
         self.result = GWRead(self.path)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
-        assert isinstance(self.result, pesummary.gw.file.formats.default.Default)
+        assert isinstance(
+            self.result, pesummary.gw.file.formats.default.SingleAnalysisDefault
+        )
 
     def test_parameters(self):
         """Test the parameter property of the default class
         """
         super(TestGWDatFile, self).test_parameters(self.parameters)
 
     def test_samples(self):
@@ -956,46 +1312,52 @@
         """Test the to_lalinference dat=True method
         """
         super(TestGWDatFile, self).test_to_lalinference_dat()
 
     def test_file_format_read(self):
         """Test that when the file_format is specified, that correct class is used
         """
-        from pesummary.gw.file.formats.default import Default
+        from pesummary.gw.file.formats.default import SingleAnalysisDefault
 
-        super(TestGWDatFile, self).test_file_format_read(self.path, "dat", Default)
+        super(TestGWDatFile, self).test_file_format_read(
+            self.path, "dat", SingleAnalysisDefault
+        )
 
     def test_downsample(self):
         """Test that the posterior table is correctly downsampled
         """
         super(TestGWDatFile, self).test_downsample()
 
 
 class TestGWHDF5File(GWBaseRead):
     """Class to test loading in an HDF5 file with the gw Read function
     """
     def setup(self):
         """Setup the TestCoreHDF5File class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        self.parameters, self.samples = make_result_file(extension="hdf5", gw=True)
-        self.path = os.path.join(".outdir", "test.h5")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="hdf5", gw=True
+        )
+        self.path = os.path.join(tmpdir, "test.h5")
         self.result = GWRead(self.path)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
-        assert isinstance(self.result, pesummary.gw.file.formats.default.Default)
+        assert isinstance(
+            self.result, pesummary.gw.file.formats.default.SingleAnalysisDefault
+        )
 
     def test_parameters(self):
         """Test the parameter property of the default class
         """
         super(TestGWHDF5File, self).test_parameters(self.parameters)
 
     def test_samples(self):
@@ -1034,46 +1396,52 @@
         """Test the to_lalinference dat=True method
         """
         super(TestGWHDF5File, self).test_to_lalinference_dat()
 
     def test_file_format_read(self):
         """Test that when the file_format is specified, that correct class is used
         """
-        from pesummary.gw.file.formats.default import Default
+        from pesummary.gw.file.formats.default import SingleAnalysisDefault
 
-        super(TestGWHDF5File, self).test_file_format_read(self.path, "hdf5", Default)
+        super(TestGWHDF5File, self).test_file_format_read(
+            self.path, "hdf5", SingleAnalysisDefault
+        )
 
     def test_downsample(self):
         """Test that the posterior table is correctly downsampled
         """
         super(TestGWHDF5File, self).test_downsample()
 
 
 class TestGWJsonFile(GWBaseRead):
     """Class to test loading in an json file with the gw Read function
     """
     def setup(self):
         """Setup the TestGWDatFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        self.parameters, self.samples = make_result_file(extension="json", gw=True)
-        self.path = os.path.join(".outdir", "test.json")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters, self.samples = make_result_file(
+            outdir=tmpdir, extension="json", gw=True
+        )
+        self.path = os.path.join(tmpdir, "test.json")
         self.result = GWRead(self.path)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
-        assert isinstance(self.result, pesummary.gw.file.formats.default.Default)
+        assert isinstance(
+            self.result, pesummary.gw.file.formats.default.SingleAnalysisDefault
+        )
 
     def test_parameters(self):
         """Test the parameter property of the default class
         """
         super(TestGWJsonFile, self).test_parameters(self.parameters)
 
     def test_samples(self):
@@ -1112,42 +1480,45 @@
         """Test the to_lalinference dat=True method
         """
         super(TestGWJsonFile, self).test_to_lalinference_dat()
 
     def test_file_format_read(self):
         """Test that when the file_format is specified, that correct class is used
         """
-        from pesummary.gw.file.formats.default import Default
+        from pesummary.gw.file.formats.default import SingleAnalysisDefault
 
-        super(TestGWJsonFile, self).test_file_format_read(self.path, "json", Default)
+        super(TestGWJsonFile, self).test_file_format_read(
+            self.path, "json", SingleAnalysisDefault
+        )
 
     def test_downsample(self):
         """Test that the posterior table is correctly downsampled
         """
         super(TestGWJsonFile, self).test_downsample()
 
 
 class TestGWJsonBilbyFile(GWBaseRead):
     """Class to test loading in a bilby json file with the gw Read function
     """
     def setup(self):
         """Setup the TestCoreBilbyFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
         self.parameters, self.samples = make_result_file(
-            extension="json", gw=True, bilby=True)
-        self.path = os.path.join(".outdir", "test.json")
-        self.result = GWRead(self.path)
+            outdir=tmpdir, extension="json", gw=True, bilby=True
+        )
+        self.path = os.path.join(tmpdir, "test.json")
+        self.result = GWRead(self.path, disable_prior=True)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
         assert isinstance(self.result, pesummary.gw.file.formats.bilby.Bilby)
 
     def test_parameters(self):
@@ -1176,15 +1547,17 @@
         """Test the extra_kwargs property of the default class
         """
         true = {"sampler": {
             "log_bayes_factor": 0.5,
             "log_noise_evidence": 0.1,
             "log_evidence": 0.2,
             "log_evidence_err": 0.1},
-            "meta_data": {"time_marginalization": True}}
+            "meta_data": {"time_marginalization": True},
+            "other": {"likelihood": {"time_marginalization": "True"}}
+        }
         super(TestGWJsonBilbyFile, self).test_extra_kwargs(true)
 
     def test_injection_parameters(self):
         """Test the injection_parameters property
         """
         true = {par: 1. for par in self.parameters}
         super(TestGWJsonBilbyFile, self).test_injection_parameters(true)
@@ -1211,42 +1584,62 @@
         """
         super(TestGWJsonBilbyFile, self).test_downsample()
 
     def test_priors(self, read_function=GWRead):
         """Test that the priors are correctly extracted from the bilby result
         file
         """
+        self.result = GWRead(self.path)
         assert "final_mass_source_non_evolved" not in self.result.parameters
         for param, prior in self.result.priors["samples"].items():
             assert isinstance(prior, np.ndarray)
         assert "final_mass_source_non_evolved" in self.result.priors["samples"].keys()
         f = read_function(self.path, disable_prior_conversion=True)
         assert "final_mass_source_non_evolved" not in f.priors["samples"].keys()
         f = read_function(self.path, disable_prior=True)
-        assert not hasattr(f, "priors")
+        assert not len(f.priors["samples"])
+        f = read_function(self.path, nsamples_for_prior=200)
+        params = list(f.priors["samples"].keys())
+        assert len(f.priors["samples"][params[0]]) == 200
 
 
 class TestGWLALInferenceFile(GWBaseRead):
     """Class to test loading in a LALInference file with the gw Read function
     """
     def setup(self):
         """Setup the TestCoreBilbyFile class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
         self.parameters, self.samples = make_result_file(
-            extension="hdf5", gw=True, lalinference=True)
-        self.path = os.path.join(".outdir", "test.hdf5")
+            outdir=tmpdir, extension="hdf5", gw=True, lalinference=True
+        )
+        self.path = os.path.join(tmpdir, "test.hdf5")
         self.result = GWRead(self.path)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def test_hdf5_dataset_to_list(self):
+        """Test method to convert hdf5 dataset to list
+        """
+        import h5py
+        f = h5py.File(self.path)
+        path_to_samples = "lalinference/lalinference_nest/posterior_samples"
+        parameters = f[path_to_samples].dtype.names
+        old = [
+            [float(i[parameters.index(j)]) for j in parameters] for
+            i in f[path_to_samples]
+        ]
+        new = np.array(f[path_to_samples]).view((float, len(parameters))).tolist()
+        for n in range(len(old)):
+            np.testing.assert_almost_equal(old[n], new[n])
 
     def test_class_name(self):
         """Test the class used to load in this file
         """
         assert isinstance(
             self.result, pesummary.gw.file.formats.lalinference.LALInference)
 
@@ -1301,7 +1694,473 @@
             self.path, "lalinference", LALInference
         )
 
     def test_downsample(self):
         """Test that the posterior table is correctly downsampled
         """
         super(TestGWLALInferenceFile, self).test_downsample()
+
+
+class TestPublicPycbc(object):
+    """Test that data files produced by Nitz et al.
+    (https://github.com/gwastro/2-ogc) can be read in correctly.
+    """
+    def setup(self):
+        """Setup the TestCoreBilbyFile class
+        """
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+
+    def teardown(self):
+        """Remove the files and directories created from this class
+        """
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def _pycbc_check(self, filename):
+        """Test a public pycbc posterior samples file
+
+        Parameters
+        ----------
+        filename: str
+            url of pycbc posterior samples file you wish to download, read and
+            test
+        """
+        from pesummary.core.fetch import download_and_read_file
+        from pesummary.gw.file.standard_names import standard_names
+        import h5py
+        self.file = download_and_read_file(
+            filename, read_file=False, outdir=tmpdir
+        )
+        self.result = GWRead(self.file, path_to_samples="samples")
+        samples = self.result.samples_dict
+        fp = h5py.File(self.file, 'r')
+        fp_samples = fp["samples"]
+        for param in fp_samples.keys():
+            np.testing.assert_almost_equal(
+                fp_samples[param], samples[standard_names.get(param, param)]
+            )
+        fp.close()
+
+    def test_2_OGC(self):
+        """Test the samples released as part of the 2-OGC catalog
+        """
+        self._pycbc_check(
+            "https://github.com/gwastro/2-ogc/raw/master/posterior_samples/"
+            "H1L1V1-EXTRACT_POSTERIOR_150914_09H_50M_45UTC-0-1.hdf"
+        )
+
+    def test_3_OGC(self):
+        """Test the samples released as part of the 3-OGC catalog
+        """
+        self._pycbc_check(
+            "https://github.com/gwastro/3-ogc/raw/master/posterior/"
+            "GW150914_095045-PYCBC-POSTERIOR-XPHM.hdf"
+        )
+        
+
+class TestPublicPrincetonO1O2(object):
+    """Test that data files produced by Venumadhav et al.
+    (https://github.com/jroulet/O2_samples) can be read in correctly
+    """
+    def setup(self):
+        """Setup the TestCoreBilbyFile class
+        """
+        from pesummary.core.fetch import download_and_read_file
+        if not os.path.isdir(".outdir"):
+            os.mkdir(".outdir")
+        self.file = download_and_read_file(
+            "https://github.com/jroulet/O2_samples/raw/master/GW150914.npy",
+            read_file=False, outdir=".outdir"
+        )
+        self.result = GWRead(self.file, file_format="princeton")
+
+    def teardown(self):
+        """Remove the files and directories created from this class
+        """
+        if os.path.isdir(".outdir"):
+            shutil.rmtree(".outdir")
+
+    def test_samples_dict(self):
+        """
+        """
+        data = np.load(self.file)
+        samples = self.result.samples_dict
+        map = {
+            "mchirp": "chirp_mass", "eta": "symmetric_mass_ratio",
+            "s1z": "spin_1z", "s2z": "spin_2z", "RA": "ra", "DEC": "dec",
+            "psi": "psi", "iota": "iota", "vphi": "phase", "tc": "geocent_time",
+            "DL": "luminosity_distance"
+        }
+        columns = [
+            'mchirp', 'eta', 's1z', 's2z', 'RA', 'DEC', 'psi', 'iota', 'vphi',
+            'tc', 'DL'
+        ]
+        for num, param in enumerate(columns):
+            np.testing.assert_almost_equal(data.T[num], samples[map[param]])
+
+
+class TestMultiAnalysis(object):
+    """Class to test that a file which contains multiple analyses can be read
+    in appropiately
+    """
+    def setup(self):
+        """Setup the TestMultiAnalysis class
+        """
+        from pesummary.utils.samples_dict import MultiAnalysisSamplesDict
+        from pesummary.io import write
+
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.data = MultiAnalysisSamplesDict(
+            {"label1": {
+                "mass_1": np.random.uniform(20, 100, 10),
+                "mass_2": np.random.uniform(5, 20, 10),
+            }, "label2": {
+                "mass_1": np.random.uniform(20, 100, 10),
+                "mass_2": np.random.uniform(5, 20, 10)
+            }}
+        )
+        write(
+            self.data, file_format="sql", filename="multi_analysis.db",
+            outdir=tmpdir, overwrite=True, delete_existing=True
+        )
+        self.result = read(
+            os.path.join(tmpdir, "multi_analysis.db"),
+            add_zero_likelihood=False, remove_row_column="ROW"
+        )
+        self.samples_dict = self.result.samples_dict
+
+    def teardown(self):
+        """Remove all files and directories created from this class
+        """
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def test_multi_analysis_db(self):
+        """Test that an sql database with more than one set of samples can
+        be read in appropiately
+        """
+        assert sorted(self.samples_dict.keys()) == sorted(self.data.keys())
+        for key in self.samples_dict.keys():
+            assert sorted(self.samples_dict[key].keys()) == sorted(
+                self.data[key].keys()
+            )
+            for param in self.samples_dict[key].keys():
+                np.testing.assert_almost_equal(
+                    self.samples_dict[key][param], self.data[key][param]
+                )
+        self.result.generate_all_posterior_samples()
+        self.samples_dict = self.result.samples_dict
+        for key in self.samples_dict.keys():
+            assert "total_mass" in self.samples_dict[key].keys()
+            np.testing.assert_almost_equal(
+                self.data[key]["mass_1"] + self.data[key]["mass_2"],
+                self.samples_dict[key]["total_mass"]
+            )
+
+
+class TestSingleAnalysisChangeFormat(object):
+    """Test that when changing file format through the 'write' method, the
+    samples are conserved
+    """
+    def setup(self):
+        """Setup the TestChangeFormat class
+        """
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters = ["log_likelihood", "mass_1", "mass_2"]
+        self.samples = np.array(
+            [
+                np.random.uniform(20, 100, 1000),
+                np.random.uniform(5, 10, 1000), np.random.uniform(0, 1, 1000)
+            ]
+        ).T
+        write(
+            self.parameters, self.samples, outdir=tmpdir, filename="test.dat",
+            overwrite=True
+        )
+        self.result = read(os.path.join(tmpdir, "test.dat"))
+
+    def teardown(self):
+        """Remove all files and directories created from this class
+        """
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def save_and_check(
+        self, file_format, bilby=False, pesummary=False, lalinference=False
+    ):
+        """Save the result file and check the contents
+        """
+        if bilby:
+            filename = "test_bilby.json"
+        elif pesummary or lalinference:
+            filename = "test_pesummary.h5"
+        else:
+            filename = "test.{}".format(file_format)
+        self.result.write(
+            file_format=file_format, outdir=tmpdir, filename=filename
+        )
+        result = read(os.path.join(tmpdir, filename), disable_prior=True)
+        if pesummary:
+            assert result.parameters[0] == self.parameters
+            np.testing.assert_almost_equal(result.samples[0], self.samples)
+        else:
+            original = result.parameters
+            sorted_params = sorted(result.parameters)
+            idxs = [original.index(i) for i in sorted_params]
+            assert sorted(result.parameters) == self.parameters
+            np.testing.assert_almost_equal(
+                np.array(result.samples)[:, idxs], self.samples
+            )
+
+    def test_to_bilby(self):
+        """Test saving to bilby format
+        """
+        self.save_and_check("bilby", bilby=True)
+
+    def test_to_hdf5(self):
+        """Test saving to hdf5
+        """
+        self.save_and_check("hdf5")
+
+    def test_to_json(self):
+        """Test saving to json
+        """
+        self.save_and_check("json")
+
+    def test_to_sql(self):
+        """Test saving to sql
+        """
+        self.save_and_check("sql")
+
+    def test_to_pesummary(self):
+        self.save_and_check("pesummary", pesummary=True)
+
+    def test_to_lalinference(self):
+        self.save_and_check("lalinference", lalinference=True)
+
+
+class TestMultipleAnalysisChangeFormat(object):
+    """Test that when changing file format through the 'write' method, the
+    samples are conserved
+    """
+    def setup(self):
+        """Setup the TestMultiplAnalysisChangeFormat class
+        """
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parameters = [
+            ["log_likelihood", "mass_1", "mass_2"],
+            ["chirp_mass", "log_likelihood", "total_mass"]
+        ]
+        self.samples = np.array(
+            [np.array(
+                [
+                    np.random.uniform(20, 100, 1000),
+                    np.random.uniform(5, 10, 1000),
+                    np.random.uniform(0, 1, 1000)
+                ]
+            ).T, np.array(
+                [
+                    np.random.uniform(20, 100, 1000),
+                    np.random.uniform(5, 10, 1000),
+                    np.random.uniform(0, 1, 1000)
+                ]
+            ).T]
+        )
+        write(
+            self.parameters, self.samples, outdir=tmpdir, filename="test.db",
+            overwrite=True, file_format="sql"
+        )
+        self.result = read(os.path.join(tmpdir, "test.db"))
+
+    def teardown(self):
+        """Remove all files and directories created from this class
+        """
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+
+    def save_and_check(
+        self, file_format, bilby=False, pesummary=False, lalinference=False,
+        multiple_files=True
+    ):
+        """Save the result file and check the contents
+        """
+        if bilby:
+            filename = "test_bilby.json"
+        elif pesummary or lalinference:
+            filename = "test_pesummary.h5"
+        else:
+            filename = "test.{}".format(file_format)
+        self.result.write(
+            file_format=file_format, outdir=tmpdir, filename=filename
+        )
+        if multiple_files:
+            files = sorted(glob.glob(tmpdir + "/{}_*.{}".format(*filename.split("."))))
+            assert len(files) == 2
+            for num, _file in enumerate(files):
+                result = read(_file, disable_prior=True)
+                original = result.parameters
+                sorted_params = sorted(result.parameters)
+                idxs = [original.index(i) for i in sorted_params]
+                assert sorted(result.parameters) == self.parameters[num]
+                np.testing.assert_almost_equal(
+                    np.array(result.samples)[:, idxs], self.samples[num]
+                )
+        else:
+            result = read(os.path.join(tmpdir, filename), disable_prior=True)
+            original = result.parameters
+            sorted_params = sorted(result.parameters)
+            idxs = [original.index(i) for i in sorted_params]
+            for ii in range(len(original)):
+                assert result.parameters[ii] == self.parameters[ii]
+            np.testing.assert_almost_equal(
+                np.array(result.samples), self.samples
+            )
+
+    def test_to_bilby(self):
+        """Test saving to bilby
+        """
+        self.save_and_check("bilby", bilby=True)
+
+    def test_to_dat(self):
+        """Test saving to dat
+        """
+        self.save_and_check("dat")
+
+    def test_to_hdf5(self):
+        """Test saving to hdf5
+        """
+        self.save_and_check("hdf5")
+
+    def test_to_json(self):
+        """Test saving to json
+        """
+        self.save_and_check("json")
+
+    def test_to_sql(self):
+        """Test saving to sql
+        """
+        self.save_and_check("sql", multiple_files=False)
+
+    def test_to_pesummary(self):
+        self.save_and_check("pesummary", pesummary=True, multiple_files=False)
+
+    def test_to_lalinference(self):
+        self.save_and_check("lalinference", lalinference=True)
+
+
+def test_remove_nan_likelihoods():
+    """Test that samples with 'nan' log_likelihoods are removed from the
+    posterior table
+    """
+    from pesummary.utils.samples_dict import MultiAnalysisSamplesDict
+    import math
+
+    if not os.path.isdir(tmpdir):
+        os.mkdir(tmpdir)
+    parameters = ["a", "b", "log_likelihood"]
+    likelihoods = np.random.uniform(0, 1, 1000)
+    inds = np.random.choice(len(likelihoods), size=100, replace=False)
+    likelihoods[inds] = float('nan')
+    samples = np.array([
+        np.random.uniform(10, 5, 1000), np.random.uniform(10, 5, 1000),
+        likelihoods
+    ]).T
+    write(parameters, samples, filename="test.dat", outdir=tmpdir)
+    f = read("{}/test.dat".format(tmpdir), remove_nan_likelihood_samples=False)
+    read_samples = f.samples_dict
+    for param in parameters:
+        assert len(read_samples[param]) == 1000
+    for num, param in enumerate(parameters):
+        np.testing.assert_almost_equal(read_samples[param], samples.T[num])
+    f = read("{}/test.dat".format(tmpdir), remove_nan_likelihood_samples=True)
+    read_samples = f.samples_dict
+    for param in parameters:
+        assert len(read_samples[param]) == 900
+    inds = np.array([math.isnan(_) for _ in likelihoods], dtype=bool)
+    for num, param in enumerate(parameters):
+        np.testing.assert_almost_equal(
+            read_samples[param], samples.T[num][~inds]
+        )
+    likelihoods = np.random.uniform(0, 1, 2000).reshape(2, 1000)
+    inds = np.random.choice(1000, size=100, replace=False)
+    likelihoods[0][inds] = float('nan')
+    inds = np.random.choice(1000, size=500, replace=False)
+    likelihoods[1][inds] = float('nan')
+    samples = {
+        "one": {
+            "a": np.random.uniform(1, 5, 1000), "b": np.random.uniform(1, 2, 1000),
+            "log_likelihood": likelihoods[0]
+        }, "two": {
+            "c": np.random.uniform(1, 5, 1000), "d": np.random.uniform(1, 2, 1000),
+            "log_likelihood": likelihoods[1]
+        }
+    }
+    data = MultiAnalysisSamplesDict(samples)
+    write(
+        data, file_format="pesummary", filename="multi.h5", outdir=tmpdir,
+    )
+    f = read("{}/multi.h5".format(tmpdir), remove_nan_likelihood_samples=True)
+    _samples_dict = f.samples_dict
+    for num, label in enumerate(["one", "two"]):
+        inds = np.array([math.isnan(_) for _ in likelihoods[num]], dtype=bool)
+        if num == 0:
+            assert len(_samples_dict["one"]["a"]) == 900
+        else:
+            assert len(_samples_dict["two"]["c"]) == 500
+        for param in samples[label].keys():
+            np.testing.assert_almost_equal(
+                _samples_dict[label][param], samples[label][param][~inds]
+            )
+    if os.path.isdir(tmpdir):
+        shutil.rmtree(tmpdir)
+
+
+def test_add_log_likelihood():
+    """Test that zero log likelihood samples are added when the posterior table
+    does not include likelihood samples
+    """
+    from pesummary.utils.samples_dict import MultiAnalysisSamplesDict
+
+    if not os.path.isdir(tmpdir):
+        os.mkdir(tmpdir)
+    parameters = ["a", "b"]
+    samples = np.array([
+        np.random.uniform(10, 5, 1000), np.random.uniform(10, 5, 1000)
+    ]).T
+    write(parameters, samples, filename="test.dat", outdir=tmpdir)
+    f = read("{}/test.dat".format(tmpdir))
+    _samples_dict = f.samples_dict
+    assert sorted(f.parameters) == ["a", "b", "log_likelihood"]
+    np.testing.assert_almost_equal(
+        _samples_dict["log_likelihood"], np.zeros(1000)
+    )
+    np.testing.assert_almost_equal(_samples_dict["a"], samples.T[0])
+    np.testing.assert_almost_equal(_samples_dict["b"], samples.T[1])
+    parameters = [["a", "b"], ["c", "d"]]
+    samples = [
+        np.array([np.random.uniform(1, 5, 1000), np.random.uniform(1, 2, 1000)]).T,
+        np.array([np.random.uniform(1, 5, 1000), np.random.uniform(1, 2, 1000)]).T
+    ]
+    data = MultiAnalysisSamplesDict({
+        "one": {
+            "a": np.random.uniform(1, 5, 1000), "b": np.random.uniform(1, 2, 1000)
+        }, "two": {
+            "c": np.random.uniform(1, 5, 1000), "d": np.random.uniform(1, 2, 1000)
+        }
+    })
+    write(
+        data, file_format="pesummary", filename="multi.h5", outdir=tmpdir,
+    )
+    f = read("{}/multi.h5".format(tmpdir))
+    _samples_dict = f.samples_dict
+    np.testing.assert_almost_equal(
+        _samples_dict["one"]["log_likelihood"], np.zeros(1000)
+    )
+    np.testing.assert_almost_equal(
+        _samples_dict["two"]["log_likelihood"], np.zeros(1000)
+    )
+    if os.path.isdir(tmpdir):
+        shutil.rmtree(tmpdir)
```

### Comparing `pesummary-0.9.1/pesummary/tests/lalinference.sh` & `pesummary-1.0.0/pesummary/tests/lalinference.sh`

 * *Files 2% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 git lfs clone https://git.ligo.org/lscsoft/ROQ_data --include "**/params.dat,*/4s/**"
 mkdir -p lalinference/test/
 mkdir -p lalinference/lib
 curl https://git.ligo.org/charlie.hoy/lalsuite/raw/master/lalinference/lib/lalinference_pipe_example.ini -o lalinference_pipe_example.ini
 mv lalinference_pipe_example.ini lalinference/lib
 curl https://git.ligo.org/charlie.hoy/lalsuite/raw/master/lalinference/test/lalinference_nestedSampling_integration_test.sh -o fast_tutorial.sh
 mv fast_tutorial.sh lalinference/test
-curl https://git.ligo.org/charlie.hoy/lalsuite/raw/master/lalinference/test/injection_standard.xml -o injection_standard.xml
+curl https://git.ligo.org/lscsoft/lalsuite/raw/master/lalinference/test/injection_standard.xml -o injection_standard.xml
 mv injection_standard.xml lalinference/test
 path=`which lalinference_pipe`
 base=`python -c "print('/'.join('${path}'.split('/')[:-2]))"`
 sed -i "s;s|/home/albert.einstein/opt/lalsuite/| |;s|/home/albert.einstein/opt/lalsuite/|${base}|;" lalinference/test/fast_tutorial.sh
 sed -i "s;tolerance=20;tolerance=500;" lalinference/test/fast_tutorial.sh
 sed -i "s;cbcBayesPostProc;summarypages;" lalinference/lib/lalinference_pipe_example.ini
 bash lalinference/test/fast_tutorial.sh
```

### Comparing `pesummary-0.9.1/pesummary/tests/utils_test.py` & `pesummary-1.0.0/pesummary/tests/input_test.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,734 +1,684 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
 import shutil
-import h5py
-import numpy as np
+import glob
 import copy
 
-import pesummary
-from pesummary.io import write
-import pesummary.cli as cli
-from pesummary.utils import utils
-from pesummary.utils.tqdm import tqdm
-from pesummary.utils.dict import Dict
-from pesummary.utils.samples_dict import (
-    Array, SamplesDict, MCMCSamplesDict, MultiAnalysisSamplesDict
+import argparse
+from pesummary.gw.cli.inputs import WebpagePlusPlottingPlusMetaFileInput
+from pesummary.gw.cli.parser import (
+    add_dynamic_PSD_to_namespace, add_dynamic_calibration_to_namespace,
+    ArgumentParser
 )
-from pesummary._version_helper import GitInformation, PackageInformation
-from pesummary._version_helper import get_version_information
-
-import pytest
-from testfixtures import LogCapture
+from .base import make_result_file, gw_parameters, data_dir, testing_dir
 
+import numpy as np
+import h5py
 
-class TestGitInformation(object):
-    """Class to test the GitInformation helper class
-    """
-    def setup(self):
-        """Setup the TestGitInformation class
-        """
-        self.git = GitInformation(directory="/builds/lscsoft/pesummary/")
+import pytest
+import tempfile
 
-    def test_last_commit_info(self):
-        """Test the last_commit_info property
-        """
-        assert len(self.git.last_commit_info) == 2
-        assert isinstance(self.git.last_commit_info[0], str)
-        assert isinstance(self.git.last_commit_info[1], str)
+tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
-    def test_last_version(self):
-        """Test the last_version property
-        """
-        assert isinstance(self.git.last_version, str)
 
-    def test_status(self):
-        """Test the status property
-        """
-        assert isinstance(self.git.status, str)
+class TestCommandLine(object):
 
-    def test_builder(self):
-        """Test the builder property
-        """
-        assert isinstance(self.git.builder, str)
+    def setup(self):
+        self.parser = ArgumentParser()
+        self.parser.add_all_known_options_to_parser()
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        make_result_file(gw=True, lalinference=True, outdir=tmpdir)
+        os.rename(
+            "{}/test.hdf5".format(tmpdir),
+            "{}/lalinference_example.h5".format(tmpdir)
+        )
+        make_result_file(gw=True, bilby=True, outdir=tmpdir, extension="hdf5")
+        os.rename(
+            "{}/test.h5".format(tmpdir), "{}/bilby_example.h5".format(tmpdir)
+        )
 
-    def test_build_date(self):
-        """Test the build_date property
+    def teardown(self):
+        """Remove the files and directories created from this class
         """
-        assert isinstance(self.git.build_date, str)
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir) 
 
+    def test_webdir(self):
+        assert self.parser.get_default("webdir") == None
+        opts = self.parser.parse_args(["--webdir", "test"])
+        assert opts.webdir == "test"
+
+    def test_baseurl(self):
+        assert self.parser.get_default("baseurl") == None
+        opts = self.parser.parse_args(["--baseurl", "test"])
+        assert opts.baseurl == "test"
+
+    def test_add_to_existing(self):
+        assert self.parser.get_default("add_to_existing") == False
+        opts = self.parser.parse_args(["--add_to_existing"])
+        assert opts.add_to_existing == True
+
+    def test_approximant(self):
+        assert self.parser.get_default("approximant") == None
+        opts = self.parser.parse_args(["--approximant", "test"])
+        assert opts.approximant == ["test"]
+
+    def test_config(self):
+        assert self.parser.get_default("config") == None
+        opts = self.parser.parse_args(["--config", data_dir + "/example.ini"])
+        assert opts.config == [data_dir + "/example.ini"]
+
+    def test_dump(self):
+        assert self.parser.get_default("dump") == False
+        opts = self.parser.parse_args(["--dump"])
+        assert opts.dump == True
+
+    def test_email(self):
+        assert self.parser.get_default("email") == None
+        opts = self.parser.parse_args(["--email", "test"])
+        assert opts.email == "test"
+
+    def test_existing(self):
+        assert self.parser.get_default("existing") == None
+        opts = self.parser.parse_args(["--existing_webdir", "test"])
+        assert opts.existing == "test"
+
+    def test_gracedb(self):
+        assert self.parser.get_default("gracedb") == None
+        opts = self.parser.parse_args(["--gracedb", "Gtest"])
+        assert opts.gracedb == "Gtest"
+
+    def test_inj_file(self):
+        assert self.parser.get_default("inj_file") == None
+        opts = self.parser.parse_args(
+            ["--inj_file", testing_dir + "/main_injection.xml"]
+        )
+        assert opts.inj_file == [testing_dir + "/main_injection.xml"]
+
+    def test_samples(self):
+        assert self.parser.get_default("samples") == None
+        opts = self.parser.parse_args(
+            ["--samples", "{}/lalinference_example.h5".format(tmpdir)]
+        )
+        assert opts.samples == ["{}/lalinference_example.h5".format(tmpdir)]
+
+    def test_sensitivity(self):
+        assert self.parser.get_default("sensitivity") == False
+        opts = self.parser.parse_args(["--sensitivity"])
+        assert opts.sensitivity == True
+
+    def test_user(self):
+        assert self.parser.get_default("user") == "albert.einstein"
+        opts = self.parser.parse_args(["--user", "test"])
+        assert opts.user == "test"
+
+    def test_verbose(self):
+        opts = self.parser.parse_args(["-v"])
+        assert opts.verbose == True
+
+    def test_gwdata(self):
+        opts = self.parser.parse_args(["--gwdata", "H1:H1-CALIB-STRAIN:hello.lcf"])
+        assert opts.gwdata == {"H1:H1-CALIB-STRAIN": "hello.lcf"}
 
-class TestPackageInformation(object):
-    """Class to test the PackageInformation helper class
-    """
-    def setup(self):
-        """Setup the TestPackageInformation class
-        """
-        self.package = PackageInformation()
 
-    def test_package_info(self):
-        """Test the package_info property
-        """
-        pi = self.package.package_info
-        assert isinstance(pi, list)
-        pkg = pi[0]
-        assert "name" in pkg
-        assert "version" in pkg
-        if "build_string" in pkg:  # conda only
-            assert "channel" in pkg
+class TestInputExceptions(object):
 
-
-class TestUtils(object):
-    """Class to test pesummary.utils.utils
-    """
     def setup(self):
-        """Setup the TestUtils class
-        """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+        os.mkdir(tmpdir)
+        self.parser = ArgumentParser()
+        self.parser.add_all_known_options_to_parser()
+        make_result_file(gw=True, lalinference=True, outdir=tmpdir)
+        os.rename(
+            "{}/test.hdf5".format(tmpdir),
+            "{}/lalinference_example.h5".format(tmpdir)
+        )
+        make_result_file(gw=True, bilby=True, outdir=tmpdir, extension="hdf5")
+        os.rename(
+            "{}/test.h5".format(tmpdir), "{}/bilby_example.h5".format(tmpdir)
+        )
 
-    def teardown(self):
-        """Remove the files created from this class
-        """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
- 
-    def test_check_condition(self):
-        """Test the check_condition method
-        """
+    def test_no_webdir(self):
         with pytest.raises(Exception) as info:
-            condition = True
-            utils.check_condition(condition, "error")
-        assert str(info.value) == "error"
-
-    def test_rename_group_in_hf5_file(self):
-        """Test the rename_group_in_hf5_file method
-        """
-        f = h5py.File("./.outdir/rename_group.h5")
-        group = f.create_group("group")
-        group.create_dataset("example", data=np.array([10]))
-        f.close()
-        utils.rename_group_or_dataset_in_hf5_file("./.outdir/rename_group.h5",
-            group=["group", "replaced"])
-        f = h5py.File("./.outdir/rename_group.h5")
-        assert list(f.keys()) == ["replaced"]
-        assert list(f["replaced"].keys()) == ["example"]
-        assert len(f["replaced/example"]) == 1
-        assert f["replaced/example"][0] == 10
-        f.close()
-
-    def test_rename_dataset_in_hf5_file(self):
-        f = h5py.File("./.outdir/rename_dataset.h5")
-        group = f.create_group("group")
-        group.create_dataset("example", data=np.array([10]))
-        f.close()
-        utils.rename_group_or_dataset_in_hf5_file("./.outdir/rename_dataset.h5",
-            dataset=["group/example", "group/replaced"])
-        f = h5py.File("./.outdir/rename_dataset.h5")
-        assert list(f.keys()) == ["group"]
-        assert list(f["group"].keys()) == ["replaced"]
-        assert len(f["group/replaced"]) == 1
-        assert f["group/replaced"][0] == 10
-        f.close()
-
-    def test_rename_unknown_hf5_file(self):
+            opts = self.parser.parse_args([])
+            x = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert "Please provide a web directory" in str(info.value)
+
+    def test_make_webdir_if_it_does_not_exist(self):
+        assert os.path.isdir("{}/path".format(tmpdir)) == False
+        opts = self.parser.parse_args(['--webdir', '{}/path'.format(tmpdir),
+                                       '--approximant', 'IMRPhenomPv2',
+                                       '--samples', "{}/bilby_example.h5".format(tmpdir),
+                                       '--disable_prior_sampling', "--no_conversion"])
+        x = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert os.path.isdir("{}/path".format(tmpdir)) == True
+
+    def test_invalid_existing_directory(self):
+        if os.path.isdir("./.existing"):
+            shutil.rmtree("./.existing")
         with pytest.raises(Exception) as info:
-            utils.rename_group_or_dataset_in_hf5_file("./.outdir/unknown.h5",
-                group=["None", "replaced"])
-        assert "does not exist" in str(info.value) 
-
-    def test_directory_creation(self):
-        directory = './.outdir/test_dir'
-        assert os.path.isdir(directory) == False
-        utils.make_dir(directory)
-        assert os.path.isdir(directory) == True
-
-    def test_url_guess(self):
-        host = ["raven", "cit", "ligo-wa", "uwm", "phy.syr.edu", "vulcan",
-                "atlas", "iucca"]
-        expected = ["https://geo2.arcca.cf.ac.uk/~albert.einstein/test",
-                    "https://ldas-jobs.ligo.caltech.edu/~albert.einstein/test",
-                    "https://ldas-jobs.ligo-wa.caltech.edu/~albert.einstein/test",
-                    "https://ldas-jobs.phys.uwm.edu/~albert.einstein/test",
-                    "https://sugar-jobs.phy.syr.edu/~albert.einstein/test",
-                    "https://galahad.aei.mpg.de/~albert.einstein/test",
-                    "https://atlas1.atlas.aei.uni-hannover.de/~albert.einstein/test",
-                    "https://ldas-jobs.gw.iucaa.in/~albert.einstein/test"]
-        user = "albert.einstein"
-        webdir = '/home/albert.einstein/public_html/test'
-        for i,j in zip(host, expected):
-            url = utils.guess_url(webdir, i, user)
-            assert url == j
-
-    def test_make_dir(self):
-        """Test the make_dir method
-        """
-        assert not os.path.isdir(os.path.join(".outdir", "test"))
-        utils.make_dir(os.path.join(".outdir", "test"))
-        assert os.path.isdir(os.path.join(".outdir", "test"))
-        with open(os.path.join(".outdir", "test", "test.dat"), "w") as f:
-            f.writelines(["test"])
-        utils.make_dir(os.path.join(".outdir", "test"))
-        assert os.path.isfile(os.path.join(".outdir", "test", "test.dat"))
-
-    def test_resample_posterior_distribution(self):
-        """Test the resample_posterior_distribution method
-        """
-        data = np.random.normal(1, 0.1, 1000)
-        resampled = utils.resample_posterior_distribution([data], 500)
-        assert len(resampled) == 500
-        assert np.round(np.mean(resampled), 1) == 1.
-        assert np.round(np.std(resampled), 1) == 0.1
-
-    def test_gw_results_file(self):
-        """Test the gw_results_file method
-        """
-        from .base import namespace
-
-        opts = namespace({"gw": True, "psd": True})
-        assert utils.gw_results_file(opts)
-        opts = namespace({"webdir": ".outdir"})
-        assert not utils.gw_results_file(opts)
-
-    def test_functions(self):
-        """Test the functions method
-        """
-        from .base import namespace
-
-        opts = namespace({"gw": True, "psd": True})
-        funcs = utils.functions(opts)
-        assert funcs["input"] == pesummary.gw.inputs.GWInput
-        assert funcs["MetaFile"] == pesummary.gw.file.meta_file.GWMetaFile
-
-        opts = namespace({"webdir": ".outdir"})
-        funcs = utils.functions(opts)
-        assert funcs["input"] == pesummary.core.inputs.Input
-        assert funcs["MetaFile"] == pesummary.core.file.meta_file.MetaFile
-
-    def test_get_version_information(self):
-        """Test the get_version_information method
-        """
-        assert isinstance(get_version_information(), str)
-
+            opts = self.parser.parse_args(['--existing_webdir', './.existing'])
+            x = WebpagePlusPlottingPlusMetaFileInput(opts)
+        dir_name = os.path.abspath('./.existing')
+        assert "Please provide a valid existing directory" in str(info.value)
+
+    def test_not_base_of_existing_directory(self):
+        if os.path.isdir("./.existing2"):
+            shutil.rmtree("./.existing2")
+        if os.path.isdir("./.existing2/samples"):
+            shutil.rmtree("./.existing2/samples")
+        os.mkdir("./.existing2")
+        os.mkdir("./.existing2/samples")
+        opts = self.parser.parse_args(['--existing_webdir', './.existing2/samples'])
+        with pytest.raises(Exception) as info:
+            x = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert "Please provide a valid existing directory" in str(info.value)
 
-class TestGelmanRubin(object):
-    """Test the Gelman Rubin calculation
-    """
-    def test_same_as_lalinference(self):
-        """Test the Gelman rubin output from pesummary is the same as
-        the one coded in LALInference
-        """
-        from lalinference.bayespputils import Posterior
-        from pesummary.utils.utils import gelman_rubin
+    def test_add_to_existing_and_no_existing_flag(self):
+        opts = self.parser.parse_args(["--add_to_existing"])
+        with pytest.raises(Exception) as info:
+            x = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert "Please provide a web directory to store the webpages" in str(info.value)
 
-        header = ["a", "b", "logL", "chain"]
-        for _ in np.arange(100):
-            samples = np.array(
-                [
-                    np.random.uniform(np.random.random(), 0.1, 3).tolist() +
-                    [np.random.randint(1, 3)] for _ in range(10)
-                ]
-            )
-            obj = Posterior([header, np.array(samples)])
-            R = obj.gelman_rubin("a")
-            chains = np.unique(obj["chain"].samples)
-            chain_index = obj.names.index("chain")
-            param_index = obj.names.index("a")
-            data, _ = obj.samples()
-            chainData=[
-                data[data[:,chain_index] == chain, param_index] for chain in
-                chains
-            ]
-            np.testing.assert_almost_equal(
-                gelman_rubin(chainData, decimal=10), R, 7
-            )
+    def test_no_samples(self):
+        opts = self.parser.parse_args(["--webdir", tmpdir])
+        with pytest.raises(Exception) as info:
+            x = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert "Please provide a results file" in str(info.value)
 
-    def test_same_samples(self):
-        """Test that when passed two identical chains (perfect convergence),
-        the Gelman Rubin is 1
-        """
-        from pesummary.core.plots.plot import gelman_rubin
+    def test_non_existance_samples(self):
+        with pytest.raises(Exception) as info:
+            opts = self.parser.parse_args(["--webdir", tmpdir,
+                                           "--samples", "{}/no_existance".format(tmpdir)])
+            x = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert "{}/no_existance".format(tmpdir) in str(info.value)
+
+    def test_napproximant_not_equal_to_nsamples(self):
+        opts = self.parser.parse_args(["--webdir", tmpdir,
+                                       "--samples", "{}/bilby_example.h5".format(tmpdir),
+                                       "{}/bilby_example.h5".format(tmpdir),
+                                       "--disable_prior_sampling", "--no_conversion",
+                                       "--approximant", "IMRPhenomPv2"])
+        with pytest.raises(Exception) as info:
+            x = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert "Please pass an approximant for each" in str(info.value)
 
-        samples = np.random.uniform(1, 0.5, 10)
-        R = gelman_rubin([samples, samples])
-        assert R == 1
 
+class TestInput(object):
 
-class TestSamplesDict(object):
-    """Test the SamplesDict class
-    """
     def setup(self):
-        self.parameters = ["a", "b"]
-        self.samples = [
-            np.random.uniform(10, 0.5, 100), np.random.uniform(200, 10, 100)
-        ]
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        write(
-            self.parameters, np.array(self.samples).T, outdir=".outdir",
-            filename="test.dat", file_format="dat"
-        )
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
+        self.parser = ArgumentParser()
+        self.parser.add_all_known_options_to_parser()
+        make_result_file(gw=True, lalinference=True, outdir=tmpdir)
+        os.rename(
+            "{}/test.hdf5".format(tmpdir),
+            "{}/lalinference_example.h5".format(tmpdir)
+        )
+        data = make_result_file(gw=True, bilby=True, outdir=tmpdir, extension="hdf5")
+        self.parameters, self.samples = data
+        os.rename(
+            "{}/test.h5".format(tmpdir), "{}/bilby_example.h5".format(tmpdir)
+        )
+        self.default_arguments = [
+            "--approximant", "IMRPhenomPv2",
+            "--webdir", tmpdir,
+            "--samples", "{}/bilby_example.h5".format(tmpdir),
+            "--email", "albert.einstein@ligo.org",
+            "--gracedb", "Grace", "--no_conversion",
+            "--labels", "example", "--disable_prior_sampling"]
+        self.original_arguments = copy.deepcopy(self.default_arguments)
+        self.make_input_object()
 
     def teardown(self):
-        """Remove the files created from this class
-        """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
-
-    def test_initalize(self):
-        """Test that the two ways to initialize the SamplesDict class are
-        equivalent
-        """
-        base = SamplesDict(self.parameters, self.samples)
-        other = SamplesDict(
-            {
-                param: sample for param, sample in zip(
-                    self.parameters, self.samples
-                )
-            }
-        )
-        assert base.parameters == other.parameters
-        assert sorted(base.parameters) == sorted(self.parameters)
-        np.testing.assert_almost_equal(base.samples, other.samples)
-        assert sorted(list(base.keys())) == sorted(list(other.keys()))
-        np.testing.assert_almost_equal(base.samples, self.samples)
-        class_method = SamplesDict.from_file(
-            ".outdir/test.dat", add_zero_likelihood=False
-        )
-        np.testing.assert_almost_equal(class_method.samples, self.samples)
-
-    def test_properties(self):
-        """Test that the properties of the SamplesDict class are correct
+        """Remove the files and directories created from this class
         """
-        import pandas as pd
-
-        dataset = SamplesDict(self.parameters, self.samples)
-        assert sorted(dataset.minimum.keys()) == sorted(self.parameters)
-        assert dataset.minimum["a"] == np.min(self.samples[0])
-        assert dataset.minimum["b"] == np.min(self.samples[1])
-        assert dataset.median["a"] == np.median(self.samples[0])
-        assert dataset.median["b"] == np.median(self.samples[1])
-        assert dataset.mean["a"] == np.mean(self.samples[0])
-        assert dataset.mean["b"] == np.mean(self.samples[1])
-        assert dataset.number_of_samples == len(self.samples[1])
-        assert len(dataset.downsample(10)["a"]) == 10
-        dataset = SamplesDict(self.parameters, self.samples)
-        assert len(dataset.discard_samples(10)["a"]) == len(self.samples[0]) - 10
-        p = dataset.to_pandas()
-        assert isinstance(p, pd.core.frame.DataFrame)
-        remove = dataset.pop("a")
-        assert list(dataset.keys()) == ["b"]
-
-
-class TestMultiAnalysisSamplesDict(object):
-    """Test the MultiAnalysisSamplesDict class
-    """
-    def setup(self):
-        self.parameters = ["a", "b"]
-        self.samples = [
-            [np.random.uniform(10, 0.5, 100), np.random.uniform(100, 10, 100)],
-            [np.random.uniform(5, 0.5, 100), np.random.uniform(80, 10, 100)],
-        ]
-        self.labels = ["one", "two"]
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        for num, _samples in enumerate(self.samples):
-            write(
-                self.parameters, np.array(_samples).T, outdir=".outdir",
-                filename="test_{}.dat".format(num + 1), file_format="dat"
-            )
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
-    def teardown(self):
-        """Remove the files created from this class
-        """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+    @staticmethod
+    def make_existing_file(path):
+        parameters = np.array(["mass_1", "mass_2", "luminosity_distance"],
+                              dtype="S")
+        samples = np.array([[10, 5, 400], [40, 20, 800], [50, 10, 200]])
+        injected_samples = np.array([float("nan"), float("nan"), float("nan")])
+
+        f = h5py.File(path + "/posterior_samples.h5", "w")
+        posterior_samples = f.create_group("posterior_samples")
+        label = posterior_samples.create_group("example")
+        label.create_dataset("parameter_names", data=parameters)
+        label.create_dataset("samples", data=samples)
+        injection_data = f.create_group("injection_data")
+        label = injection_data.create_group("example")
+        label.create_dataset("injection_values", data=injected_samples)
+        f.close()
+        return path + "/posterior_samples.h5"
 
-    def test_initalize(self):
-        """Test the different ways to initalize the class
-        """
-        dataframe = MultiAnalysisSamplesDict(
-            self.parameters, self.samples, labels=["one", "two"]
-        )
-        assert sorted(list(dataframe.keys())) == sorted(self.labels)
-        assert sorted(list(dataframe["one"])) == sorted(["a", "b"])
-        assert sorted(list(dataframe["two"])) == sorted(["a", "b"])
-        np.testing.assert_almost_equal(
-            dataframe["one"]["a"], self.samples[0][0]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["one"]["b"], self.samples[0][1]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["two"]["a"], self.samples[1][0]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["two"]["b"], self.samples[1][1]
+    def add_argument(self, argument, reset=False):
+        if reset:
+            self.default_arguments = self.original_arguments
+        if isinstance(argument, list):
+            for i in argument:
+                self.default_arguments.append(i)
+        else:
+            self.default_arguments.append(argument)
+        self.opts, unknown = self.parser.parse_known_args(self.default_arguments)
+        add_dynamic_PSD_to_namespace(self.opts)
+        add_dynamic_calibration_to_namespace(self.opts)
+        self.inputs = WebpagePlusPlottingPlusMetaFileInput(self.opts)
+
+    def replace_existing_argument(self, argument, new_value):
+        if argument in self.default_arguments:
+            index = self.default_arguments.index(argument)
+            arguments = self.default_arguments
+            arguments[index+1] = new_value
+            self.default_arguments = arguments
+        self.make_input_object()
+
+    def make_input_object(self):
+        self.opts, unknown = self.parser.parse_known_args(self.default_arguments)
+        add_dynamic_PSD_to_namespace(self.opts)
+        add_dynamic_calibration_to_namespace(self.opts)
+        self.inputs = WebpagePlusPlottingPlusMetaFileInput(self.opts)
+
+    def test_webdir(self):
+        assert self.inputs.webdir == os.path.abspath(tmpdir)
+
+    def test_samples(self):
+        assert self.inputs.result_files == ["{}/bilby_example.h5".format(tmpdir)]
+
+    def test_approximant(self):
+        assert self.inputs.approximant == {"example": "IMRPhenomPv2"}
+
+    def test_existing(self):
+        assert self.inputs.existing == None
+
+    def test_baseurl(self):
+        assert self.inputs.baseurl == "https://" + os.path.abspath(tmpdir)
+
+    def test_inj_file(self):
+        assert self.inputs.injection_file == [None]
+
+    def test_config(self):
+        assert self.inputs.config == [None]
+
+    def test_email(self):
+        assert self.inputs.email == "albert.einstein@ligo.org"
+
+    def test_add_to_existing(self):
+        assert self.inputs.add_to_existing == False
+
+    def test_sensitivity(self):
+        assert self.inputs.sensitivity == False
+
+    def test_dump(self):
+        assert self.inputs.dump == False
+        self.add_argument(["--dump"])
+        assert self.inputs.dump == True
+
+    def test_gracedb(self):
+        assert self.inputs.gracedb == "Grace"
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        default_arguments = [
+            "--approximant", "IMRPhenomPv2",
+            "--webdir", tmpdir,
+            "--samples", "{}/bilby_example.h5".format(tmpdir),
+            "--email", "albert.einstein@ligo.org",
+            "--gracedb", "Mock", "--disable_prior_sampling",
+            "--labels", "example", "--no_conversion"]
+        opts = parser.parse_args(default_arguments)
+        inputs = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert inputs.gracedb is None
+
+    def test_detectors(self):
+        assert self.inputs.detectors == {"example": None}
+
+    def test_labels(self):
+        assert self.inputs.labels == ["example"]
+        self.add_argument(["--label", "new_example"])
+        assert self.inputs.labels == ["new_example"]
+
+    def test_existing_labels(self):
+        assert self.inputs.existing_labels == None
+        path = self.make_existing_file("{}/samples".format(tmpdir))
+        with open("{}/home.html".format(tmpdir), "w") as f:
+            f.writelines("test")
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        default_arguments = [
+            "--approximant", "IMRPhenomPv2",
+            "--existing_webdir", tmpdir,
+            "--samples", "{}/bilby_example.h5".format(tmpdir), "--no_conversion",
+            "--gracedb", "Grace", "--disable_prior_sampling"]
+        opts = parser.parse_args(default_arguments)
+        inputs = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert inputs.existing_labels == ["example"]
+
+    def test_existing_samples(self):
+        assert self.inputs.existing_samples == None
+        path = self.make_existing_file("{}/samples".format(tmpdir))
+        with open("{}/home.html".format(tmpdir), "w") as f:
+            f.writelines("test")
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        default_arguments = [
+            "--approximant", "IMRPhenomPv2",
+            "--existing_webdir", tmpdir,
+            "--samples", "{}/bilby_example.h5".format(tmpdir),
+            "--email", "albert.einstein@ligo.org",
+            "--gracedb", "Grace"]
+        opts = parser.parse_args(default_arguments)
+        inputs = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert all(
+            i == j for i, j in zip(inputs.existing_samples["example"]["mass_1"], [10, 40, 50]))
+        assert all(
+            i == j for i,j in zip(inputs.existing_samples["example"]["mass_2"], [5, 20, 10]))
+        assert all(
+            i == j for i,j in zip(inputs.existing_samples["example"]["luminosity_distance"], [400, 800, 200]))
+
+    def test_psd(self):
+        with open("{}/psd.dat".format(tmpdir), "w") as f:
+            f.writelines(["1.00 3.44\n", "2.00 5.66\n", "3.00 4.56\n", "4.00 9.83\n"])
+        assert self.inputs.psd == {"example": {}}
+        self.add_argument(
+            ["--psd", "{}/psd.dat".format(tmpdir), "--f_low", "1.0", "--f_final", "3.0"]
+        )
+        assert list(self.inputs.psd["example"].keys()) == ["psd.dat"]
+        self.add_argument(
+            ["--psd", "H1:{}/psd.dat".format(tmpdir), "--f_low", "1.0", "--f_final", "3.0"]
+        )
+        assert list(self.inputs.psd["example"].keys()) == ["H1"]
+        np.testing.assert_almost_equal(
+            self.inputs.psd["example"]["H1"],
+            [[1.00, 3.44], [2.00, 5.66], [3.00, 4.56], [4.00, 9.83]]
+        )
+
+    def test_preliminary_pages_for_single_analysis(self):
+        """Test that preliminary watermarks are added when an analysis
+        is not reproducible
+        """
+        # when neither a psd or config is passed, add preliminary watermark
+        assert self.inputs.preliminary_pages == {"example": True}
+        # when a psd is added but not a config, add preliminary watermark
+        with open("{}/psd.dat".format(tmpdir), "w") as f:
+            f.writelines(["1.00 3.44\n", "2.00 5.66\n", "3.00 4.56\n", "4.00 9.83\n"])
+        self.add_argument(["--psd", "{}/psd.dat".format(tmpdir)])
+        assert self.inputs.preliminary_pages == {"example": True}
+        # when a config is added but no psd, add preliminary watermark
+        self.add_argument(
+            ["--config", data_dir + "/config_lalinference.ini"], reset=True
+        )
+        assert self.inputs.preliminary_pages == {"example": True}
+        # when both config and psd is added, do not add preliminary watermark
+        self.add_argument(
+            ["--psd", "{}/psd.dat".format(tmpdir)], reset=True
+        )
+        self.add_argument(["--config", data_dir + "/config_lalinference.ini"])
+        assert self.inputs.preliminary_pages == {"example": False}
+
+    def test_add_existing_plot(self):
+        """Test that existing plots are assigned correctly
+        """
+        # when no plot is passed, add_existing_plot = None
+        assert self.inputs.existing_plot is None
+        with open("{}/test.png".format(tmpdir), "w") as f:
+            f.writelines("")
+        with open("{}/test2.png".format(tmpdir), "w") as f:
+            f.writelines("")
+        # when the standard dict format is provided, assign to correct label
+        real_path = os.path.abspath(tmpdir)
+        self.add_argument(["--add_existing_plot", "example:{}/test.png".format(tmpdir)])
+        # check file now points to webdir/plots
+        assert self.inputs.existing_plot == {"example": real_path + "/plots/test.png"}
+        # check that it copied to plots dir
+        assert any("test.png" in i for i in glob.glob("{}/plots/*.png".format(tmpdir)))
+        os.remove("{}/plots/test.png".format(tmpdir))
+        # when a file is passed without label, assign plot to each label
+        self.add_argument(["--add_existing_plot", "{}/test.png".format(tmpdir)])
+        assert self.inputs.existing_plot == {"example": real_path + "/plots/test.png"}
+
+        os.remove("{}/plots/test.png".format(tmpdir))
+        # when multiple files are passed for one label, check that it is correctly
+        # assigned
+        self.add_argument(
+            [
+                "--add_existing_plot", "example:{}/test.png".format(tmpdir),
+                "example:{}/test2.png".format(tmpdir)
+            ]
         )
-        _other = MCMCSamplesDict({
-            label: {
-                param: self.samples[num][idx] for idx, param in enumerate(
-                    self.parameters
-                )
-            } for num, label in enumerate(self.labels)
-        })
-        class_method = MultiAnalysisSamplesDict.from_files(
-            {'one': ".outdir/test_1.dat", 'two': ".outdir/test_2.dat"},
-            add_zero_likelihood=False
-        )
-        for other in [_other, class_method]:
-            assert sorted(other.keys()) == sorted(dataframe.keys())
-            assert sorted(other["one"].keys()) == sorted(
-                dataframe["one"].keys()
-            )
-            np.testing.assert_almost_equal(
-                other["one"]["a"], dataframe["one"]["a"]
-            )
-            np.testing.assert_almost_equal(
-                other["one"]["b"], dataframe["one"]["b"]
-            )
-            np.testing.assert_almost_equal(
-                other["two"]["a"], dataframe["two"]["a"]
-            )
-            np.testing.assert_almost_equal(
-                other["two"]["b"], dataframe["two"]["b"]
-            )
-        
-
-    def test_different_samples_for_different_analyses(self):
-        """Test that nothing breaks when different samples have different parameters
-        """
-        data = {
-            "one": {
-                "a": np.random.uniform(10, 0.5, 100),
-                "b": np.random.uniform(5, 0.5, 100)
-            }, "two": {
-                "a": np.random.uniform(10, 0.5, 100)
-            }
+        assert self.inputs.existing_plot == {
+            "example": [real_path + "/plots/test.png", real_path + "/plots/test2.png"]
         }
-        dataframe = MultiAnalysisSamplesDict(data)
-        assert sorted(dataframe["one"].keys()) == sorted(data["one"].keys())
-        assert sorted(dataframe["two"].keys()) == sorted(data["two"].keys())
-        np.testing.assert_almost_equal(
-            dataframe["one"]["a"], data["one"]["a"]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["one"]["b"], data["one"]["b"]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["two"]["a"], data["two"]["a"]
-        )
-        with pytest.raises(ValueError):
-            transpose = dataframe.T
-
-
-class TestMCMCSamplesDict(object):
-    """Test the MCMCSamplesDict class
-    """
-    def setup(self):
-        self.parameters = ["a", "b"]
-        self.chains = [
-            [np.random.uniform(10, 0.5, 100), np.random.uniform(100, 10, 100)],
-            [np.random.uniform(5, 0.5, 100), np.random.uniform(80, 10, 100)]
-        ]
-
-    def test_initalize(self):
-        """Test the different ways to initalize the class
-        """
-        dataframe = MCMCSamplesDict(self.parameters, self.chains)
-        assert sorted(list(dataframe.keys())) == sorted(
-            ["chain_{}".format(num) for num in range(len(self.chains))]
-        )
-        assert sorted(list(dataframe["chain_0"].keys())) == sorted(["a", "b"])
-        assert sorted(list(dataframe["chain_1"].keys())) == sorted(["a", "b"])
-        np.testing.assert_almost_equal(
-            dataframe["chain_0"]["a"], self.chains[0][0]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["chain_0"]["b"], self.chains[0][1]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["chain_1"]["a"], self.chains[1][0]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["chain_1"]["b"], self.chains[1][1]
-        )
-        other = MCMCSamplesDict({
-            "chain_{}".format(num): {
-                param: self.chains[num][idx] for idx, param in enumerate(
-                    self.parameters
-                )
-            } for num in range(len(self.chains))
-        })
-        assert sorted(other.keys()) == sorted(dataframe.keys())
-        assert sorted(other["chain_0"].keys()) == sorted(
-            dataframe["chain_0"].keys()
-        )
-        np.testing.assert_almost_equal(
-            other["chain_0"]["a"], dataframe["chain_0"]["a"]
-        )
-        np.testing.assert_almost_equal(
-            other["chain_0"]["b"], dataframe["chain_0"]["b"]
-        )
-        np.testing.assert_almost_equal(
-            other["chain_1"]["a"], dataframe["chain_1"]["a"]
-        )
-        np.testing.assert_almost_equal(
-            other["chain_1"]["b"], dataframe["chain_1"]["b"]
+        os.remove("{}/plots/test.png".format(tmpdir))
+        os.remove("{}/plots/test2.png".format(tmpdir))
+        # when files are passed without labels, assign to each label
+        self.add_argument(
+            [
+                "--add_existing_plot", "{}/test.png".format(tmpdir),
+                "{}/test2.png".format(tmpdir)
+            ]
         )
-
-    def test_unequal_chain_length(self):
-        """Test that when inverted, the chains keep their unequal chain
-        length
-        """
-        chains = [
-            [np.random.uniform(10, 0.5, 100), np.random.uniform(100, 10, 100)],
-            [np.random.uniform(5, 0.5, 200), np.random.uniform(80, 10, 200)]
-        ]
-        dataframe = MCMCSamplesDict(self.parameters, chains)
-        transpose = dataframe.T
-        assert len(transpose["a"]["chain_0"]) == 100
-        assert len(transpose["a"]["chain_1"]) == 200
-        assert dataframe.number_of_samples == {
-            "chain_0": 100, "chain_1": 200
+        assert self.inputs.existing_plot == {
+            "example": [real_path + "/plots/test.png", real_path + "/plots/test2.png"]
         }
-        assert dataframe.minimum_number_of_samples == 100
-        assert transpose.number_of_samples == dataframe.number_of_samples
-        assert transpose.minimum_number_of_samples == \
-            dataframe.minimum_number_of_samples
-        combined = dataframe.combine
-        assert combined.number_of_samples == 300
-        np.testing.assert_almost_equal(
-            np.concatenate(
-                [dataframe["chain_0"]["a"], dataframe["chain_1"]["a"]]
-            ), combined["a"]
-        )
 
-    def test_properties(self):
-        """Test that the properties of the MCMCSamplesDict class are correct
-        """
-        dataframe = MCMCSamplesDict(self.parameters, self.chains)
-        transpose = dataframe.T
-        np.testing.assert_almost_equal(
-            dataframe["chain_0"]["a"], transpose["a"]["chain_0"]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["chain_0"]["b"], transpose["b"]["chain_0"]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["chain_1"]["a"], transpose["a"]["chain_1"]
-        )
-        np.testing.assert_almost_equal(
-            dataframe["chain_1"]["b"], transpose["b"]["chain_1"]
-        )
-        average = dataframe.average
-        transpose_average = transpose.average
-        for param in self.parameters:
-            np.testing.assert_almost_equal(
-                average[param], transpose_average[param]
-            )
-        assert dataframe.total_number_of_samples == 200
-        assert dataframe.total_number_of_samples == \
-            transpose.total_number_of_samples
-        combined = dataframe.combine
-        assert combined.number_of_samples == 200
-        np.testing.assert_almost_equal(
-            np.concatenate(
-                [dataframe["chain_0"]["a"], dataframe["chain_1"]["a"]]
-            ), combined["a"]
+        # when a plot does not exist, do not add it
+        self.add_argument(
+            [
+                "--add_existing_plot", "{}/test.png".format(tmpdir),
+                "{}/test3.png".format(tmpdir)
+            ]
         )
-
-    def test_burnin_removal(self):
-        """Test that the different methods for removing the samples as burnin
-        as expected
-        """
-        uniform = np.random.uniform
-        parameters = ["a", "b", "cycle"]
-        chains = [
-            [uniform(10, 0.5, 100), uniform(100, 10, 100), uniform(1, 0.8, 100)],
-            [uniform(5, 0.5, 100), uniform(80, 10, 100), uniform(1, 0.8, 100)],
-            [uniform(1, 0.8, 100), uniform(90, 10, 100), uniform(1, 0.8, 100)]
+        assert self.inputs.existing_plot == {
+            "example": real_path + "/plots/test.png"
+        }
+        # when no plots exist, return None
+        self.add_argument(["--add_existing_plot", "does_not_exist.png"])
+        assert self.inputs.existing_plot is None
+
+    def test_preliminary_pages_for_multiple_analysis(self):
+        """Test that preliminary watermarks are added when multiple analyses
+        are not reproducible
+        """
+        with open("{}/psd.dat".format(tmpdir), "w") as f:
+            f.writelines(["1.00 3.44\n", "2.00 5.66\n", "3.00 4.56\n", "4.00 9.83\n"])
+        self.default_arguments = [
+            "--approximant", "IMRPhenomPv2", "IMRPhenomPv2",
+            "--webdir", tmpdir,
+            "--samples", "{}/bilby_example.h5".format(tmpdir),
+            "{}/bilby_example.h5".format(tmpdir),
+            "--email", "albert.einstein@ligo.org",
+            "--gracedb", "Grace", "--no_conversion",
+            "--labels", "example", "example2"]
+        self.original_arguments = copy.deepcopy(self.default_arguments)
+        self.make_input_object()
+        # when neither a psd or config is passed for each file, add preliminary
+        # watermark
+        assert self.inputs.preliminary_pages["example"] == True
+        assert self.inputs.preliminary_pages["example2"] == True
+        # When a psd and config is provided to both analyses, add preliminary
+        # watermark to both files
+        self.add_argument(["--psd", "H1:{}/psd.dat".format(tmpdir)])
+        self.add_argument(
+            ["--config", data_dir + "/config_lalinference.ini",
+             data_dir + "/config_lalinference.ini"]
+        )
+        assert self.inputs.preliminary_pages["example"] == False
+        assert self.inputs.preliminary_pages["example2"] == False
+
+    def test_calibration(self):
+        with open("{}/calibration.dat".format(tmpdir), "w") as f:
+            f.writelines(["1.0 2.0 3.0 4.0 5.0 6.0 7.0\n"])
+            f.writelines(["1.0 2.0 3.0 4.0 5.0 6.0 7.0"])
+        assert self.inputs.calibration == {"example": {None: None}}
+        self.add_argument(["--calibration", "{}/calibration.dat".format(tmpdir)])
+        assert self.inputs.calibration["example"] == {None: None}
+        assert list(self.inputs.priors["calibration"]["example"].keys()) == ['calibration.dat']
+
+    def test_custom_psd(self):
+        with open("{}/psd.dat".format(tmpdir), "w") as f:
+            f.writelines(["1.00 3.44\n", "2.00 5.66\n", "3.00 4.56\n", "4.00 9.83\n"])
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        default_arguments = [
+            "--approximant", "IMRPhenomPv2", "IMRPhenomPv2",
+            "--webdir", tmpdir,
+            "--samples", "{}/bilby_example.h5".format(tmpdir),
+            "{}/bilby_example.h5".format(tmpdir),
+            "--email", "albert.einstein@ligo.org",
+            "--gracedb", "Grace",
+            "--labels", "test", "test2",
+            "--test_psd", "L1:{}/psd.dat".format(tmpdir),
+            "--test2_psd", "V1:{}/psd.dat".format(tmpdir),
+            "--f_low", "1.0", "1.0", "--f_final",
+            "3.0", "3.0", "--gw", "--no_conversion"
         ]
-        dataframe = MCMCSamplesDict(parameters, chains)
-        burnin = dataframe.burnin(algorithm="burnin_by_step_number")
-        idxs = np.argwhere(chains[0][2] > 0)
-        assert len(burnin["chain_0"]["a"]) == len(idxs)
-        dataframe = MCMCSamplesDict(parameters, chains)
-        burnin = dataframe.burnin(10, algorithm="burnin_by_first_n")
-        assert len(burnin["chain_0"]["a"]) == 90
-        dataframe = MCMCSamplesDict(parameters, chains)
-        burnin = dataframe.burnin(
-            10, algorithm="burnin_by_first_n", step_number=True
-        )
-        assert len(burnin["chain_0"]["a"]) == len(idxs) - 10
-        
+        opts, unknown = parser.parse_known_args(default_arguments)
+        add_dynamic_PSD_to_namespace(opts, command_line=default_arguments)
+        add_dynamic_calibration_to_namespace(
+            opts, command_line=default_arguments
+        )
+        inputs = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert sorted(list(inputs.psd.keys())) == sorted(["test", "test2"])
+        assert list(inputs.psd["test"].keys()) == ["L1"]
+        assert list(inputs.psd["test2"].keys()) == ["V1"]
+        np.testing.assert_almost_equal(
+            inputs.psd["test"]["L1"],
+            [[1.00, 3.44], [2.00, 5.66], [3.00, 4.56], [4.00, 9.83]]
+        )
+
+    def test_IFO_from_file_name(self):
+        file_name = "IFO0.dat"
+        assert WebpagePlusPlottingPlusMetaFileInput.get_ifo_from_file_name(
+            file_name
+        ) == "H1"
+        file_name = "IFO1.dat"
+        assert WebpagePlusPlottingPlusMetaFileInput.get_ifo_from_file_name(
+            file_name
+        ) == "L1"
+        file_name = "IFO2.dat"
+        assert WebpagePlusPlottingPlusMetaFileInput.get_ifo_from_file_name(
+            file_name
+        ) == "V1"
+
+        file_name = "IFO_H1.dat"
+        assert WebpagePlusPlottingPlusMetaFileInput.get_ifo_from_file_name(
+            file_name
+        ) == "H1"
+        file_name = "IFO_L1.dat"
+        assert WebpagePlusPlottingPlusMetaFileInput.get_ifo_from_file_name(
+            file_name
+        ) == "L1"
+        file_name = "IFO_V1.dat"
+        assert WebpagePlusPlottingPlusMetaFileInput.get_ifo_from_file_name(
+            file_name
+        ) == "V1"
         
-
-
-class TestArray(object):
-    """Test the Array class
-    """
-    def test_properties(self):
-        samples = np.random.uniform(100, 10, 100)
-        array = Array(samples)
-        assert array.average(type="mean") == np.mean(samples)
-        assert array.average(type="median") == np.median(samples)
-        assert array.standard_deviation == np.std(samples)
-        np.testing.assert_almost_equal(
-            array.confidence_interval(percentile=[5, 95]),
-            [np.percentile(array, 5), np.percentile(array, 95)]
+        file_name = "example.dat"
+        assert WebpagePlusPlottingPlusMetaFileInput.get_ifo_from_file_name(
+            file_name
+        ) == "example.dat"
+
+    def test_ignore_parameters(self):
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        default_arguments = [
+            "--approximant", "IMRPhenomPv2",
+            "--webdir", tmpdir,
+            "--samples", "{}/bilby_example.h5".format(tmpdir),
+            "--labels", "example"]
+        opts = parser.parse_args(default_arguments)
+        original = WebpagePlusPlottingPlusMetaFileInput(opts)
+
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        default_arguments = [
+            "--approximant", "IMRPhenomPv2",
+            "--webdir", tmpdir,
+            "--samples", "{}/bilby_example.h5".format(tmpdir),
+            "--ignore_parameters", "cos*",
+            "--labels", "example"]
+        opts = parser.parse_args(default_arguments)
+        ignored = WebpagePlusPlottingPlusMetaFileInput(opts)
+        ignored_params = [
+            param for param in list(original.samples["example"].keys()) if
+            "cos" in param
+        ]
+        assert all(
+            param not in list(ignored.samples["example"].keys()) for param in
+            ignored_params
         )
+        for param, samples in ignored.samples["example"].items():
+            np.testing.assert_almost_equal(
+                samples, original.samples["example"][param]
+            )
 
-    def test_weighted_percentile(self):
-        x = np.random.normal(100, 20, 10000)
-        weights = np.array([np.random.randint(100) for _ in range(10000)])
-        array = Array(x, weights=weights)
-        numpy = np.percentile(np.repeat(x, weights), 90)
-        pesummary = array.confidence_interval(percentile=90)
-        np.testing.assert_almost_equal(numpy, pesummary, 6)
-
-
-class TestTQDM(object):
-    """Test the pesummary.utils.tqdm.tqdm class
-    """
-    def setup(self):
-        self._range = range(100)
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-
-    def teardown(self):
-        """Remove the files and directories created from this class
-        """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
-
-    def test_basic_iterator(self):
-        """Test that the core functionality of the tqdm class remains
-        """
-        for j in tqdm(self._range):
-            _ = j*j
-
-    def test_interaction_with_logger(self):
-        """Test that tqdm interacts nicely with logger
-        """
-        from pesummary.utils.utils import logger, LOG_FILE
-
-        with open("./.outdir/test.dat", "w") as f:
-            for j in tqdm(self._range, logger=logger, file=f):
-                _ = j*j
-
-        with open("./.outdir/test.dat", "r") as f:
-            lines = f.readlines()
-            assert "PESummary" in lines[-1]
-            assert "INFO" in lines[-1]
-        
-
-def test_jensen_shannon_divergence():
-    """Test that the `jensen_shannon_divergence` method returns the same
-    values as the scipy function
-    """
-    from scipy.spatial.distance import jensenshannon
-    from scipy import stats
-
-    samples = [
-        np.random.uniform(5, 4, 100),
-        np.random.uniform(5, 4, 100)
-    ]
-    x = np.linspace(np.min(samples), np.max(samples), 100)
-    kde = [stats.gaussian_kde(i)(x) for i in samples]
-    _scipy = jensenshannon(*kde)**2
-    _pesummary = utils.jensen_shannon_divergence(samples, decimal=9)
-    np.testing.assert_almost_equal(_scipy, _pesummary)
-
-    from pesummary.core.plots.bounded_1d_kde import Bounded_1d_kde
-
-    _pesummary = utils.jensen_shannon_divergence(
-        samples, decimal=9, kde=Bounded_1d_kde, xlow=4.5, xhigh=5.5
-    )
-
-
-def test_make_cache_style_file():
-    """Test that the `make_cache_style_file` works as expected
-    """
-    from pesummary.utils.utils import make_cache_style_file
-
-    sty = os.path.expanduser("~/.cache/pesummary/style/matplotlib_rcparams.sty")
-    with open("test.sty", "w") as f:
-        f.writelines(["test : 10"])
-    make_cache_style_file("test.sty")
-    assert os.path.isfile(sty)
-    with open(sty, "r") as f:
-        lines = f.readlines()
-    assert len(lines) == 1
-    assert lines[0] == "test : 10"
-
-
-def test_logger():
-    with LogCapture() as l:
-        utils.logger.info("info")
-        utils.logger.warning("warning")
-    l.check(("PESummary", "INFO", "info"),
-            ("PESummary", "WARNING", "warning"),)
-
-
-class TestDict(object):
-    """Class to test the NestedDict object
-    """
-    def test_initiate(self):
-        """Initiate the Dict class
-        """
-        from pesummary.gw.file.psd import PSD
-
-        x = Dict(
-            {"a": [[10, 20], [10, 20]]}, value_class=PSD,
-            value_columns=["value", "value2"]
-        )
-        assert list(x.keys()) == ["a"]
-        np.testing.assert_almost_equal(x["a"], [[10, 20], [10, 20]])
-        assert isinstance(x["a"], PSD)
-        np.testing.assert_almost_equal(x["a"].value, [10, 10])
-        np.testing.assert_almost_equal(x["a"].value2, [20, 20])
-
-        x = Dict(
-            ["a"], [[[10, 20], [10, 20]]], value_class=PSD,
-            value_columns=["value", "value2"]
-        )
-        assert list(x.keys()) == ["a"]
-        np.testing.assert_almost_equal(x["a"], [[10, 20], [10, 20]])
-        assert isinstance(x["a"], PSD)
-        np.testing.assert_almost_equal(x["a"].value, [10, 10])
-        np.testing.assert_almost_equal(x["a"].value2, [20, 20])
-
-
-def make_cache_style_file(style_file):
-    """Make a cache directory which stores the style file you wish to use
-    when plotting
-
-    Parameters
-    ----------
-    style_file: str
-        path to the style file that you wish to use when plotting
-    """
-    make_dir(CACHE_DIR)
-    shutil.copyfile(
-        style_file, os.path.join(CACHE_DIR, "matplotlib_rcparams.sty")
-    )
-
-
-def get_matplotlib_style_file():
-    """Return the path to the matplotlib style file that you wish to use
-    """
-    return os.path.join(CACHE_DIR, "matplotlib_rcparams.sty")
+    def test_make_directories(self):
+        assert os.path.isdir("{}/samples/samples".format(tmpdir)) == False
+        self.replace_existing_argument("--webdir", "{}/samples".format(tmpdir))
+        self.inputs.make_directories()
+        assert os.path.isdir("{}/samples/samples".format(tmpdir)) == True
+
+    def test_copy_files(self):
+        if os.path.isdir("{}/samples".format(tmpdir)):
+            shutil.rmtree("{}/samples".format(tmpdir))
+        assert os.path.isfile(
+            "{}/samples/js/combine_corner.js".format(tmpdir)
+        ) == False
+        self.replace_existing_argument("--webdir", "{}/samples".format(tmpdir))
+        self.add_argument(["--config", data_dir + "/config_lalinference.ini"])
+        self.inputs.copy_files()
+        assert os.path.isfile(
+            "{}/samples/js/combine_corner.js".format(tmpdir)
+        ) == True
+        assert os.path.isfile(
+            "{}/samples/config/example_config.ini".format(tmpdir)
+        ) == True
+
+    def test_injection_data(self):
+        assert sorted(list(self.inputs.injection_data["example"].keys())) == sorted(
+            gw_parameters())
+
+    def test_maxL_samples(self):
+        ind = self.parameters.index("log_likelihood")
+        likelihood = np.array(self.samples).T[ind]
+        max_ind = np.argmax(likelihood)
+        for param in self.parameters:
+            ind = self.parameters.index(param)
+            assert self.inputs.maxL_samples["example"][param] == self.samples.T[ind][max_ind]
+        assert self.inputs.maxL_samples["example"]["approximant"] == "IMRPhenomPv2"
+
+    def test_same_parameters(self):
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        opts = parser.parse_args(["--approximant", "IMRPhenomPv2",
+            "IMRPhenomPv2", "--webdir", tmpdir, "--samples",
+            "{}/bilby_example.h5".format(tmpdir),
+            "{}/lalinference_example.h5".format(tmpdir)])
+        inputs = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert sorted(inputs.same_parameters) == sorted(gw_parameters())
+
+    def test_psd_labels(self):
+        assert list(self.inputs.psd.keys()) == ["example"]
+        assert self.inputs.psd["example"] == {}
+        with open("{}/psd.dat".format(tmpdir), "w") as f:
+            f.writelines(["1.00 3.44\n", "2.00 5.66\n", "3.00 4.56\n", "4.00 9.83\n"])
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        opts = parser.parse_args(["--approximant", "IMRPhenomPv2",
+            "IMRPhenomPv2", "--webdir", tmpdir, "--samples",
+            "{}/bilby_example.h5".format(tmpdir),
+            "{}/lalinference_example.h5".format(tmpdir),
+            "--psd", "L1:{}/psd.dat".format(tmpdir),
+            "L1:{}/psd.dat".format(tmpdir),
+            "--labels", "example", "example2", "--f_low", "1.0", "1.0", "--f_final",
+            "3.0", "3.0", "--gw", "--no_conversion"])
+        inputs = WebpagePlusPlottingPlusMetaFileInput(opts)
+        assert sorted(list(inputs.psd["example"].keys())) == ["L1"]
+        assert sorted(list(inputs.psd["example2"].keys())) == ["L1"]
```

### Comparing `pesummary-0.9.1/pesummary/tests/write_test.py` & `pesummary-1.0.0/pesummary/tests/write_test.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,13 +1,20 @@
+# Licensed under an MIT style license -- see LICENSE.md
+
 import os
 import shutil
 import numpy as np
 import pytest
 
 from pesummary.io import write, read
+import tempfile
+
+tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 class Base(object):
     """Base class containing useful functions
     """
     def write(self, file_format, filename, **kwargs):
         """Write the samples to file
@@ -15,15 +22,15 @@
         self.parameters = ["a", "b"]
         self.samples = np.array([
             np.random.uniform(10, 5, 100),
             np.random.uniform(100, 2, 100)
         ]).T
         write(
             self.parameters, self.samples, file_format=file_format, filename=filename,
-            outdir=".outdir", **kwargs
+            outdir=tmpdir, **kwargs
         )
         return self.parameters, self.samples
 
     def check_samples(self, filename, parameters, samples, pesummary=False):
         """Check the saved posterior samples
         """
         f = read(filename)
@@ -38,130 +45,150 @@
 
 class TestWrite(Base):
     """Class to test the pesummary.io.write method
     """
     def setup(self):
         """Setup the Write class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir") 
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir) 
 
     def test_dat(self):
         """Test that the user can write to a dat file
         """
         parameters, samples = self.write("dat", "pesummary.dat")
-        self.check_samples(".outdir/pesummary.dat", parameters, samples.T)
+        self.check_samples("{}/pesummary.dat".format(tmpdir), parameters, samples.T)
 
     def test_json(self):
         """Test that the user can write to a json file
         """
         parameters, samples = self.write("json", "pesummary.json")
-        self.check_samples(".outdir/pesummary.json", parameters, samples.T)
+        self.check_samples("{}/pesummary.json".format(tmpdir), parameters, samples.T)
 
     def test_hdf5(self):
         """Test that the user can write to a hdf5 file
         """
         parameters, samples = self.write("h5", "pesummary.h5")
-        self.check_samples(".outdir/pesummary.h5", parameters, samples.T)
+        self.check_samples("{}/pesummary.h5".format(tmpdir), parameters, samples.T)
 
     def test_bilby(self):
         """Test that the user can write to a bilby file
         """
         parameters, samples = self.write("bilby", "bilby.json")
-        self.check_samples(".outdir/bilby.json", parameters, samples.T)
+        self.check_samples("{}/bilby.json".format(tmpdir), parameters, samples.T)
         parameters, samples = self.write("bilby", "bilby.h5", extension="hdf5")
-        self.check_samples(".outdir/bilby.h5", parameters, samples.T)
+        self.check_samples("{}/bilby.h5".format(tmpdir), parameters, samples.T)
 
     def test_lalinference(self):
         """Test that the user can write to a lalinference file
         """
         parameters, samples = self.write("lalinference", "lalinference.hdf5")
-        self.check_samples(".outdir/lalinference.hdf5", parameters, samples.T)
+        self.check_samples("{}/lalinference.hdf5".format(tmpdir), parameters, samples.T)
+
+    def test_sql(self):
+        """Test that the user can write to an sql database
+        """
+        parameters, samples = self.write("sql", "sql.db")
+        self.check_samples("{}/sql.db".format(tmpdir), parameters, samples.T)
+
+    def test_numpy(self):
+        """Test that the user can write to a npy file
+        """
+        parameters, samples = self.write("numpy", "numpy.npy")
+        self.check_samples("{}/numpy.npy".format(tmpdir), parameters, samples.T)
 
     def test_pesummary(self):
         """Test that the user can write to a pesummary file
         """
         parameters, samples = self.write("pesummary", "pesummary.hdf5", label="label")
         self.check_samples(
-            ".outdir/pesummary.hdf5", parameters, samples.T, pesummary=True
+            "{}/pesummary.hdf5".format(tmpdir), parameters, samples.T, pesummary=True
         )
 
 
 class TestWritePESummary(object):
     """Test the `.write` function as part of the
     `pesummary.gw.file.formats.pesummary.PESummary class
     """
     @pytest.fixture(scope='class', autouse=True)
     def setup(self):
         """Setup the TestWritePESummary class
         """
-        import os
-
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
-        os.system(
-           "curl https://dcc.ligo.org/public/0163/P190412/009/posterior_samples.h5 "
-           "-o .outdir/GW190412_posterior_samples.h5" 
+        from pesummary.core.fetch import download_dir
+        downloaded_file = os.path.join(
+            download_dir, "GW190814_posterior_samples.h5"
         )
-        type(self).result = read(".outdir/GW190412_posterior_samples.h5")
+        if not os.path.isfile(downloaded_file):
+            os.system(
+               "curl https://dcc.ligo.org/public/0168/P2000183/008/GW190814_posterior_samples.h5 "
+               "-o {}/GW190814_posterior_samples.h5".format(tmpdir)
+            )
+            downloaded_file = "{}/GW190814_posterior_samples.h5".format(tmpdir)
+
+        type(self).result = read(downloaded_file)
         type(self).posterior = type(self).result.samples_dict
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
 
     def _write(self, file_format, extension, pesummary=False, **kwargs):
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        if not os.path.isdir(tmpdir):
+            os.mkdir(tmpdir)
         filename = {
-            "IMRPhenomHM": "test.{}".format(extension),
-            "IMRPhenomPv3HM": "test2.{}".format(extension)
+            "C01:IMRPhenomHM": "test.{}".format(extension),
+            "C01:IMRPhenomPv3HM": "test2.{}".format(extension)
         }
         self.result.write(
-            labels=["IMRPhenomHM", "IMRPhenomPv3HM"], file_format=file_format,
-            filenames=filename, outdir=".outdir", **kwargs
+            labels=["C01:IMRPhenomHM", "C01:IMRPhenomPv3HM"], file_format=file_format,
+            filenames=filename, outdir=tmpdir, **kwargs
         )
         if not pesummary:
-            assert os.path.isfile(".outdir/test.{}".format(extension))
-            assert os.path.isfile(".outdir/test2.{}".format(extension))
-            one = read(".outdir/test.{}".format(extension))
-            two = read(".outdir/test2.{}".format(extension))
+            assert os.path.isfile("{}/test.{}".format(tmpdir, extension))
+            assert os.path.isfile("{}/test2.{}".format(tmpdir, extension))
+            one = read("{}/test.{}".format(tmpdir, extension))
+            two = read("{}/test2.{}".format(tmpdir, extension))
             np.testing.assert_almost_equal(
-                one.samples_dict["mass_1"], self.posterior["IMRPhenomHM"]["mass_1"]
+                one.samples_dict["mass_1"], self.posterior["C01:IMRPhenomHM"]["mass_1"]
             )
             np.testing.assert_almost_equal(
-                two.samples_dict["mass_1"], self.posterior["IMRPhenomPv3HM"]["mass_1"]
+                two.samples_dict["mass_1"], self.posterior["C01:IMRPhenomPv3HM"]["mass_1"]
             )
-            os.system("rm .outdir/test.{}".format(extension))
-            os.system("rm .outdir/test2.{}".format(extension))
+            os.system("rm {}/test.{}".format(tmpdir, extension))
+            os.system("rm {}/test2.{}".format(tmpdir, extension))
         else:
-            assert os.path.isfile(".outdir/test.h5")
-            one = read(".outdir/test.h5")
-            assert sorted(one.labels) == sorted(["IMRPhenomHM"])
+            assert os.path.isfile("{}/test.h5".format(tmpdir))
+            one = read("{}/test.h5".format(tmpdir))
+            assert sorted(one.labels) == sorted(["C01:IMRPhenomHM"])
             np.testing.assert_almost_equal(
-                one.samples_dict["IMRPhenomHM"]["mass_1"],
-                self.posterior["IMRPhenomHM"]["mass_1"]
+                one.samples_dict["C01:IMRPhenomHM"]["mass_1"],
+                self.posterior["C01:IMRPhenomHM"]["mass_1"]
             )
             np.testing.assert_almost_equal(
-                one.psd["IMRPhenomHM"]["H1"], self.result.psd["IMRPhenomHM"]["H1"]
+                one.psd["C01:IMRPhenomHM"]["H1"], self.result.psd["C01:IMRPhenomHM"]["H1"]
             )
 
     def test_write_dat(self):
         """Test write to dat
         """
         self._write("dat", "dat")
 
+    def test_write_numpy(self):
+        """Test write to numpy
+        """
+        self._write("numpy", "npy")
+
     def test_write_json(self):
         """Test write to dat
         """
         self._write("json", "json")
 
     def test_write_hdf5(self):
         """Test write to dat
```

### Comparing `pesummary-0.9.1/pesummary/tests/main_injection.xml` & `pesummary-1.0.0/pesummary/tests/main_injection.xml`

 * *Files 2% similar despite different names*

#### Comparing `pesummary-0.9.1/pesummary/tests/main_injection.xml` & `pesummary-1.0.0/pesummary/tests/main_injection.xml`

```diff
@@ -71,15 +71,14 @@
     <Column Type="lstring" Name="sim_inspiral:source"/>
     <Column Type="real_4" Name="sim_inspiral:latitude"/>
     <Column Type="lstring" Name="sim_inspiral:numrel_data"/>
     <Column Type="int_4s" Name="sim_inspiral:geocent_end_time"/>
     <Column Type="real_4" Name="sim_inspiral:spin2x"/>
     <Column Type="real_4" Name="sim_inspiral:spin2y"/>
     <Column Type="real_4" Name="sim_inspiral:spin2z"/>
-    <Column Type="ilwd:char" Name="sim_inspiral:process_id"/>
     <Column Type="int_4s" Name="sim_inspiral:h_end_time"/>
     <Column Type="real_4" Name="sim_inspiral:distance"/>
     <Column Type="int_4s" Name="sim_inspiral:t_end_time"/>
     <Column Type="lstring" Name="sim_inspiral:taper"/>
     <Column Type="real_4" Name="sim_inspiral:longitude"/>
     <Column Type="int_4s" Name="sim_inspiral:v_end_time_ns"/>
     <Column Type="int_4s" Name="sim_inspiral:bandpass"/>
@@ -100,29 +99,29 @@
     <Column Type="real_4" Name="sim_inspiral:alpha4"/>
     <Column Type="real_4" Name="sim_inspiral:alpha5"/>
     <Column Type="int_4s" Name="sim_inspiral:l_end_time"/>
     <Column Type="real_4" Name="sim_inspiral:polarization"/>
     <Column Type="lstring" Name="sim_inspiral:waveform"/>
     <Column Type="real_4" Name="sim_inspiral:phi0"/>
     <Column Type="real_4" Name="sim_inspiral:inclination"/>
-    <Column Type="ilwd:char" Name="sim_inspiral:simulation_id"/>
+    <Column Type="int_8s" Name="sim_inspiral:simulation_id"/>
     <Column Type="real_4" Name="sim_inspiral:f_lower"/>
     <Column Type="int_4s" Name="sim_inspiral:g_end_time_ns"/>
     <Column Type="real_4" Name="sim_inspiral:eff_dist_v"/>
     <Column Type="real_4" Name="sim_inspiral:beta"/>
     <Column Type="int_4s" Name="sim_inspiral:g_end_time"/>
     <Column Type="real_4" Name="sim_inspiral:alpha"/>
     <Column Type="real_4" Name="sim_inspiral:f_final"/>
     <Column Type="real_4" Name="sim_inspiral:mass1"/>
     <Column Type="real_4" Name="sim_inspiral:mass2"/>
     <Column Type="int_4s" Name="sim_inspiral:v_end_time"/>
     <Column Type="real_4" Name="sim_inspiral:eta"/>
     <Column Type="real_4" Name="sim_inspiral:psi0"/>
     <Column Type="real_4" Name="sim_inspiral:psi3"/>
-    <Stream Delimiter="," Type="Local" Name="sim_inspiral:table">0,0,0,0,0,32.446098,0,0,&quot;&quot;,1.949725,&quot;&quot;,1186741861,0,0,0,&quot;sim_inspiral:process_id:0&quot;,1186741860,139.76429,0,&quot;TAPER_STARTEND&quot;,-1.261573,0,0,509.97357,441.14846,0,0,0,0,0,987277507,0,988571405,0,0,0,0,0,0,1186741860,1.75,&quot;IMRPhenomDpseudoFourPN&quot;,0,1.0471976,&quot;sim_inspiral:simulation_id:0&quot;,26.620552,0,0,0,0,0,0,53.333333,26.666667,0,0.22222222,0,0</Stream>
+    <Stream Delimiter="," Type="Local" Name="sim_inspiral:table">0,0,0,0,0,32.446098,0,0,&quot;&quot;,1.949725,&quot;&quot;,1186741861,0,0,0,1186741860,139.76429,0,&quot;TAPER_STARTEND&quot;,-1.261573,0,0,509.97357,441.14846,0,0,0,0,0,987277507,0,988571405,0,0,0,0,0,0,1186741860,1.75,&quot;IMRPhenomDpseudoFourPN&quot;,0,1.0471976,0,26.620552,0,0,0,0,0,0,53.333333,26.666667,0,0.22222222,0,0</Stream>
   </Table>
   <Table Name="sngl_inspiral:table">
     <Column Type="real_4" Name="sngl_inspiral:cont_chisq"/>
     <Column Type="real_4" Name="sngl_inspiral:bank_chisq"/>
     <Column Type="int_4s" Name="sngl_inspiral:chisq_dof"/>
     <Column Type="real_8" Name="sngl_inspiral:end_time_gmst"/>
     <Column Type="real_8" Name="sngl_inspiral:event_duration"/>
@@ -154,15 +153,14 @@
     <Column Type="real_4" Name="sngl_inspiral:mtotal"/>
     <Column Type="real_4" Name="sngl_inspiral:alpha3"/>
     <Column Type="real_4" Name="sngl_inspiral:spin1z"/>
     <Column Type="real_4" Name="sngl_inspiral:Gamma5"/>
     <Column Type="real_4" Name="sngl_inspiral:spin2x"/>
     <Column Type="real_4" Name="sngl_inspiral:f_final"/>
     <Column Type="real_4" Name="sngl_inspiral:beta"/>
-    <Column Type="ilwd:char" Name="sngl_inspiral:process_id"/>
     <Column Type="real_4" Name="sngl_inspiral:snr"/>
     <Column Type="int_4s" Name="sngl_inspiral:bank_chisq_dof"/>
     <Column Type="real_4" Name="sngl_inspiral:kappa"/>
     <Column Type="real_4" Name="sngl_inspiral:eff_distance"/>
     <Column Type="real_4" Name="sngl_inspiral:Gamma7"/>
     <Column Type="real_4" Name="sngl_inspiral:Gamma6"/>
     <Column Type="lstring" Name="sngl_inspiral:search"/>
@@ -181,10 +179,10 @@
     <Column Type="real_4" Name="sngl_inspiral:psi0"/>
     <Column Type="int_4s" Name="sngl_inspiral:end_time"/>
     <Column Type="real_4" Name="sngl_inspiral:amplitude"/>
     <Column Type="real_4" Name="sngl_inspiral:psi3"/>
     <Column Type="int_4s" Name="sngl_inspiral:end_time_ns"/>
     <Column Type="lstring" Name="sngl_inspiral:ifo"/>
     <Column Type="real_8" Name="sngl_inspiral:sigmasq"/>
-    <Stream Delimiter="," Type="Local" Name="sngl_inspiral:table">0,0,0,0,0,0,0,0,0,0,0,32.446098,0,0,0,0,&quot;sngl_inspiral:event_id:0&quot;,0,0,0,0,0,0,0,0,0,0,0,0,&quot;&quot;,80,0,0,0,0,0,0,&quot;sim_inspiral:process_id:0&quot;,0,0,0,0,0,0,&quot;&quot;,0,53.333333,0,0,26.666667,0,0,0,0,0,0,0.22222222,0,1186741860,0,0,988571405,&quot;&quot;,0</Stream>
+    <Stream Delimiter="," Type="Local" Name="sngl_inspiral:table">0,0,0,0,0,0,0,0,0,0,0,32.446098,0,0,0,0,&quot;sngl_inspiral:event_id:0&quot;,0,0,0,0,0,0,0,0,0,0,0,0,&quot;&quot;,80,0,0,0,0,0,0,0,0,0,0,0,0,&quot;&quot;,0,53.333333,0,0,26.666667,0,0,0,0,0,0,0.22222222,0,1186741860,0,0,988571405,&quot;&quot;,0</Stream>
   </Table>
 </LIGO_LW>
```

### Comparing `pesummary-0.9.1/pesummary/tests/existing_file.py` & `pesummary-1.0.0/pesummary/tests/existing_file.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,10 +1,14 @@
+# Licensed under an MIT style license -- see LICENSE.md
+
 import argparse
 import pesummary
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 def command_line():
     """Generate an Argument Parser object to control the command line options
     """
     parser = argparse.ArgumentParser(description=__doc__)
     parser.add_argument("-f", "--file", help="Result file you wish to test")
     parser.add_argument(
```

### Comparing `pesummary-0.9.1/pesummary/tests/files/config_lalinference.ini` & `pesummary-1.0.0/pesummary/tests/files/config_lalinference.ini`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/tests/files/psd_file.txt` & `pesummary-1.0.0/pesummary/tests/files/psd_file.txt`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/tests/files/calibration_envelope.txt` & `pesummary-1.0.0/pesummary/tests/files/calibration_envelope.txt`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/tests/cosmology_test.py` & `pesummary-1.0.0/pesummary/tests/cosmology_test.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,10 +1,14 @@
+# Licensed under an MIT style license -- see LICENSE.md
+
 from pesummary.gw.cosmology import get_cosmology, available_cosmologies
 import pytest
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class TestCosmology(object):
     """Test the get_cosmology function as part of the `pesummary.gw.cosmology`
     package
     """
     def test_invalid_string(self):
         """Test that a ValueError is passed when an invalid string is passed
@@ -23,27 +27,50 @@
             else:
                 assert lal_values[param] == getattr(cosmology, param)
 
     def test_astropy_cosmology(self):
         """Test that the astropy cosmology is correct
         """
         from astropy import cosmology
+        from astropy.cosmology import parameters
 
-        for cosmo in cosmology.parameters.available:
+        for cosmo in parameters.available:
             _cosmo = get_cosmology(cosmology=cosmo)
             astropy_cosmology = getattr(cosmology, cosmo)
             for key, value in vars(_cosmo).items():
-                assert vars(astropy_cosmology)[key] == value
+                try:
+                    assert vars(astropy_cosmology)[key] == value
+                except ValueError:
+                    assert all(
+                        vars(astropy_cosmology)[key][_] == value[_] for _
+                        in range(len(vars(astropy_cosmology)[key]))
+                    )
 
     def test_Riess2019_H0(self):
         """Test that the Riess2019 H0 cosmology is correct
         """
         riess_H0 = 74.03
         for cosmo in available_cosmologies:
             if "riess" not in cosmo:
                 continue
             _cosmo = get_cosmology(cosmology=cosmo)
             base_cosmo = cosmo.split("_with_riess2019_h0")[0]
             _base_cosmo = get_cosmology(cosmology=base_cosmo)
             assert _cosmo.H0.value == riess_H0
             for key in ["Om0", "Ode0"]:
                 assert getattr(_base_cosmo, key) == getattr(_cosmo, key)
+
+    def test_upper_lower_case(self):
+        """Test that `get_cosmology` works with random upper and lower cases
+        """
+        from astropy import cosmology
+        for cosmo in ["Planck18", "PLANCK18", "planck18", "PlAnCk18"]:
+            _cosmo = get_cosmology(cosmology=cosmo)
+            astropy_cosmology = cosmology.Planck18
+            for key, value in vars(_cosmo).items():
+                try:
+                    assert vars(astropy_cosmology)[key] == value
+                except ValueError:
+                    assert all(
+                        vars(astropy_cosmology)[key][_] == value[_] for _
+                        in range(len(vars(astropy_cosmology)[key]))
+                    )
```

### Comparing `pesummary-0.9.1/pesummary/tests/imports.sh` & `pesummary-1.0.0/pesummary/tests/imports.sh`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/tests/plot_test.py` & `pesummary-1.0.0/pesummary/tests/plot_test.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,44 +1,36 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
 import shutil
 
 import argparse
 
 from pesummary.core.plots import plot
 from pesummary.gw.plots import plot as gwplot
-from pesummary.utils.samples_dict import Array
+from pesummary.utils.array import Array
 from subprocess import CalledProcessError
 
 import numpy as np
 import matplotlib
 from matplotlib import rcParams
 import pytest
+import tempfile
+
+tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 rcParams["text.usetex"] = False
 
 class TestPlot(object):
 
     def setup(self):
-        if os.path.isdir("./.outdir"):
-            shutil.rmtree("./.outdir")
-        os.makedirs("./.outdir")
+        if os.path.isdir(tmpdir):
+            shutil.rmtree(tmpdir)
+        os.makedirs(tmpdir)
 
     def _grab_frequencies_from_psd_data_file(self, file):
         """Return the frequencies stored in the psd data files
 
         Parameters
         ----------
         file: str
@@ -58,18 +50,17 @@
             path to the psd data file
         """
         fil = open(file)
         fil = fil.readlines()
         fil = [i.strip().split() for i in fil]
         return [float(i[1]) for i in fil]
 
-    @pytest.mark.parametrize("param, samples", [("mass_1",
-        Array([10, 20, 30, 40])),])
-    def test_autocorrelation_plot(self, param, samples):
-        fig = plot._autocorrelation_plot(param, samples)
+    def test_autocorrelation_plot(self):
+        rcParams["text.usetex"] = False
+        fig = plot._autocorrelation_plot("mass_1", Array([10, 20, 30, 40]))
         assert isinstance(fig, matplotlib.figure.Figure) == True
 
     @pytest.mark.parametrize("param, samples", [("mass_1",
         [Array([10, 20, 30, 40]), Array([10, 20, 30, 40])]), ])
     def test_autocorrelation_plot_mcmc(self, param, samples):
         fig = plot._autocorrelation_plot_mcmc(param, samples)
         assert isinstance(fig, matplotlib.figure.Figure) == True
@@ -290,20 +281,24 @@
                        "phi_12": 0., "a_1": 0.5, "a_2": 0., "phase": 0.,
                        "ra": 1., "dec": 1., "psi": 0., "geocent_time": 0.,
                        "luminosity_distance": 100}
         fig = gwplot._sky_sensitivity(["H1", "L1"], 1.0, maxL_params)
         assert isinstance(fig, matplotlib.figure.Figure) == True
 
     def test_psd_plot(self):
-        with open("./.outdir/psd.dat", "w") as f:
+        with open("{}/psd.dat".format(tmpdir), "w") as f:
             f.writelines(["0.5 100"])
             f.writelines(["1.0 150"])
             f.writelines(["5.0 200"])
-        frequencies = [self._grab_frequencies_from_psd_data_file("./.outdir/psd.dat")]
-        strains = [self._grab_frequencies_from_psd_data_file("./.outdir/psd.dat")]
+        frequencies = [
+            self._grab_frequencies_from_psd_data_file("{}/psd.dat".format(tmpdir))
+        ]
+        strains = [
+            self._grab_frequencies_from_psd_data_file("{}/psd.dat".format(tmpdir))
+        ]
         fig = gwplot._psd_plot(frequencies, strains, labels=["H1"])
         assert isinstance(fig, matplotlib.figure.Figure) == True
 
     def test_calibration_plot(self):
         frequencies = np.arange(20, 100, 0.2)
         ifos = ["H1"]
         calibration = [[
@@ -360,27 +355,27 @@
 class TestPublication(object):
     """Class to test the `pesummary.gw.plots.publication` module
     """
     def test_twod_contour_plots(self):
         from pesummary.gw.plots.publication import twod_contour_plots
 
         parameters = ["a", "b"]
-        samples = [[
+        samples = [np.array([
             np.random.uniform(0., 3000, 1000),
             np.random.uniform(0., 3000, 1000)
-        ]]
+        ])]
         labels = ["a", "b"]
         fig = twod_contour_plots(
             parameters, samples, labels, {"a": "a", "b": "b"}
         )
         assert isinstance(fig, matplotlib.figure.Figure)
 
     def test_violin(self):
         from pesummary.gw.plots.publication import violin_plots
-        from pesummary.gw.plots.violin import split_dataframe
+        from pesummary.core.plots.seaborn.violin import split_dataframe
 
         parameter = "a"
         samples = [
             np.random.uniform(0., 3000, 1000),
             np.random.uniform(0., 3000, 1000)
         ]
         labels = ["a", "b"]
```

### Comparing `pesummary-0.9.1/pesummary/tests/base.py` & `pesummary-1.0.0/pesummary/tests/base.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,13 +1,18 @@
+# Licensed under an MIT style license -- see LICENSE.md
+
+import os
 import numpy as np
 from pathlib import Path
-from pesummary.core.command_line import command_line
-from pesummary.gw.command_line import insert_gwspecific_option_group
-from pesummary.gw.inputs import GWInput
-from pesummary.core.inputs import Input
+from pesummary.gw.cli.inputs import WebpagePlusPlottingPlusMetaFileInput
+from pesummary.core.cli.inputs import (
+    WebpagePlusPlottingPlusMetaFileInput as Input
+)
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 class Namespace(object):
     def __init__(self, **kwargs):
         self.__dict__.update(kwargs)
 
 
@@ -33,129 +38,161 @@
         'chirp_mass', 'symmetric_mass_ratio', 'iota', 'spin_1x', 'spin_1y',
         'spin_1z', 'spin_2x', 'spin_2y', 'spin_2z', 'chi_p', 'chi_eff',
         'cos_tilt_1', 'cos_tilt_2', 'redshift', 'comoving_distance',
         'mass_1_source', 'mass_2_source', 'total_mass_source',
         'chirp_mass_source', 'phi_1', 'phi_2', 'cos_theta_jn', 'cos_iota',
         'peak_luminosity_non_evolved', 'final_spin_non_evolved',
         'final_mass_non_evolved', 'final_mass_source_non_evolved',
-        'radiated_energy_non_evolved', 'inverted_mass_ratio'
+        'radiated_energy_non_evolved', 'inverted_mass_ratio',
+        'viewing_angle', 'chi_p_2spin'
     ]
     return parameters
 
 
-def get_list_of_files(gw=False, number=1):
+def get_list_of_files(
+    gw=False, number=1, existing_plot=False, parameters=[], sections=[],
+    outdir=".outdir", extra_gw_pages=True
+):
     """Return a list of files that should be generated from a typical workflow
     """
-    if not gw:
+    if not gw and not len(parameters):
         import string
-
         parameters = list(string.ascii_lowercase)[:17] + ["log_likelihood"]
+    elif not len(parameters):
+        parameters = gw_parameters()
+    if not gw:
         label = "core"
     else:
-        parameters = gw_parameters()
         label = "gw"
     html = [
-        "./.outdir/html/error.html",
-        "./.outdir/html/Version.html",
-        "./.outdir/html/Logging.html",
-        "./.outdir/html/About.html",
-        "./.outdir/html/Downloads.html"]
+        "%s/html/error.html" % (outdir),
+        "%s/html/Version.html" % (outdir),
+        "%s/html/Logging.html" % (outdir),
+        "%s/html/About.html" % (outdir),
+        "%s/html/Downloads.html" % (outdir)]
+    if gw and not len(sections):
+        sections = [
+            "spins", "spin_angles", "timings", "source", "remnant", "others",
+            "masses", "location", "inclination", "energy"
+        ]
+    elif not len(sections):
+        sections = ["A-D", "E-F", "I-L", "M-P", "Q-T"]
     for num in range(number):
-        html.append("./.outdir/html/%s%s_%s%s.html" % (label, num, label, num))
-        if gw:
-            html.append("./.outdir/html/%s%s_%s%s_Classification.html" % (label, num, label, num))
-        html.append("./.outdir/html/%s%s_%s%s_Corner.html" % (label, num, label, num))
-        html.append("./.outdir/html/%s%s_%s%s_Config.html" % (label, num, label, num))
-        html.append("./.outdir/html/%s%s_%s%s_Custom.html" % (label, num, label, num))
-        html.append("./.outdir/html/%s%s_%s%s_All.html" % (label, num, label, num))
-        html.append("./.outdir/html/%s%s_%s%s_Interactive_Corner.html" % (
-            label, num, label, num
+        html.append("%s/html/%s%s_%s%s.html" % (outdir, label, num, label, num))
+        if gw and extra_gw_pages:
+            html.append("%s/html/%s%s_%s%s_Classification.html" % (outdir, label, num, label, num))
+        html.append("%s/html/%s%s_%s%s_Corner.html" % (outdir, label, num, label, num))
+        html.append("%s/html/%s%s_%s%s_Config.html" % (outdir, label, num, label, num))
+        html.append("%s/html/%s%s_%s%s_Custom.html" % (outdir, label, num, label, num))
+        html.append("%s/html/%s%s_%s%s_All.html" % (outdir, label, num, label, num))
+        html.append("%s/html/%s%s_%s%s_Interactive_Corner.html" % (
+            outdir, label, num, label, num
         ))
+        for section in sections:
+            html.append("%s/html/%s%s_%s%s_%s_all.html" % (outdir, label, num, label, num, section))
         for j in parameters:
-            html.append("./.outdir/html/%s%s_%s%s_%s.html" % (label, num, label, num, j))
+            html.append("%s/html/%s%s_%s%s_%s.html" % (outdir, label, num, label, num, j))
+        if existing_plot:
+            html.append("%s/html/%s%s_%s%s_Additional.html" % (outdir, label, num, label, num))
 
     if number > 1:
-        html.append("./.outdir/html/Comparison.html")
-        html.append("./.outdir/html/Comparison_Custom.html")
-        html.append("./.outdir/html/Comparison_All.html")
-        html.append("./.outdir/html/Comparison_Interactive_Ridgeline.html")
+        html.append("%s/html/Comparison.html" % (outdir))
+        html.append("%s/html/Comparison_Custom.html" % (outdir))
+        html.append("%s/html/Comparison_All.html" % (outdir))
+        html.append("%s/html/Comparison_Interactive_Ridgeline.html" % (outdir))
         for j in parameters:
             if j != "classification":
-                html.append("./.outdir/html/Comparison_%s.html" % (j))
+                html.append("%s/html/Comparison_%s.html" % (outdir, j))
+        for section in sections:
+            html.append("%s/html/Comparison_%s_all.html" % (outdir, section))
     return sorted(html)
 
 
 def get_list_of_plots(
     gw=False, number=1, mcmc=False, label=None, outdir=".outdir",
-    comparison=True, psd=False, calibration=False
+    comparison=True, psd=False, calibration=False, existing_plot=False,
+    expert=False, parameters=[], extra_gw_plots=True
 ):
     """Return a list of plots that should be generated from a typical workflow
     """
-    if not gw:
+    if not gw and not len(parameters):
         import string
 
         parameters = list(string.ascii_lowercase)[:17] + ["log_likelihood"]
-        if label is None:
-            label = "core"
-    else:
+    elif not len(parameters):
         parameters = gw_parameters()
-        if label is None:
-            label = "gw"
+    if not gw and label is None:
+        label = "core"
+    elif label is None:
+        label = "gw"
 
     plots = []
     for num in range(number):
         for i in ["sample_evolution", "autocorrelation", "1d_posterior", "cdf"]:
             for j in parameters:
-                plots.append("./%s/plots/%s%s_%s_%s.png" % (outdir, label, num, i, j))
+                plots.append("%s/plots/%s%s_%s_%s.png" % (outdir, label, num, i, j))
         if mcmc:
             for j in parameters:
-                plots.append("./%s/plots/%s%s_1d_posterior_%s_combined.png" % (outdir, label, num, j))
+                plots.append("%s/plots/%s%s_1d_posterior_%s_combined.png" % (outdir, label, num, j))
         if psd:
-            plots.append("./%s/plots/%s%s_psd_plot.png" % (outdir, label, num))
+            plots.append("%s/plots/%s%s_psd_plot.png" % (outdir, label, num))
         if calibration:
-            plots.append("./%s/plots/%s%s_calibration_plot.png" % (outdir, label, num))
+            plots.append("%s/plots/%s%s_calibration_plot.png" % (outdir, label, num))
+        if existing_plot:
+            plots.append("%s/plots/test.png" % (outdir))
+        if expert:
+            for j in parameters:
+                if j != "log_likelihood":
+                    plots.append("%s/plots/%s%s_2d_contour_%s_log_likelihood.png" % (outdir, label, num, j))
+                plots.append("%s/plots/%s%s_1d_posterior_%s_bootstrap.png" % (outdir, label, num, j))
+                plots.append("%s/plots/%s%s_sample_evolution_%s_log_likelihood_colored.png" % (outdir, label, num, j))
     if number > 1 and comparison:
         for i in ["1d_posterior", "boxplot", "cdf"]:
             for j in parameters:
-                plots.append("./%s/plots/combined_%s_%s.png" % (outdir, i, j))
+                plots.append("%s/plots/combined_%s_%s.png" % (outdir, i, j))
 
-    if gw:
+    if gw and extra_gw_plots:
         for num in range(number):
-            plots.append("./%s/plots/%s%s_skymap.png" % (outdir, label, num))
-            plots.append("./%s/plots/%s%s_default_pepredicates.png" % (outdir, label, num))
-            plots.append("./%s/plots/%s%s_default_pepredicates_bar.png" % (outdir, label, num))
-            plots.append("./%s/plots/%s%s_population_pepredicates.png" % (outdir, label, num))
-            plots.append("./%s/plots/%s%s_population_pepredicates_bar.png" % (outdir, label, num))
+            plots.append("%s/plots/%s%s_skymap.png" % (outdir, label, num))
+            plots.append("%s/plots/%s%s_default_pepredicates.png" % (outdir, label, num))
+            plots.append("%s/plots/%s%s_default_pepredicates_bar.png" % (outdir, label, num))
+            plots.append("%s/plots/%s%s_population_pepredicates.png" % (outdir, label, num))
+            plots.append("%s/plots/%s%s_population_pepredicates_bar.png" % (outdir, label, num))
         if number > 1 and comparison:
-            plots.append("./%s/plots/combined_skymap.png" % (outdir))
+            plots.append("%s/plots/combined_skymap.png" % (outdir))
         
     return sorted(plots)
 
 
 def make_argparse(gw=True, extension="json", bilby=False, lalinference=False,
-                  number=1, existing=False):
+                  number=1, existing=False, disable_expert=True, outdir="./.outdir"):
     """
     """
-    parser = command_line()
     default_args = []
     if gw:
-        insert_gwspecific_option_group(parser)
+        from pesummary.gw.cli.parser import ArgumentParser
         default_args.append("--gw")
         default_args.append("--nsamples_for_skymap")
         default_args.append("10")
+    else:
+        from pesummary.core.cli.parser import ArgumentParser
+    parser = ArgumentParser()
+    parser.add_all_known_options_to_parser()
     params, data = make_result_file(
-        extension=extension, gw=gw, bilby=bilby, lalinference=lalinference)
+        extension=extension, gw=gw, bilby=bilby, lalinference=lalinference,
+        outdir=outdir
+    )
     if not existing:
         default_args.append("--webdir")
     else:
         default_args.append("--existing_webdir")
-    default_args.append(".outdir")
+    default_args.append(outdir)
     default_args.append("--samples")
     for i in range(number):
-        default_args.append("./.outdir/test.%s" % (extension))
+        default_args.append("%s/test.%s" % (outdir, extension))
     default_args.append("--labels")
     if not existing:
         for i in range(number):
             if not gw:
                 default_args.append("core%s" % (i))
             else:
                 default_args.append("gw%s" % (i))
@@ -163,17 +200,19 @@
         if not gw:
             default_args.append("core1")
         else:
             default_args.append("gw1")
     default_args.append("--config")
     for i in range(number):
         default_args.append(testing_dir + "/example_config.ini")
+    if disable_expert:
+        default_args.append("--disable_expert")
     opts = parser.parse_args(default_args)
     if gw:
-        func = GWInput
+        func = WebpagePlusPlottingPlusMetaFileInput
     else:
         func = Input
     return opts, func(opts)
 
 
 def read_result_file(outdir="./.outdir", extension="json", bilby=False,
                      lalinference=False, pesummary=False):
@@ -248,15 +287,14 @@
 
 def make_injection_file(
     outdir="./.outdir", extension="json", return_filename=True,
     return_injection_dict=True
 ):
     """
     """
-    import os
     from pesummary.io import write
 
     filename = os.path.join(outdir, "injection.{}".format(extension))
     parameters = gw_parameters()
     samples = np.array([[np.random.random()] for i in range(len(parameters))]).T
     write(parameters, samples, filename=filename, file_format=extension)
     args = []
@@ -265,48 +303,63 @@
     if return_injection_dict:
         args.append({param: samples[0][num] for num, param in enumerate(parameters)})
     return args
 
 
 def make_result_file(outdir="./.outdir/", extension="json", gw=True, bilby=False,
                      lalinference=False, pesummary=False, pesummary_label="label",
-                     config=None, psd=None, calibration=None):
+                     config=None, psd=None, calibration=None, random_seed=None,
+                     n_samples=1000):
     """Make a result file that can be read in by PESummary
 
     Parameters
     ----------
     outdir: str
         directory where you would like to store the result file
     extension: str
         the file extension of the result file
     gw: Bool
         if True, gw parameters will be used
     """
+    if random_seed is not None:
+        np.random.seed(random_seed)
     print(extension, gw, bilby, lalinference, pesummary)
-    data = np.array([np.random.random(18) for i in range(1000)])
+    if outdir[-1] != "/":
+        outdir += "/"
+    data = np.array([np.random.random(18) for i in range(n_samples)])
     if gw:
         parameters = ["mass_1", "mass_2", "a_1", "a_2", "tilt_1", "tilt_2",
                       "phi_jl", "phi_12", "psi", "theta_jn", "ra", "dec",
                       "luminosity_distance", "geocent_time", "redshift",
                       "mass_1_source", "mass_2_source", "log_likelihood"]
-        distance = np.random.random(1000) * 500
+        distance = np.random.random(n_samples) * 500
+        mass_1 = np.random.random(n_samples) * 100
+        q = np.random.random(n_samples) * 100
+        a_1 = np.random.uniform(0, 0.99, n_samples)
+        a_2 = np.random.uniform(0, 0.99, n_samples)
         for num, i in enumerate(data):
             data[num][12] = distance[num]
-        mass_1 = np.random.random(1000) * 100
-        for num, i in enumerate(data):
             data[num][0] = mass_1[num]
-        for num, i in enumerate(data):
-            data[num][1] = mass_1[num]
+            data[num][1] = mass_1[num] * q[num]
+            data[num][2] = a_1[num]
+            data[num][3] = a_2[num]
     else:
         import string
 
         parameters = list(string.ascii_lowercase)[:17] + ["log_likelihood"]
     if extension == "dat":
-            np.savetxt(outdir + "test.dat", data, delimiter=" ",
-                       header=" ".join(parameters), comments="")
+        np.savetxt(outdir + "test.dat", data, delimiter=" ",
+                   header=" ".join(parameters), comments="")
+    elif extension == "csv":
+        np.savetxt(outdir + "test.csv", data, delimiter=",",
+                   header=",".join(parameters), comments="")
+    elif extension == "npy":
+        from pesummary.utils.samples_dict import SamplesDict
+        samples = SamplesDict(parameters, np.array(data).T).to_structured_array()
+        np.save(outdir + "test.npy", samples)
     elif extension == "json" and not bilby and not pesummary and not lalinference:
         import json
 
         dictionary = {"NameOfCode": {"posterior_samples": {key:
                       [i[num] for i in data] for num, key in
                       enumerate(parameters)}}}
         with open(outdir + "test.json", "w") as f:
@@ -330,17 +383,17 @@
         priors.update({"%s" % (i): bilby.core.prior.Uniform(0.1, 0.5, 0) for i in parameters})
         posterior_data_frame = DataFrame(data, columns=parameters)
         injection_parameters = {par: 1. for par in parameters}
         bilby_object = Result(
             search_parameter_keys=parameters, samples=data,
             posterior=posterior_data_frame, label="test",
             injection_parameters=injection_parameters,
-            version="bilby=0.5.3:", priors=priors,
+            priors=priors,
             log_bayes_factor=0.5, log_evidence_err=0.1, log_noise_evidence=0.1,
-            log_evidence=0.2,
+            log_evidence=0.2, version=["bilby=0.5.3:"],
             meta_data={"likelihood": {"time_marginalization": "True"}})
         if extension == "json":
             bilby_object.save_to_file(
                 filename=outdir + "test.json", extension="json")
         elif extension == "hdf5" or extension == "h5":
             bilby_object.save_to_file(
                 filename=outdir + "test.h5", extension="hdf5")
```

### Comparing `pesummary-0.9.1/pesummary/tests/workflow_test.py` & `pesummary-1.0.0/pesummary/tests/workflow_test.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,507 +1,544 @@
+# Licensed under an MIT style license -- see LICENSE.md
+
 import os
 import shutil
 import glob
+import pytest
 import numpy as np
 
 from .base import make_argparse, get_list_of_plots, get_list_of_files
 from .base import read_result_file
 from pesummary.utils.utils import functions
 from pesummary.cli.summarypages import WebpageGeneration
 from pesummary.cli.summaryplots import PlotGeneration
+import tempfile
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 class Base(object):
     """Base class to test the full workflow
     """
+    @pytest.mark.workflowtest
     def test_single_run(self, extension, bilby=False):
         """Test the full workflow for a single result file case
 
         Parameters
         ----------
         result_file: str
             path to result file you wish to run with
         """
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=False, extension=extension, bilby=bilby)
         func = functions(opts)
         PlotGeneration(inputs)
         WebpageGeneration(inputs)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots = sorted(glob.glob("./.outdir/plots/*.png"))
-        files = sorted(glob.glob("./.outdir/html/*.html"))
-        assert all(i == j for i, j in zip(plots, get_list_of_plots(gw=False)))
-        assert all(i in plots for i in get_list_of_plots(gw=False))
-        assert all(i in get_list_of_plots(gw=False) for i in plots)
-        assert all(i == j for i, j in zip(files, get_list_of_files(gw=False)))
-        assert all(i in files for i in get_list_of_files(gw=False))
-        assert all(i in get_list_of_files(gw=False) for i in files)
+        plots = sorted(glob.glob("{}/plots/*.png".format(self.tmpdir)))
+        files = sorted(glob.glob("{}/html/*.html".format(self.tmpdir)))
+        assert all(i == j for i, j in zip(plots, get_list_of_plots(outdir=self.tmpdir, gw=False)))
+        assert all(i in plots for i in get_list_of_plots(outdir=self.tmpdir, gw=False))
+        assert all(i in get_list_of_plots(outdir=self.tmpdir, gw=False) for i in plots)
+        assert all(i == j for i, j in zip(files, get_list_of_files(outdir=self.tmpdir, gw=False)))
+        assert all(i in files for i in get_list_of_files(outdir=self.tmpdir, gw=False))
+        assert all(i in get_list_of_files(outdir=self.tmpdir, gw=False) for i in files)
         self.check_samples(extension, bilby=bilby)
 
     def check_samples(self, extension, bilby=False):
         """Check that the samples in the result file are consistent with the
         inputs
         """
         from pesummary.core.file.read import read
 
-        initial_samples = read_result_file(extension=extension, bilby=bilby)
-        data = read("./.outdir/samples/posterior_samples.h5")
+        initial_samples = read_result_file(extension=extension, bilby=bilby, outdir=self.tmpdir)
+        data = read("{}/samples/posterior_samples.h5".format(self.tmpdir))
         samples = data.samples_dict
         label = data.labels[0]
         for param in initial_samples.keys():
             for i, j in zip(initial_samples[param], samples[label][param]):
                 assert np.round(i, 8) == np.round(j, 8)
         
 
 class GWBase(Base):
     """Base class to test the full workflow including gw specific options
     """
+    @pytest.mark.workflowtest
     def test_single_run(self, extension, bilby=False, lalinference=False):
         """Test the full workflow for a single result file case
 
         Parameters
         ----------
         result_file: str
             path to result file you wish to run with
         """
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=True, extension=extension, bilby=bilby, lalinference=lalinference)
         print(opts)
         func = functions(opts)
         PlotGeneration(inputs, gw=True)
         WebpageGeneration(inputs, gw=True)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots = sorted(glob.glob("./.outdir/plots/*.png"))
-        files = sorted(glob.glob("./.outdir/html/*.html"))
-        assert all(i == j for i, j in zip(plots, get_list_of_plots(gw=True)))
-        assert all(i in plots for i in get_list_of_plots(gw=True))
-        assert all(i in get_list_of_plots(gw=True) for i in plots)
-        for i, j in zip(files, get_list_of_files(gw=True)):
+        plots = sorted(glob.glob("{}/plots/*.png".format(self.tmpdir)))
+        files = sorted(glob.glob("{}/html/*.html".format(self.tmpdir)))
+        assert all(i == j for i, j in zip(plots, get_list_of_plots(outdir=self.tmpdir, gw=True)))
+        assert all(i in plots for i in get_list_of_plots(outdir=self.tmpdir, gw=True))
+        assert all(i in get_list_of_plots(outdir=self.tmpdir, gw=True) for i in plots)
+        for i, j in zip(files, get_list_of_files(outdir=self.tmpdir, gw=True)):
             print(i, j)
-        assert all(i == j for i, j in zip(files, get_list_of_files(gw=True)))
-        assert all(i in files for i in get_list_of_files(gw=True))
-        assert all(i in get_list_of_files(gw=True) for i in files)
+        assert all(i == j for i, j in zip(files, get_list_of_files(outdir=self.tmpdir, gw=True)))
+        assert all(i in files for i in get_list_of_files(outdir=self.tmpdir, gw=True))
+        assert all(i in get_list_of_files(outdir=self.tmpdir, gw=True) for i in files)
         self.check_samples(extension, bilby=bilby, lalinference=lalinference)
 
     def check_samples(self, extension, bilby=False, lalinference=False):
         """Check that the samples in the result file are consistent with the
         inputs
         """
         from pesummary.core.file.read import read
 
         initial_samples = read_result_file(
-            extension=extension, bilby=bilby, lalinference=lalinference)
-        data = read("./.outdir/samples/posterior_samples.h5")
+            extension=extension, bilby=bilby, lalinference=lalinference,
+            outdir=self.tmpdir
+        )
+        data = read("{}/samples/posterior_samples.h5".format(self.tmpdir))
         samples = data.samples_dict
         label = data.labels[0]
         for param in initial_samples.keys():
             for i, j in zip(initial_samples[param], samples[label][param]):
                 assert np.round(i, 8) == np.round(j, 8)
 
 
 class TestCoreDat(Base):
     """Test the full workflow with a core dat file
     """
     def setup(self):
         """Setup the TestCoreDat class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a core dat result file
         """
         extension = "dat"
         super(TestCoreDat, self).test_single_run(extension)
 
 
 class TestCoreJson(Base):
     """Test the full workflow with a core json file
     """
     def setup(self):
         """Setup the TestCoreJson class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a core json result file
         """
         extension = "json"
         super(TestCoreJson, self).test_single_run(extension)
 
 
 class TestCoreHDF5(Base):
     """Test the full workflow with a core hdf5 file
     """
     def setup(self):
         """Setup the TestCoreHDF5 class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a core hdf5 result file
         """
         extension = "h5"
         super(TestCoreHDF5, self).test_single_run(extension)
 
 
 class TestCoreBilbyJson(Base):
     """Test the full workflow with a core json bilby file
     """
     def setup(self):
         """Setup the TestCoreBilby class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
-        if os.path.isdir(".outdir_pesummary"):
-            shutil.rmtree(".outdir_pesummary")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
+        if os.path.isdir("{}_pesummary".format(self.tmpdir)):
+            shutil.rmtree("{}_pesummary".format(self.tmpdir))
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a core bilby result file
         """
         extension = "json"
         super(TestCoreBilbyJson, self).test_single_run(extension, bilby=True)
 
+    @pytest.mark.workflowtest
     def test_double_run(self):
         """Test the full workflow for 2 lalinference result files
         """
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=False, extension="json", bilby=True, number=2)
         func = functions(opts)
         PlotGeneration(inputs)
         WebpageGeneration(inputs)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots = sorted(glob.glob("./.outdir/plots/*.png"))
-        files = sorted(glob.glob("./.outdir/html/*.html"))
-        assert all(i == j for i, j in zip(plots, get_list_of_plots(
+        plots = sorted(glob.glob("{}/plots/*.png".format(self.tmpdir)))
+        files = sorted(glob.glob("{}/html/*.html".format(self.tmpdir)))
+        assert all(i == j for i, j in zip(plots, get_list_of_plots(outdir=self.tmpdir, 
                        gw=False, number=2)))
-        assert all(i in plots for i in get_list_of_plots(gw=False, number=2))
-        assert all(i in get_list_of_plots(gw=False, number=2) for i in plots)
-        assert all(i == j for i, j in zip(files, get_list_of_files(
+        assert all(i in plots for i in get_list_of_plots(outdir=self.tmpdir, gw=False, number=2))
+        assert all(i in get_list_of_plots(outdir=self.tmpdir, gw=False, number=2) for i in plots)
+        assert all(i == j for i, j in zip(files, get_list_of_files(outdir=self.tmpdir, 
                        gw=False, number=2)))
-        assert all(i in files for i in get_list_of_files(gw=False, number=2))
-        assert all(i in get_list_of_files(gw=False, number=2) for i in files)
+        assert all(i in files for i in get_list_of_files(outdir=self.tmpdir, gw=False, number=2))
+        assert all(i in get_list_of_files(outdir=self.tmpdir, gw=False, number=2) for i in files)
 
+    @pytest.mark.workflowtest
     def test_existing_run(self):
         """Test the fill workflow for when you add to an existing webpage
         """
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=False, extension="json", bilby=True)
         func = functions(opts)
         PlotGeneration(inputs)
         WebpageGeneration(inputs)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=False, extension="json", bilby=True, existing=True)
         func = functions(opts)
         PlotGeneration(inputs)
         WebpageGeneration(inputs)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots = sorted(glob.glob("./.outdir/plots/*.png"))
-        files = sorted(glob.glob("./.outdir/html/*.html"))
+        plots = sorted(glob.glob("{}/plots/*.png".format(self.tmpdir)))
+        files = sorted(glob.glob("{}/html/*.html".format(self.tmpdir)))
 
-        assert all(i == j for i, j in zip(plots, get_list_of_plots(
+        assert all(i == j for i, j in zip(plots, get_list_of_plots(outdir=self.tmpdir, 
                        gw=False, number=2)))
-        assert all(i in plots for i in get_list_of_plots(gw=False, number=2))
-        assert all(i in get_list_of_plots(gw=False, number=2) for i in plots)
-        assert all(i == j for i, j in zip(files, get_list_of_files(
+        assert all(i in plots for i in get_list_of_plots(outdir=self.tmpdir, gw=False, number=2))
+        assert all(i in get_list_of_plots(outdir=self.tmpdir, gw=False, number=2) for i in plots)
+        assert all(i == j for i, j in zip(files, get_list_of_files(outdir=self.tmpdir, 
                        gw=False, number=2)))
-        assert all(i in files for i in get_list_of_files(gw=False, number=2))
-        assert all(i in get_list_of_files(gw=False, number=2) for i in files)
+        assert all(i in files for i in get_list_of_files(outdir=self.tmpdir, gw=False, number=2))
+        assert all(i in get_list_of_files(outdir=self.tmpdir, gw=False, number=2) for i in files)
 
+    @pytest.mark.workflowtest
     def test_pesummary_input(self):
         """Test the full workflow for a pesummary input file
         """
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=False, extension="json", bilby=True, number=2)
         func = functions(opts)
         PlotGeneration(inputs)
         WebpageGeneration(inputs)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots = sorted(glob.glob("./.outdir/plots/*.png"))
-        files = sorted(glob.glob("./.outdir/html/*.html"))
+        plots = sorted(glob.glob("{}/plots/*.png".format(self.tmpdir)))
+        files = sorted(glob.glob("{}/html/*.html".format(self.tmpdir)))
 
-        from pesummary.core.command_line import command_line
+        from pesummary.core.cli.parser import ArgumentParser
 
-        parser = command_line()
-        default_args = ["--webdir", ".outdir_pesummary",
-                        "--samples", ".outdir/samples/posterior_samples.h5"]
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        default_args = ["--webdir", "{}_pesummary".format(self.tmpdir),
+                        "--samples", "{}/samples/posterior_samples.h5".format(self.tmpdir),
+                        "--disable_expert"]
         opts = parser.parse_args(default_args)
         func = functions(opts)
         inputs = func["input"](opts)
         PlotGeneration(inputs)
         WebpageGeneration(inputs)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots_pesummary = sorted(glob.glob("./.outdir_pesummary/plots/*.png"))
-        files_pesummary = sorted(glob.glob("./.outdir_pesummary/html/*.html"))
+        plots_pesummary = sorted(glob.glob("{}_pesummary/plots/*.png".format(self.tmpdir)))
+        files_pesummary = sorted(glob.glob("{}_pesummary/html/*.html".format(self.tmpdir)))
 
         assert all(i.split("/")[-1] == j.split("/")[-1] for i, j in zip(
                    plots, plots_pesummary))
         assert all(i.split("/")[-1] == j.split("/")[-1] for i, j in zip(
                    files, files_pesummary))
 
 class TestCoreBilbyHDF5(Base):
     """Test the full workflow with a core hdf5 bilby file
     """
     def setup(self):
         """Setup the TestCoreBilby class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a core bilby result file
         """
         extension = "h5"
         super(TestCoreBilbyHDF5, self).test_single_run(extension, bilby=True)
 
 
 class TestGWDat(GWBase):
     """Test the full workflow with a gw dat file
     """
     def setup(self):
         """Setup the TestCoreDat class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a gw dat result file
         """
         extension = "dat"
         super(TestGWDat, self).test_single_run(extension)
 
 
 class TestGWJson(GWBase):
     """Test the full workflow with a json dat file
     """
     def setup(self):
         """Setup the TestGWJson class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a gw json result file
         """
         extension = "json"
         super(TestGWJson, self).test_single_run(extension)
 
 
 class TestGWBilbyJson(GWBase):
     """Test the full workflow with a gw bilby json file
     """
     def setup(self):
         """Setup the TestGWJson class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a gw bilby json result file
         """
         extension = "json"
         super(TestGWBilbyJson, self).test_single_run(extension, bilby=True)
 
 
 class TestGWBilbyHDF5(GWBase):
     """Test the full workflow with a gw bilby HDF5 file
     """
     def setup(self):
         """Setup the TestGWJson class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a gw bilby HDF5 result file
         """
         extension = "h5"
         super(TestGWBilbyHDF5, self).test_single_run(extension, bilby=True)
 
 
 class TestGWLALInference(GWBase):
     """Test the full workflow with a lalinference file
     """
     def setup(self):
         """Setup the TestGWJson class
         """
-        if not os.path.isdir(".outdir"):
-            os.mkdir(".outdir")
+        self.tmpdir = tempfile.TemporaryDirectory(prefix=".", dir=".").name
+        if not os.path.isdir(self.tmpdir):
+            os.mkdir(self.tmpdir)
 
     def teardown(self):
         """Remove the files and directories created from this class
         """
-        if os.path.isdir(".outdir"):
-            shutil.rmtree(".outdir")
-        if os.path.isdir(".outdir_pesummary"):
-            shutil.rmtree(".outdir_pesummary")
+        if os.path.isdir(self.tmpdir):
+            shutil.rmtree(self.tmpdir)
+        if os.path.isdir("{}_pesummary".format(self.tmpdir)):
+            shutil.rmtree("{}_pesummary".format(self.tmpdir))
 
+    @pytest.mark.workflowtest
     def test_single_run(self):
         """Test the full workflow with a lalinference result file
         """
         extension = "hdf5"
         super(TestGWLALInference, self).test_single_run(extension, lalinference=True)
 
+    @pytest.mark.workflowtest
     def test_double_run(self):
         """Test the full workflow for 2 lalinference result files
         """
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=True, extension="hdf5", lalinference=True, number=2)
         func = functions(opts)
         PlotGeneration(inputs, gw=True)
         WebpageGeneration(inputs, gw=True)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots = sorted(glob.glob("./.outdir/plots/*.png"))
-        files = sorted(glob.glob("./.outdir/html/*.html"))
-        assert all(i == j for i, j in zip(plots, get_list_of_plots(
+        plots = sorted(glob.glob("{}/plots/*.png".format(self.tmpdir)))
+        files = sorted(glob.glob("{}/html/*.html".format(self.tmpdir)))
+        assert all(i == j for i, j in zip(plots, get_list_of_plots(outdir=self.tmpdir, 
                        gw=True, number=2)))
-        assert all(i in plots for i in get_list_of_plots(gw=True, number=2))
-        assert all(i in get_list_of_plots(gw=True, number=2) for i in plots)
-        for i, j in zip(files, get_list_of_files(
+        assert all(i in plots for i in get_list_of_plots(outdir=self.tmpdir, gw=True, number=2))
+        assert all(i in get_list_of_plots(outdir=self.tmpdir, gw=True, number=2) for i in plots)
+        for i, j in zip(files, get_list_of_files(outdir=self.tmpdir, 
                        gw=True, number=2)):
             print(i, j)
-        assert all(i == j for i, j in zip(files, get_list_of_files(
+        assert all(i == j for i, j in zip(files, get_list_of_files(outdir=self.tmpdir, 
                        gw=True, number=2)))
-        assert all(i in files for i in get_list_of_files(gw=True, number=2))
-        assert all(i in get_list_of_files(gw=True, number=2) for i in files)
+        assert all(i in files for i in get_list_of_files(outdir=self.tmpdir, gw=True, number=2))
+        assert all(i in get_list_of_files(outdir=self.tmpdir, gw=True, number=2) for i in files)
 
+    @pytest.mark.workflowtest
     def test_existing_run(self):
         """Test the fill workflow for when you add to an existing webpage
         """
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=True, extension="hdf5", lalinference=True)
         func = functions(opts)
         PlotGeneration(inputs, gw=True)
         WebpageGeneration(inputs, gw=True)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=True, extension="hdf5", lalinference=True, existing=True)
         func = functions(opts)
         PlotGeneration(inputs, gw=True)
         WebpageGeneration(inputs, gw=True)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots = sorted(glob.glob("./.outdir/plots/*.png"))
-        files = sorted(glob.glob("./.outdir/html/*.html"))
-        assert all(i == j for i, j in zip(plots, get_list_of_plots(
+        plots = sorted(glob.glob("{}/plots/*.png".format(self.tmpdir)))
+        files = sorted(glob.glob("{}/html/*.html".format(self.tmpdir)))
+        assert all(i == j for i, j in zip(plots, get_list_of_plots(outdir=self.tmpdir, 
                        gw=True, number=2)))
-        assert all(i in plots for i in get_list_of_plots(gw=True, number=2))
-        assert all(i in get_list_of_plots(gw=True, number=2) for i in plots)
-        assert all(i == j for i, j in zip(files, get_list_of_files(
+        assert all(i in plots for i in get_list_of_plots(outdir=self.tmpdir, gw=True, number=2))
+        assert all(i in get_list_of_plots(outdir=self.tmpdir, gw=True, number=2) for i in plots)
+        assert all(i == j for i, j in zip(files, get_list_of_files(outdir=self.tmpdir, 
                        gw=True, number=2)))
-        assert all(i in files for i in get_list_of_files(gw=True, number=2))
-        assert all(i in get_list_of_files(gw=True, number=2) for i in files)
+        assert all(i in files for i in get_list_of_files(outdir=self.tmpdir, gw=True, number=2))
+        assert all(i in get_list_of_files(outdir=self.tmpdir, gw=True, number=2) for i in files)
 
+    @pytest.mark.workflowtest
     def test_pesummary_input(self):
         """Test the full workflow for a pesummary input file
         """
-        opts, inputs = make_argparse(
+        opts, inputs = make_argparse(outdir=self.tmpdir, 
             gw=True, extension="hdf5", lalinference=True, number=2)
         func = functions(opts)
         PlotGeneration(inputs, gw=True)
         WebpageGeneration(inputs, gw=True)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots = sorted(glob.glob("./.outdir/plots/*.png"))
-        files = sorted(glob.glob("./.outdir/html/*.html"))
+        plots = sorted(glob.glob("{}/plots/*.png".format(self.tmpdir)))
+        files = sorted(glob.glob("{}/html/*.html".format(self.tmpdir)))
 
-        from pesummary.core.command_line import command_line
-        from pesummary.gw.command_line import insert_gwspecific_option_group
+        from pesummary.gw.cli.parser import ArgumentParser
 
-        parser = command_line()
-        insert_gwspecific_option_group(parser)
-        default_args = ["--webdir", ".outdir_pesummary",
-                        "--samples", ".outdir/samples/posterior_samples.h5",
-                        "--gw"]
+        parser = ArgumentParser()
+        parser.add_all_known_options_to_parser()
+        default_args = ["--webdir", "{}_pesummary".format(self.tmpdir),
+                        "--samples", "{}/samples/posterior_samples.h5".format(self.tmpdir),
+                        "--gw", "--disable_expert"]
         from pesummary.gw.file.read import read
-        f = read(".outdir/samples/posterior_samples.h5")
+        f = read("{}/samples/posterior_samples.h5".format(self.tmpdir))
         opts = parser.parse_args(default_args)
         inputs = func["input"](opts)
         PlotGeneration(inputs, gw=True)
         WebpageGeneration(inputs, gw=True)
         func["MetaFile"](inputs)
         func["FinishingTouches"](inputs)
 
-        plots_pesummary = sorted(glob.glob("./.outdir_pesummary/plots/*.png"))
-        files_pesummary = sorted(glob.glob("./.outdir_pesummary/html/*.html"))
+        plots_pesummary = sorted(glob.glob("{}_pesummary/plots/*.png".format(self.tmpdir)))
+        files_pesummary = sorted(glob.glob("{}_pesummary/html/*.html".format(self.tmpdir)))
 
         assert all(i.split("/")[-1] == j.split("/")[-1] for i, j in zip(
                    plots, plots_pesummary))
         assert all(i.split("/")[-1] == j.split("/")[-1] for i, j in zip(
                    files, files_pesummary))
```

### Comparing `pesummary-0.9.1/pesummary/utils/samples_dict.py` & `pesummary-1.0.0/pesummary/gw/plots/main.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,1445 +1,1253 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#! /usr/bin/env python
 
-import copy
-import numpy as np
-from pesummary.utils.utils import resample_posterior_distribution, logger
-from pesummary.utils.decorators import docstring_subfunction
+# Licensed under an MIT style license -- see LICENSE.md
+
+import os
+
+from pesummary.core.plots.main import _PlotGeneration as _BasePlotGeneration
 from pesummary.core.plots.latex_labels import latex_labels
+from pesummary.core.plots import interactive
+from pesummary.core.plots.bounded_1d_kde import ReflectionBoundedKDE
 from pesummary.gw.plots.latex_labels import GWlatex_labels
-from pesummary import conf
-import importlib
+from pesummary.utils.utils import logger, resample_posterior_distribution
+from pesummary.utils.decorators import no_latex_plot
+from pesummary.gw.plots import publication
+from pesummary.gw.plots import plot as gw
+
+import multiprocessing as mp
+import numpy as np
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 latex_labels.update(GWlatex_labels)
 
 
-class SamplesDict(dict):
-    """Class to store the samples from a single run
+class _PlotGeneration(_BasePlotGeneration):
+    def __init__(
+        self, savedir=None, webdir=None, labels=None, samples=None,
+        kde_plot=False, existing_labels=None, existing_injection_data=None,
+        existing_file_kwargs=None, existing_samples=None,
+        existing_metafile=None, same_parameters=None, injection_data=None,
+        result_files=None, file_kwargs=None, colors=None, custom_plotting=None,
+        add_to_existing=False, priors={}, no_ligo_skymap=False,
+        nsamples_for_skymap=None, detectors=None, maxL_samples=None,
+        gwdata=None, calibration=None, psd=None,
+        multi_threading_for_skymap=None, approximant=None,
+        pepredicates_probs=None, include_prior=False, publication=False,
+        existing_approximant=None, existing_psd=None, existing_calibration=None,
+        existing_weights=None, weights=None, disable_comparison=False,
+        linestyles=None, disable_interactive=False, disable_corner=False,
+        publication_kwargs={}, multi_process=1, mcmc_samples=False,
+        skymap=None, existing_skymap=None, corner_params=None,
+        preliminary_pages=False, expert_plots=True, checkpoint=False
+    ):
+        super(_PlotGeneration, self).__init__(
+            savedir=savedir, webdir=webdir, labels=labels,
+            samples=samples, kde_plot=kde_plot, existing_labels=existing_labels,
+            existing_injection_data=existing_injection_data,
+            existing_samples=existing_samples,
+            existing_weights=existing_weights,
+            same_parameters=same_parameters,
+            injection_data=injection_data, mcmc_samples=mcmc_samples,
+            colors=colors, custom_plotting=custom_plotting,
+            add_to_existing=add_to_existing, priors=priors,
+            include_prior=include_prior, weights=weights,
+            disable_comparison=disable_comparison, linestyles=linestyles,
+            disable_interactive=disable_interactive, disable_corner=disable_corner,
+            multi_process=multi_process, corner_params=corner_params,
+            expert_plots=expert_plots, checkpoint=checkpoint
+        )
+        self.preliminary_pages = preliminary_pages
+        if not isinstance(self.preliminary_pages, dict):
+            if self.preliminary_pages:
+                self.preliminary_pages = {
+                    label: True for label in self.labels
+                }
+            else:
+                self.preliminary_pages = {
+                    label: False for label in self.labels
+                }
+        self.preliminary_comparison_pages = any(
+            value for value in self.preliminary_pages.values()
+        )
+        self.package = "gw"
+        self.file_kwargs = file_kwargs
+        self.existing_file_kwargs = existing_file_kwargs
+        self.no_ligo_skymap = no_ligo_skymap
+        self.nsamples_for_skymap = nsamples_for_skymap
+        self.detectors = detectors
+        self.maxL_samples = maxL_samples
+        self.gwdata = gwdata
+        if skymap is None:
+            skymap = {label: None for label in self.labels}
+        self.skymap = skymap
+        self.existing_skymap = skymap
+        self.calibration = calibration
+        self.existing_calibration = existing_calibration
+        self.psd = psd
+        self.existing_psd = existing_psd
+        self.multi_threading_for_skymap = multi_threading_for_skymap
+        self.approximant = approximant
+        self.existing_approximant = existing_approximant
+        self.pepredicates_probs = pepredicates_probs
+        self.publication = publication
+        self.publication_kwargs = publication_kwargs
+        self._ligo_skymap_PID = {}
+
+        self.plot_type_dictionary.update({
+            "psd": self.psd_plot,
+            "calibration": self.calibration_plot,
+            "skymap": self.skymap_plot,
+            "waveform_fd": self.waveform_fd_plot,
+            "waveform_td": self.waveform_td_plot,
+            "data": self.gwdata_plots,
+            "violin": self.violin_plot,
+            "spin_disk": self.spin_dist_plot,
+            "pepredicates": self.pepredicates_plot
+        })
+        if self.make_comparison:
+            self.plot_type_dictionary.update({
+                "skymap_comparison": self.skymap_comparison_plot,
+                "waveform_comparison_fd": self.waveform_comparison_fd_plot,
+                "waveform_comparison_td": self.waveform_comparison_td_plot,
+                "2d_comparison_contour": self.twod_comparison_contour_plot,
+            })
+
+    @property
+    def ligo_skymap_PID(self):
+        return self._ligo_skymap_PID
+
+    def generate_plots(self):
+        """Generate all plots for all result files
+        """
+        if self.calibration or "calibration" in list(self.priors.keys()):
+            self.try_to_make_a_plot("calibration")
+        if self.psd:
+            self.try_to_make_a_plot("psd")
+        super(_PlotGeneration, self).generate_plots()
+
+    def _generate_plots(self, label):
+        """Generate all plots for a given result file
+        """
+        super(_PlotGeneration, self)._generate_plots(label)
+        self.try_to_make_a_plot("skymap", label=label)
+        self.try_to_make_a_plot("waveform_td", label=label)
+        self.try_to_make_a_plot("waveform_fd", label=label)
+        if self.pepredicates_probs[label] is not None:
+            self.try_to_make_a_plot("pepredicates", label=label)
+        if self.gwdata:
+            self.try_to_make_a_plot("data", label=label)
+
+    def _generate_comparison_plots(self):
+        """Generate all comparison plots
+        """
+        super(_PlotGeneration, self)._generate_comparison_plots()
+        self.try_to_make_a_plot("skymap_comparison")
+        self.try_to_make_a_plot("waveform_comparison_td")
+        self.try_to_make_a_plot("waveform_comparison_fd")
+        if self.publication:
+            self.try_to_make_a_plot("2d_comparison_contour")
+            self.try_to_make_a_plot("violin")
+            self.try_to_make_a_plot("spin_disk")
 
-    Parameters
-    ----------
-    parameters: list
-        list of parameters
-    samples: nd list
-        list of samples for each parameter
-    autoscale: Bool, optional
-        If True, the posterior samples for each parameter are scaled to the
-        same length
-
-    Attributes
-    ----------
-    maxL: pesummary.utils.samples_dict.SamplesDict
-        SamplesDict object containing the maximum likelihood sample keyed by
-        the parameter
-    minimum: pesummary.utils.samples_dict.SamplesDict
-        SamplesDict object containing the minimum sample for each parameter
-    maximum: pesummary.utils.samples_dict.SamplesDict
-        SamplesDict object containing the maximum sample for each parameter
-    median: pesummary.utils.samples_dict.SamplesDict
-        SamplesDict object containining the median of each marginalized
-        posterior distribution
-    mean: pesummary.utils.samples_dict.SamplesDict
-        SamplesDict object containing the mean of each marginalized posterior
-        distribution
-    number_of_samples: int
-        Number of samples stored in the SamplesDict object
-    latex_labels: dict
-        Dictionary of latex labels for each parameter
-    available_plots: list
-        list of plots which the user may user to display the contained posterior
-        samples
+    @staticmethod
+    def _corner_plot(
+        savedir, label, samples, latex_labels, webdir, params, preliminary=False,
+        checkpoint=False
+    ):
+        """Generate a corner plot for a given set of samples
 
-    Methods
-    -------
-    from_file:
-        Initialize the SamplesDict class with the contents of a file
-    to_pandas:
-        Convert the SamplesDict object to a pandas DataFrame
-    to_structured_array:
-        Convert the SamplesDict object to a numpy structured array
-    pop:
-        Remove an entry from the SamplesDict object
-    downsample:
-        Downsample the samples stored in the SamplesDict object. See the
-        pesummary.utils.utils.resample_posterior_distribution method
-    discard_samples:
-        Remove the first N samples from each distribution
-    plot:
-        Generate a plot based on the posterior samples stored
-
-    Examples
-    --------
-    How the initialize the SamplesDict class
-
-    >>> from pesummary.utils.samples_dict import SamplesDict
-    >>> data = {
-    ...     "a": [1, 1.2, 1.7, 1.1, 1.4, 0.8, 1.6],
-    ...     "b": [10.2, 11.3, 11.6, 9.5, 8.6, 10.8, 10.9]
-    ... }
-    >>> dataset = SamplesDict(data)
-    >>> parameters = ["a", "b"]
-    >>> samples = [
-    ...     [1, 1.2, 1.7, 1.1, 1.4, 0.8, 1.6],
-    ...     [10.2, 11.3, 11.6, 9.5, 8.6, 10.8, 10.9]
-    ... }
-    >>> dataset = SamplesDict(parameters, samples)
-    >>> fig = dataset.plot("a", type="hist", bins=30)
-    >>> fig.show()
-    """
-    def __init__(self, *args, logger_warn="warn", autoscale=True):
-        super(SamplesDict, self).__init__()
-        if len(args) == 1 and isinstance(args[0], dict):
-            self.parameters = list(args[0].keys())
-            self.samples = np.array(
-                [args[0][param] for param in self.parameters]
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        label: str
+            the label corresponding to the results file
+        samples: dict
+            dictionary of samples for a given result file
+        latex_labels: dict
+            dictionary of latex labels
+        webdir: str
+            directory where the javascript is written
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        import warnings
+
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
+            filename = os.path.join(
+                savedir, "corner", "{}_all_density_plots.png".format(label)
             )
-            for key, item in args[0].items():
-                self[key] = Array(item)
-        else:
-            self.parameters, self.samples = args
-            lengths = [len(i) for i in self.samples]
-            if len(np.unique(lengths)) > 1 and autoscale:
-                nsamples = np.min(lengths)
-                getattr(logger, logger_warn)(
-                    "Unequal number of samples for each parameter. "
-                    "Restricting all posterior samples to have {} "
-                    "samples".format(nsamples)
+            if os.path.isfile(filename) and checkpoint:
+                pass
+            else:
+                fig, params, data = gw._make_corner_plot(
+                    samples, latex_labels, corner_parameters=params
                 )
-                self.samples = [
-                    dataset[:nsamples] for dataset in self.samples
-                ]
-            self.make_dictionary()
-        self.latex_labels = {
-            param: latex_labels[param] if param in latex_labels.keys() else
-            param for param in self.parameters
-        }
-
-    def __getitem__(self, key):
-        """Return an object representing the specialization of SamplesDict
-        by type arguments found in key.
-        """
-        if isinstance(key, slice):
-            return SamplesDict(
-                self.parameters,
-                [i[key.start:key.stop:key.step] for i in self.samples]
-            )
-        if isinstance(key, str):
-            if key not in self.keys():
-                raise KeyError(
-                    "{} not in dictionary. The list of available keys are "
-                    "{}".format(key, self.keys())
+                fig.savefig(filename)
+                fig.close()
+                combine_corner = open(
+                    os.path.join(webdir, "js", "combine_corner.js")
                 )
-        return super(SamplesDict, self).__getitem__(key)
-
-    def __str__(self):
-        """Print a summary of the information stored in the dictionary
-        """
-        def format_string(string, row):
-            """Format a list into a table
-
-            Parameters
-            ----------
-            string: str
-                existing table
-            row: list
-                the row you wish to be written to a table
-            """
-            string += "{:<8}".format(row[0])
-            for i in range(1, len(row)):
-                if isinstance(row[i], str):
-                    string += "{:<15}".format(row[i])
-                elif isinstance(row[i], (float, int, np.int64, np.int32)):
-                    string += "{:<15.6f}".format(row[i])
-            string += "\n"
-            return string
-
-        string = ""
-        string = format_string(string, ["idx"] + list(self.keys()))
-
-        if self.number_of_samples < 8:
-            for i in range(self.number_of_samples):
-                string = format_string(
-                    string, [i] + [item[i] for key, item in self.items()]
+                combine_corner = combine_corner.readlines()
+                params = [str(i) for i in params]
+                ind = [
+                    linenumber for linenumber, line in enumerate(combine_corner)
+                    if "var list = {}" in line
+                ][0]
+                combine_corner.insert(
+                    ind + 1, "    list['{}'] = {};\n".format(label, params)
                 )
-        else:
-            for i in range(4):
-                string = format_string(
-                    string, [i] + [item[i] for key, item in self.items()]
+                new_file = open(
+                    os.path.join(webdir, "js", "combine_corner.js"), "w"
+                )
+                new_file.writelines(combine_corner)
+                new_file.close()
+                combine_corner = open(
+                    os.path.join(webdir, "js", "combine_corner.js")
                 )
-            for i in range(2):
-                string = format_string(string, ["."] * (len(self.keys()) + 1))
-            for i in range(self.number_of_samples - 2, self.number_of_samples):
-                string = format_string(
-                    string, [i] + [item[i] for key, item in self.items()]
+                combine_corner = combine_corner.readlines()
+                params = [str(i) for i in params]
+                ind = [
+                    linenumber for linenumber, line in enumerate(combine_corner)
+                    if "var data = {}" in line
+                ][0]
+                combine_corner.insert(
+                    ind + 1, "    data['{}'] = {};\n".format(label, data)
                 )
-        return string
+                new_file = open(
+                    os.path.join(webdir, "js", "combine_corner.js"), "w"
+                )
+                new_file.writelines(combine_corner)
+                new_file.close()
 
-    @classmethod
-    def from_file(cls, filename, **kwargs):
-        """Initialize the SamplesDict class with the contents of a result file
+            filename = os.path.join(
+                savedir, "corner", "{}_sourceframe.png".format(label)
+            )
+            if os.path.isfile(filename) and checkpoint:
+                pass
+            else:
+                fig = gw._make_source_corner_plot(samples, latex_labels)
+                fig.savefig(filename)
+                fig.close()
+            filename = os.path.join(
+                savedir, "corner", "{}_extrinsic.png".format(label)
+            )
+            if os.path.isfile(filename) and checkpoint:
+                pass
+            else:
+                fig = gw._make_extrinsic_corner_plot(samples, latex_labels)
+                fig.savefig(filename)
+                fig.close()
+
+    def skymap_plot(self, label):
+        """Generate a skymap plot for a given result file
 
         Parameters
         ----------
-        filename: str
-            path to the result file you wish to load.
-        **kwargs: dict
-            all kwargs are passed to the pesummary.io.read function
+        label: str
+            the label for the results file that you wish to plot
         """
-        from pesummary.io import read
-
-        return read(filename, **kwargs).samples_dict
+        try:
+            import ligo.skymap  # noqa: F401
+        except ImportError:
+            SKYMAP = False
+        else:
+            SKYMAP = True
 
-    @property
-    def maxL(self):
-        return SamplesDict(
-            self.parameters, [[item.maxL] for key, item in self.items()]
-        )
+        if self.mcmc_samples:
+            samples = self.samples[label].combine
+        else:
+            samples = self.samples[label]
+        _injection = [
+            self.injection_data[label]["ra"], self.injection_data[label]["dec"]
+        ]
+        self._skymap_plot(
+            self.savedir, samples["ra"], samples["dec"], label,
+            self.weights[label], _injection,
+            preliminary=self.preliminary_pages[label]
+        )
+
+        if SKYMAP and not self.no_ligo_skymap and self.skymap[label] is None:
+            from pesummary.utils.utils import RedirectLogger
+
+            logger.info("Launching subprocess to generate skymap plot with "
+                        "ligo.skymap")
+            try:
+                _time = samples["geocent_time"]
+            except KeyError:
+                logger.warning(
+                    "Unable to find 'geocent_time' in the posterior table for {}. "
+                    "The ligo.skymap fits file will therefore not store the "
+                    "DATE_OBS field in the header".format(label)
+                )
+                _time = None
+            with RedirectLogger("ligo.skymap", level="DEBUG") as redirector:
+                process = mp.Process(
+                    target=self._ligo_skymap_plot,
+                    args=[
+                        self.savedir, samples["ra"], samples["dec"],
+                        samples["luminosity_distance"], _time,
+                        label, self.nsamples_for_skymap, self.webdir,
+                        self.multi_threading_for_skymap, _injection,
+                        self.preliminary_pages[label]
+                    ]
+                )
+                process.start()
+                PID = process.pid
+            self._ligo_skymap_PID[label] = PID
+        elif SKYMAP and not self.no_ligo_skymap:
+            self._ligo_skymap_array_plot(
+                self.savedir, self.skymap[label], label,
+                self.preliminary_pages[label]
+            )
 
-    @property
-    def minimum(self):
-        return SamplesDict(
-            self.parameters, [[item.minimum] for key, item in self.items()]
-        )
+    @staticmethod
+    @no_latex_plot
+    def _skymap_plot(
+        savedir, ra, dec, label, weights, injection=None, preliminary=False
+    ):
+        """Generate a skymap plot for a given set of samples
 
-    @property
-    def maximum(self):
-        return SamplesDict(
-            self.parameters, [[item.maximum] for key, item in self.items()]
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        ra: pesummary.utils.utils.Array
+            array containing the samples for right ascension
+        dec: pesummary.utils.utils.Array
+            array containing the samples for declination
+        label: str
+            the label corresponding to the results file
+        weights: list
+            list of weights for the samples
+        injection: list, optional
+            list containing the injected value of ra and dec
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        import math
+
+        if injection is not None and any(math.isnan(inj) for inj in injection):
+            injection = None
+        fig = gw._default_skymap_plot(ra, dec, weights, injection=injection)
+        _PlotGeneration.save(
+            fig, os.path.join(savedir, "{}_skymap".format(label)),
+            preliminary=preliminary
         )
 
-    @property
-    def median(self):
-        return SamplesDict(
-            self.parameters,
-            [[item.average(type="median")] for key, item in self.items()]
-        )
+    @staticmethod
+    @no_latex_plot
+    def _ligo_skymap_plot(savedir, ra, dec, dist, time, label, nsamples_for_skymap,
+                          webdir, multi_threading_for_skymap, injection,
+                          preliminary=False):
+        """Generate a skymap plot for a given set of samples using the
+        ligo.skymap package
 
-    @property
-    def mean(self):
-        return SamplesDict(
-            self.parameters,
-            [[item.average(type="mean")] for key, item in self.items()]
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        ra: pesummary.utils.utils.Array
+            array containing the samples for right ascension
+        dec: pesummary.utils.utils.Array
+            array containing the samples for declination
+        dist: pesummary.utils.utils.Array
+            array containing the samples for luminosity distance
+        time: pesummary.utils.utils.Array
+            array containing the samples for the geocentric time of merger
+        label: str
+            the label corresponding to the results file
+        nsamples_for_skymap: int
+            the number of samples used to generate skymap
+        webdir: str
+            the directory to store the fits file
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        import math
+
+        downsampled = False
+        if nsamples_for_skymap is not None:
+            ra, dec, dist = resample_posterior_distribution(
+                [ra, dec, dist], nsamples_for_skymap
+            )
+            downsampled = True
+        if injection is not None and any(math.isnan(inj) for inj in injection):
+            injection = None
+        fig = gw._ligo_skymap_plot(
+            ra, dec, dist=dist, savedir=os.path.join(webdir, "samples"),
+            nprocess=multi_threading_for_skymap, downsampled=downsampled,
+            label=label, time=time, injection=injection
+        )
+        _PlotGeneration.save(
+            fig, os.path.join(savedir, "{}_skymap".format(label)),
+            preliminary=preliminary
         )
 
-    @property
-    def number_of_samples(self):
-        return len(self[self.parameters[0]])
-
-    @property
-    def plotting_map(self):
-        return {
-            "marginalized_posterior": self._marginalized_posterior,
-            "skymap": self._skymap,
-            "hist": self._marginalized_posterior,
-            "corner": self._corner,
-            "spin_disk": self._spin_disk
-        }
-
-    @property
-    def available_plots(self):
-        return list(self.plotting_map.keys())
+    @staticmethod
+    @no_latex_plot
+    def _ligo_skymap_array_plot(savedir, skymap, label, preliminary=False):
+        """Generate a skymap based on skymap probability array already generated with
+        `ligo.skymap`
 
-    def to_pandas(self, **kwargs):
-        """Convert a SamplesDict object to a pandas dataframe
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        skymap: np.ndarray
+            array of skymap probabilities
+        label: str
+            the label corresponding to the results file
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
-        from pandas import DataFrame
+        fig = gw._ligo_skymap_plot_from_array(skymap)
+        _PlotGeneration.save(
+            fig, os.path.join(savedir, "{}_skymap".format(label)),
+            preliminary=preliminary
+        )
 
-        return DataFrame(self, **kwargs)
+    def waveform_fd_plot(self, label):
+        """Generate a frequency domain waveform plot for a given result file
 
-    def to_structured_array(self, **kwargs):
-        """Convert a SamplesDict object to a structured numpy array
+        Parameters
+        ----------
+        label: str
+            the label corresponding to the results file
         """
-        return self.to_pandas(**kwargs).to_records(
-            index=False, column_dtypes=np.float
+        if self.approximant[label] == {}:
+            return
+        self._waveform_fd_plot(
+            self.savedir, self.detectors[label], self.maxL_samples[label], label,
+            self.preliminary_pages[label], self.checkpoint
         )
 
-    def pop(self, parameter):
-        """Delete a parameter from the SamplesDict
+    @staticmethod
+    def _waveform_fd_plot(
+        savedir, detectors, maxL_samples, label, preliminary=False,
+        checkpoint=False
+    ):
+        """Generate a frequency domain waveform plot for a given detector
+        network and set of samples
 
         Parameters
         ----------
-        parameter: str
-            name of the parameter you wish to remove from the SamplesDict
+        savedir: str
+            the directory you wish to save the plot in
+        detectors: list
+            list of detectors used in your analysis
+        maxL_samples: dict
+            dictionary of maximum likelihood values
+        label: str
+            the label corresponding to the results file
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
-        if parameter not in self.parameters:
-            logger.info(
-                "{} not in SamplesDict. Unable to remove {}".format(
-                    parameter, parameter
-                )
-            )
+        filename = os.path.join(savedir, "{}_waveform.png".format(label))
+        if os.path.isfile(filename) and checkpoint:
             return
-        ind = self.parameters.index(parameter)
-        self.parameters.remove(parameter)
-        remove = self.samples[ind]
-        samples = self.samples
-        if isinstance(self.samples, np.ndarray):
-            samples = self.samples.tolist()
-            remove = self.samples[ind].tolist()
-        samples.remove(remove)
-        if isinstance(self.samples, np.ndarray):
-            self.samples = np.array(samples)
-        return super(SamplesDict, self).pop(parameter)
+        if detectors is None:
+            detectors = ["H1", "L1"]
+        else:
+            detectors = detectors.split("_")
+
+        fig = gw._waveform_plot(detectors, maxL_samples)
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
+        )
 
-    def downsample(self, number):
-        """Downsample the samples stored in the SamplesDict class
+    def waveform_td_plot(self, label):
+        """Generate a time domain waveform plot for a given result file
 
         Parameters
         ----------
-        number: int
-            Number of samples you wish to downsample to
+        label: str
+            the label corresponding to the results file
         """
-        self.samples = resample_posterior_distribution(self.samples, number)
-        self.make_dictionary()
-        return self
+        if self.approximant[label] == {}:
+            return
+        self._waveform_td_plot(
+            self.savedir, self.detectors[label], self.maxL_samples[label], label,
+            self.preliminary_pages[label], self.checkpoint
+        )
 
-    def discard_samples(self, number):
-        """Remove the first n samples
+    @staticmethod
+    def _waveform_td_plot(
+        savedir, detectors, maxL_samples, label, preliminary=False,
+        checkpoint=False
+    ):
+        """Generate a time domain waveform plot for a given detector network
+        and set of samples
 
         Parameters
         ----------
-        number: int
-            Number of samples that you wish to remove
+        savedir: str
+            the directory you wish to save the plot in
+        detectors: list
+            list of detectors used in your analysis
+        maxL_samples: dict
+            dictionary of maximum likelihood values
+        label: str
+            the label corresponding to the results file
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
-        self.make_dictionary(discard_samples=number)
-        return self
-
-    def make_dictionary(self, discard_samples=None):
-        """Add the parameters and samples to the class
-        """
-        if "log_likelihood" in self.parameters:
-            likelihoods = self.samples[self.parameters.index("log_likelihood")]
-            likelihoods = likelihoods[discard_samples:]
-        else:
-            likelihoods = None
-        if "log_prior" in self.parameters:
-            priors = self.samples[self.parameters.index("log_prior")]
-            priors = priors[discard_samples:]
-        else:
-            priors = None
-        if any(i in self.parameters for i in ["weights", "weight"]):
-            ind = (
-                self.parameters.index("weights") if "weights" in self.parameters
-                else self.parameters.index("weight")
-            )
-            weights = self.samples[ind][discard_samples:]
+        filename = os.path.join(
+            savedir, "{}_waveform_time_domain.png".format(label)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        if detectors is None:
+            detectors = ["H1", "L1"]
         else:
-            weights = None
-        for key, val in zip(self.parameters, self.samples):
-            self[key] = Array(
-                val[discard_samples:], likelihood=likelihoods, prior=priors,
-                weights=weights
-            )
-
-    @docstring_subfunction([
-        'pesummary.core.plots.plot._1d_histogram_plot',
-        'pesummary.gw.plots.plot._1d_histogram_plot',
-        'pesummary.gw.plots.plot._ligo_skymap_plot',
-        'pesummary.gw.plots.publication.spin_distribution_plots',
-        'pesummary.core.plots.plot._make_corner_plot',
-        'pesummary.gw.plots.plot._make_corner_plot'
-    ])
-    def plot(self, *args, type="marginalized_posterior", **kwargs):
-        """Generate a plot for the posterior samples stored in SamplesDict
+            detectors = detectors.split("_")
+
+        fig = gw._time_domain_waveform(detectors, maxL_samples)
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
+        )
+
+    def gwdata_plots(self, label):
+        """Generate all plots associated with the gwdata
 
         Parameters
         ----------
-        *args: tuple
-            all arguments are passed to the plotting function
-        type: str
-            name of the plot you wish to make
-        **kwargs: dict
-            all additional kwargs are passed to the plotting function
-        """
-        if type not in self.plotting_map.keys():
-            raise NotImplementedError(
-                "The {} method is not currently implemented. The allowed "
-                "plotting methods are {}".format(
-                    type, ", ".join(self.available_plots)
-                )
-            )
-        return self.plotting_map[type](*args, **kwargs)
+        label: str
+            the label corresponding to the results file
+        """
+        from pesummary.utils.utils import determine_gps_time_and_window
 
-    def _marginalized_posterior(self, parameter, module="core", **kwargs):
-        """Wrapper for the `pesummary.core.plots.plot._1d_histogram_plot` or
-        `pesummary.gw.plots.plot._1d_histogram_plot`
+        base_error = "Failed to generate a %s because {}"
+        gps_time, window = determine_gps_time_and_window(
+            self.maxL_samples, self.labels
+        )
+        functions = [
+            self.strain_plot, self.spectrogram_plot, self.omegascan_plot
+        ]
+        args = [[label], [], [gps_time, window]]
+        func_names = ["strain_plot", "spectrogram plot", "omegascan plot"]
+
+        for func, args, name in zip(functions, args, func_names):
+            self._try_to_make_a_plot(args, func, base_error % (name))
+            continue
+
+    def strain_plot(self, label):
+        """Generate a plot showing the comparison between the data and the
+        maxL waveform gfor a given result file
 
         Parameters
         ----------
-        parameter: str
-            name of the parameter you wish to plot
-        module: str, optional
-            module you wish to use for the plotting
-        **kwargs: dict
-            all additional kwargs are passed to the `_1d_histogram_plot`
-            function
+        label: str
+            the label corresponding to the results file
         """
-        module = importlib.import_module(
-            "pesummary.{}.plots.plot".format(module)
-        )
-        return getattr(module, "_1d_histogram_plot")(
-            parameter, self[parameter], self.latex_labels[parameter], **kwargs
+        logger.info("Launching subprocess to generate strain plot")
+        process = mp.Process(
+            target=self._strain_plot,
+            args=[self.savedir, self.gwdata, self.maxL_samples[label], label]
         )
+        process.start()
 
-    def _skymap(self, **kwargs):
-        """Wrapper for the `pesummary.gw.plots.plot._ligo_skymap_plot`
-        function
+    @staticmethod
+    def _strain_plot(savedir, gwdata, maxL_samples, label, checkpoint=False):
+        """Generate a strain plot for a given set of samples
 
         Parameters
         ----------
-        **kwargs: dict
-            All kwargs are passed to the `_ligo_skymap_plot` function
+        savedir: str
+            the directory to save the plot
+        gwdata: dict
+            dictionary of strain data for each detector
+        maxL_samples: dict
+            dictionary of maximum likelihood values
+        label: str
+            the label corresponding to the results file
         """
-        from pesummary.gw.plots.plot import _ligo_skymap_plot
+        filename = os.path.join(savedir, "{}_strain.png".format(label))
+        if os.path.isfile(filename) and checkpoint:
+            return
+        fig = gw._strain_plot(gwdata, maxL_samples)
+        _PlotGeneration.save(fig, filename)
 
-        if "luminosity_distance" in self.keys():
-            dist = self["luminosity_distance"]
-        else:
-            dist = None
+    def spectrogram_plot(self):
+        """Generate a plot showing the spectrogram for all detectors
+        """
+        figs = self._spectrogram_plot(self.savedir, self.gwdata)
 
-        return _ligo_skymap_plot(self["ra"], self["dec"], dist=dist, **kwargs)
+    @staticmethod
+    def _spectrogram_plot(savedir, strain):
+        """Generate a plot showing the spectrogram for all detectors
 
-    def _spin_disk(self, **kwargs):
-        """Wrapper for the `pesummary.gw.plots.publication.spin_distribution_plots`
-        function
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        strain: dict
+            dictionary of gwpy timeseries objects containing the strain data for
+            each IFO
         """
-        from pesummary.gw.plots.publication import spin_distribution_plots
+        from pesummary.gw.plots import detchar
 
-        required = ["a_1", "a_2", "cos_tilt_1", "cos_tilt_2"]
-        if not all(param in self.keys() for param in required):
-            raise ValueError(
-                "The spin disk plot requires samples for the following "
-                "parameters: {}".format(", ".join(required))
+        figs = detchar.spectrogram(strain)
+        for det, fig in figs.items():
+            _PlotGeneration.save(
+                fig, os.path.join(savedir, "spectrogram_{}".format(det))
             )
-        samples = [self[param] for param in required]
-        return spin_distribution_plots(required, samples, None, **kwargs)
 
-    def _corner(self, module="core", parameters=None, **kwargs):
-        """Wrapper for the `pesummary.core.plots.plot._make_corner_plot` or
-        `pesummary.gw.plots.plot._make_corner_plot` function
+    def omegascan_plot(self, gps_time, window):
+        """Generate a plot showing the omegascan for all detectors
 
         Parameters
         ----------
-        module: str, optional
-            module you wish to use for the plotting
-        **kwargs: dict
-            all additional kwargs are passed to the `_make_corner_plot`
-            function
+        gps_time: float
+            time around which to centre the omegascan
+        window: float
+            window around gps time to generate plot for
         """
-        module = importlib.import_module(
-            "pesummary.{}.plots.plot".format(module)
+        figs = self._omegascan_plot(
+            self.savedir, self.gwdata, gps_time, window
         )
-        _parameters = None
-        if parameters is not None:
-            _parameters = [param for param in parameters if param in self.keys()]
-            if not len(_parameters):
-                raise ValueError(
-                    "None of the chosen parameters are in the posterior "
-                    "samples table. Please choose other parameters to plot"
-                )
-        return getattr(module, "_make_corner_plot")(
-            self, self.latex_labels, corner_parameters=_parameters, **kwargs
-        )[0]
 
-    def classification(self, prior=None):
-        """Return the classification probabilities
+    @staticmethod
+    def _omegascan_plot(savedir, strain, gps, window):
+        """Generate a plot showing the spectrogram for all detectors
 
         Parameters
         ----------
-        prior: str, optional
-            prior you wish to use when generating the classification
-            probabilities.
+        savedir: str
+            the directory you wish to save the plot in
+        strain: dict
+            dictionary of gwpy timeseries objects containing the strain data for
+            each IFO
+        gps: float
+            time around which to centre the omegascan
+        window: float
+            window around gps time to generate plot for
         """
-        from pesummary.gw.pepredicates import get_classifications
-        from pesummary.gw.p_astro import get_probabilities
-
-        _prior = ["default", "population", None]
-        if prior not in _prior:
-            raise ValueError(
-                "Unrecognised prior. Prior must be either: {}".format(
-                    ", ".join(_prior)
-                )
-            )
-        classifications = get_classifications(self)
-        embright = get_probabilities(self)
-        classifications["default"].update(embright[0])
-        classifications["population"].update(embright[1])
-        if prior is not None:
-            return classifications[prior]
-        return classifications
-
-
-class _MultiDimensionalSamplesDict(dict):
-    """Class to store multiple SamplesDict objects
-
-    Parameters
-    ----------
-    parameters: list
-        list of parameters
-    samples: nd list
-        list of samples for each parameter for each chain
-    label_prefix: str, optional
-        prefix to use when distinguishing different analyses. The label is then
-        '{label_prefix}_{num}' where num is the result file index. Default
-        is 'dataset'
-    transpose: Bool, optional
-        True if the input is a transposed dictionary
-    labels: list, optional
-        the labels to use to distinguish different analyses. If provided
-        label_prefix is ignored
-
-    Attributes
-    ----------
-    T: pesummary.utils.samples_dict._MultiDimensionalSamplesDict
-        Transposed _MultiDimensionalSamplesDict object keyed by parameters
-        rather than label
-    combine: pesummary.utils.samples_dict.SamplesDict
-        Combine all samples from all analyses into a single SamplesDict object
-    nsamples: int
-        Total number of analyses stored in the _MultiDimensionalSamplesDict
-        object
-    number_of_samples: dict
-        Number of samples stored in the _MultiDimensionalSamplesDict for each
-        analysis
-    total_number_of_samples: int
-        Total number of samples stored across the multiple analyses
-    minimum_number_of_samples: int
-        The number of samples in the smallest analysis
-
-    Methods
-    -------
-    samples:
-        Return a list of samples stored in the _MultiDimensionalSamplesDict
-        object for a given parameter
-    """
-    def __init__(
-        self, *args, label_prefix="dataset", transpose=False, labels=None
-    ):
-        if labels is not None and len(np.unique(labels)) != len(labels):
-            raise ValueError(
-                "Please provide a unique set of labels for each analysis"
-            )
-        invalid_label_number_error = "Please provide a label for each analysis"
-        self.labels = labels
-        self.name = _MultiDimensionalSamplesDict
-        self.transpose = transpose
-        if len(args) == 1 and isinstance(args[0], dict):
-            if transpose:
-                parameters = list(args[0].keys())
-                _labels = list(args[0][parameters[0]].keys())
-                outer_iterator, inner_iterator = parameters, _labels
-            else:
-                _labels = list(args[0].keys())
-                parameters = {
-                    label: list(args[0][label].keys()) for label in _labels
-                }
-                outer_iterator, inner_iterator = _labels, parameters
-            if labels is None:
-                self.labels = _labels
-            for num, dataset in enumerate(outer_iterator):
-                if isinstance(inner_iterator, dict):
-                    samples = np.array(
-                        [args[0][dataset][param] for param in inner_iterator[dataset]]
-                    )
-                else:
-                    samples = np.array(
-                        [args[0][dataset][param] for param in inner_iterator]
-                    )
-                if transpose:
-                    desc = parameters[num]
-                    self[desc] = SamplesDict(
-                        self.labels, samples, logger_warn="debug",
-                        autoscale=False
-                    )
-                else:
-                    if self.labels is not None:
-                        desc = self.labels[num]
-                    else:
-                        desc = "{}_{}".format(label_prefix, num)
-                    self[desc] = SamplesDict(parameters[self.labels[num]], samples)
-        else:
-            parameters, samples = args
-            if labels is not None and len(labels) != len(samples):
-                raise ValueError(invalid_label_number_error)
-            for num, dataset in enumerate(samples):
-                if labels is not None:
-                    desc = labels[num]
-                else:
-                    desc = "{}_{}".format(label_prefix, num)
-                self[desc] = SamplesDict(parameters, dataset)
-        if self.labels is None:
-            self.labels = [
-                "{}_{}".format(label_prefix, num) for num, _ in
-                enumerate(samples)
-            ]
-        self.parameters = parameters
-        self.latex_labels = {
-            param: latex_labels[param] if param in latex_labels.keys() else
-            param for param in self.total_list_of_parameters
-        }
-
-    @property
-    def T(self):
-        _params = sorted([param for param in self[self.labels[0]].keys()])
-        if not all(sorted(self[label].keys()) == _params for label in self.labels):
-            raise ValueError(
-                "Unable to transpose as not all samples have the same parameters"
-            )
-        return self.name({
-            param: {
-                label: dataset[param] for label, dataset in self.items()
-            } for param in self[self.labels[0]].keys()
-        }, transpose=True)
+        from pesummary.gw.plots import detchar
 
-    @property
-    def combine(self):
-        if self.transpose:
-            data = SamplesDict({
-                param: np.concatenate(
-                    [self[param][key] for key in self[param].keys()]
-                ) for param in self.parameters
-            }, logger_warn="debug")
-        else:
-            data = SamplesDict({
-                param: np.concatenate(
-                    [self[key][param] for key in self.keys()]
-                ) for param in self.parameters
-            }, logger_warn="debug")
-        return data
-
-    @property
-    def nsamples(self):
-        if self.transpose:
-            parameters = list(self.keys())
-            return len(self[parameters[0]])
-        return len(self)
-
-    @property
-    def number_of_samples(self):
-        if self.transpose:
-            return {
-                label: len(self[iterator][label]) for iterator, label in zip(
-                    self.keys(), self.labels
-                )
-            }
-        return {
-            label: self[iterator].number_of_samples for iterator, label in zip(
-                self.keys(), self.labels
+        figs = detchar.omegascan(strain, gps, window=window)
+        for det, fig in figs.items():
+            _PlotGeneration.save(
+                fig, os.path.join(savedir, "omegascan_{}".format(det))
             )
-        }
-
-    @property
-    def total_number_of_samples(self):
-        return np.sum([length for length in self.number_of_samples.values()])
 
-    @property
-    def minimum_number_of_samples(self):
-        return np.min([length for length in self.number_of_samples.values()])
-
-    @property
-    def total_list_of_parameters(self):
-        if isinstance(self.parameters, dict):
-            _parameters = [item for item in self.parameters.values()]
-            _flat_parameters = [
-                item for sublist in _parameters for item in sublist
-            ]
-        elif isinstance(self.parameters, list):
-            if np.array(self.parameters).ndim > 1:
-                _flat_parameters = [
-                    item for sublist in self.parameters for item in sublist
-                ]
-            else:
-                _flat_parameters = self.parameters
-        return list(set(_flat_parameters))
-
-    def samples(self, parameter):
-        if self.transpose:
-            samples = [self[parameter][label] for label in self.labels]
-        else:
-            samples = [self[label][parameter] for label in self.labels]
-        return samples
-
-
-class MCMCSamplesDict(_MultiDimensionalSamplesDict):
-    """Class to store the mcmc chains from a single run
-
-    Parameters
-    ----------
-    parameters: list
-        list of parameters
-    samples: nd list
-        list of samples for each parameter for each chain
-    transpose: Bool, optional
-        True if the input is a transposed dictionary
-
-    Attributes
-    ----------
-    T: pesummary.utils.samples_dict.MCMCSamplesDict
-        Transposed MCMCSamplesDict object keyed by parameters rather than
-        chain
-    average: pesummary.utils.samples_dict.SamplesDict
-        The mean of each sample across multiple chains. If the chains are of
-        different lengths, all chains are resized to the minimum number of
-        samples
-    combine: pesummary.utils.samples_dict.SamplesDict
-        Combine all samples from all chains into a single SamplesDict object
-    nchains: int
-        Total number of chains stored in the MCMCSamplesDict object
-    number_of_samples: dict
-        Number of samples stored in the MCMCSamplesDict for each chain
-    total_number_of_samples: int
-        Total number of samples stored across the multiple chains
-    minimum_number_of_samples: int
-        The number of samples in the smallest chain
-
-    Methods
-    -------
-    discard_samples:
-        Discard the first N samples for each chain
-    burnin:
-        Remove the first N samples as burnin. For different algorithms
-        see pesummary.core.file.mcmc.algorithms
-    gelman_rubin: float
-        Return the Gelman-Rubin statistic between the chains for a given
-        parameter. See pesummary.utils.utils.gelman_rubin
-    samples:
-        Return a list of samples stored in the MCMCSamplesDict object for a
-        given parameter
-
-    Examples
-    --------
-    Initializing the MCMCSamplesDict class
-
-    >>> from pesummary.utils.samplesdict import MCMCSamplesDict
-    >>> data = {
-    ...     "chain_0": {
-    ...         "a": [1, 1.2, 1.7, 1.1, 1.4, 0.8, 1.6],
-    ...         "b": [10.2, 11.3, 11.6, 9.5, 8.6, 10.8, 10.9]
-    ...     },
-    ...     "chain_1": {
-    ...         "a": [0.8, 0.5, 1.7, 1.4, 1.2, 1.7, 0.9],
-    ...         "b": [10, 10.5, 10.4, 9.6, 8.6, 11.6, 16.2]
-    ...     }
-    ... }
-    >>> dataset = MCMCSamplesDict(data)
-    >>> parameters = ["a", "b"]
-    >>> samples = [
-    ...     [
-    ...         [1, 1.2, 1.7, 1.1, 1.4, 0.8, 1.6],
-    ...         [10.2, 11.3, 11.6, 9.5, 8.6, 10.8, 10.9]
-    ...     ], [
-    ...         [0.8, 0.5, 1.7, 1.4, 1.2, 1.7, 0.9],
-    ...         [10, 10.5, 10.4, 9.6, 8.6, 11.6, 16.2]
-    ...     ]
-    ... ]
-    >>> dataset = MCMCSamplesDict(parameter, samples)
-    """
-    def __init__(self, *args, transpose=False):
-        single_chain_error = (
-            "This class requires more than one mcmc chain to be passed. "
-            "As only one dataset is available, please use the SamplesDict "
-            "class."
-        )
-        super(MCMCSamplesDict, self).__init__(
-            *args, transpose=transpose, label_prefix="chain"
-        )
-        self.name = MCMCSamplesDict
-        if len(self.labels) == 1:
-            raise ValueError(single_chain_error)
-        self.chains = self.labels
-        self.nchains = self.nsamples
-
-    @property
-    def average(self):
-        if self.transpose:
-            data = SamplesDict({
-                param: np.mean(
-                    [
-                        self[param][key][:self.minimum_number_of_samples] for
-                        key in self[param].keys()
-                    ], axis=0
-                ) for param in self.parameters
-            }, logger_warn="debug")
-        else:
-            data = SamplesDict({
-                param: np.mean(
-                    [
-                        self[key][param][:self.minimum_number_of_samples] for
-                        key in self.keys()
-                    ], axis=0
-                ) for param in self.parameters
-            }, logger_warn="debug")
-        return data
-
-    def discard_samples(self, number):
-        """Remove the first n samples
-
-        Parameters
-        ----------
-        number: int/dict
-            Number of samples that you wish to remove across all chains or a
-            dictionary containing the number of samples to remove per chain
-        """
-        if isinstance(number, int):
-            number = {chain: number for chain in self.keys()}
-        for chain in self.keys():
-            self[chain].discard_samples(number[chain])
-        return self
-
-    def burnin(self, *args, algorithm="burnin_by_step_number", **kwargs):
-        """Remove the first N samples as burnin
-
-        Parameters
-        ----------
-        algorithm: str, optional
-            The algorithm you wish to use to remove samples as burnin. Default
-            is 'burnin_by_step_number'. See
-            `pesummary.core.file.mcmc.algorithms` for list of available
-            algorithms
-        """
-        from pesummary.core.file import mcmc
-
-        if algorithm not in mcmc.algorithms:
-            raise ValueError(
-                "{} is not a valid algorithm for removing samples as "
-                "burnin".format(algorithm)
-            )
-        arguments = [self] + [i for i in args]
-        return getattr(mcmc, algorithm)(*arguments, **kwargs)
-
-    def gelman_rubin(self, parameter, decimal=5):
-        """Return the gelman rubin statistic between chains for a given
-        parameter
-
-        Parameters
-        ----------
-        parameter: str
-            name of the parameter you wish to return the gelman rubin statistic
-            for
-        decimal: int
-            number of decimal places to keep when rounding
-        """
-        from pesummary.utils.utils import gelman_rubin as _gelman_rubin
-
-        return _gelman_rubin(self.samples(parameter), decimal=decimal)
-
-
-class MultiAnalysisSamplesDict(_MultiDimensionalSamplesDict):
-    """Class to samples from multiple analyses
-
-    Parameters
-    ----------
-    parameters: list
-        list of parameters
-    samples: nd list
-        list of samples for each parameter for each chain
-    labels: list, optional
-        the labels to use to distinguish different analyses.
-    transpose: Bool, optional
-        True if the input is a transposed dictionary
-
-    Attributes
-    ----------
-    T: pesummary.utils.samples_dict.MultiAnalysisSamplesDict
-        Transposed MultiAnalysisSamplesDict object keyed by parameters
-        rather than label
-    combine: pesummary.utils.samples_dict.SamplesDict
-        Combine all samples from all analyses into a single SamplesDict object
-    nsamples: int
-        Total number of analyses stored in the MultiAnalysisSamplesDict
-        object
-    number_of_samples: dict
-        Number of samples stored in the MultiAnalysisSamplesDict for each
-        analysis
-    total_number_of_samples: int
-        Total number of samples stored across the multiple analyses
-    minimum_number_of_samples: int
-        The number of samples in the smallest analysis
-    available_plots: list
-        list of plots which the user may user to display the contained posterior
-        samples
-
-    Methods
-    -------
-    from_files:
-        Initialize the MultiAnalysisSamplesDict class with the contents of
-        multiple files
-    js_divergence: float
-        Return the JS divergence between two posterior distributions for a
-        given parameter. See pesummary.utils.utils.jensen_shannon_divergence
-    ks_statistic: float
-        Return the KS statistic between two posterior distributions for a
-        given parameter. See pesummary.utils.utils.kolmogorov_smirnov_test
-    samples:
-        Return a list of samples stored in the MCMCSamplesDict object for a
-        given parameter
-    """
-    def __init__(self, *args, labels=None, transpose=False):
-        if labels is None and not isinstance(args[0], dict):
-            raise ValueError(
-                "Please provide a unique label for each analysis"
-            )
-        super(MultiAnalysisSamplesDict, self).__init__(
-            *args, labels=labels, transpose=transpose
-        )
-        self.name = MultiAnalysisSamplesDict
-
-    @classmethod
-    def from_files(cls, filenames, **kwargs):
-        """Initialize the MultiAnalysisSamplesDict class with the contents of
-        multiple result files
+    def skymap_comparison_plot(self, label):
+        """Generate a plot to compare skymaps for all result files
 
         Parameters
         ----------
-        filenames: dict
-            dictionary containing the path to the result file you wish to load
-            as the item and a label associated with each result file as the key
-        **kwargs: dict
-            all kwargs are passed to the pesummary.io.read function
-        """
-        from pesummary.io import read
-        from pesummary.core.inputs import _Input
-
-        samples = {}
-        for label, filename in filenames.items():
-            _samples = read(filename, **kwargs).samples_dict
-            if _Input.is_pesummary_metafile(filename):
-                _stored_labels = _samples.keys()
-                cond1 = any(
-                    _label in filenames.keys() for _label in _stored_labels
-                )
-                cond2 = any(
-                    _label in samples.keys() for _label in _stored_labels
-                )
-                if cond1 or cond2:
-                    raise ValueError(
-                        "One or more of the labels stored in the PESummary "
-                        "meta file matches another label. Please provide unique "
-                        "labels for each dataset"
-                    )
-                samples.update(_samples)
-            else:
-                samples[label] = _samples
-        return cls(samples)
-
-    @property
-    def plotting_map(self):
-        return {
-            "hist": self._marginalized_posterior,
-            "corner": self._corner,
-            "triangle": self._triangle,
-            "reverse_triangle": self._reverse_triangle,
-            "violin": self._violin
-        }
-
-    @property
-    def available_plots(self):
-        return list(self.plotting_map.keys())
+        label: str
+            the label for the results file that you wish to plot
+        """
+        self._skymap_comparison_plot(
+            self.savedir, self.same_samples["ra"], self.same_samples["dec"],
+            self.labels, self.colors, self.preliminary_comparison_pages,
+            self.checkpoint
+        )
 
-    @docstring_subfunction([
-        'pesummary.core.plots.plot._1d_comparison_histogram_plot',
-        'pesummary.gw.plots.plot._1d_comparison_histogram_plot',
-        'pesummary.core.plots.publication.triangle_plot',
-        'pesummary.core.plots.publication.reverse_triangle_plot'
-    ])
-    def plot(
-        self, *args, type="hist", labels="all", colors=None, latex_friendly=True,
-        **kwargs
+    @staticmethod
+    def _skymap_comparison_plot(
+        savedir, ra, dec, labels, colors, preliminary=False, checkpoint=False
     ):
-        """Generate a plot for the posterior samples stored in
-        MultiDimensionalSamplesDict
+        """Generate a plot to compare skymaps for a given set of samples
 
         Parameters
         ----------
-        *args: tuple
-            all arguments are passed to the plotting function
-        type: str
-            name of the plot you wish to make
+        savedir: str
+            the directory you wish to save the plot in
+        ra: dict
+            dictionary of right ascension samples for each result file
+        dec: dict
+            dictionary of declination samples for each result file
         labels: list
-            list of analyses that you wish to include in the plot
+            list of labels to distinguish each result file
         colors: list
-            list of colors to use for each analysis
-        latex_friendly: Bool, optional
-            if True, make the labels latex friendly. Default True
-        **kwargs: dict
-            all additional kwargs are passed to the plotting function
-        """
-        if type not in self.plotting_map.keys():
-            raise NotImplementedError(
-                "The {} method is not currently implemented. The allowed "
-                "plotting methods are {}".format(
-                    type, ", ".join(self.available_plots)
-                )
-            )
-
-        if labels == "all":
-            labels = self.labels
-        elif isinstance(labels, list):
-            for label in labels:
-                if label not in self.labels:
-                    raise ValueError(
-                        "'{}' is not a stored analysis. The available analyses "
-                        "are: '{}'".format(label, ", ".join(self.labels))
-                    )
-        else:
-            raise ValueError(
-                "Please provide a list of analyses that you wish to plot"
-            )
-        if colors is None:
-            colors = list(conf.colorcycle)
-            while len(colors) < len(labels):
-                colors += colors
-
-        kwargs["labels"] = labels
-        kwargs["colors"] = colors
-        kwargs["latex_friendly"] = latex_friendly
-        return self.plotting_map[type](*args, **kwargs)
+            list of colors to be used to distinguish different result files
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        filename = os.path.join(savedir, "combined_skymap.png")
+        if os.path.isfile(filename) and checkpoint:
+            return
+        ra_list = [ra[key] for key in labels]
+        dec_list = [dec[key] for key in labels]
+        fig = gw._sky_map_comparison_plot(ra_list, dec_list, labels, colors)
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
+        )
 
-    def _marginalized_posterior(
-        self, parameter, module="core", labels="all", colors=None, **kwargs
-    ):
-        """Wrapper for the
-        `pesummary.core.plots.plot._1d_comparison_histogram_plot` or
-        `pesummary.gw.plots.plot._comparison_1d_histogram_plot`
+    def waveform_comparison_fd_plot(self, label):
+        """Generate a plot to compare the frequency domain waveform
 
         Parameters
         ----------
-        parameter: str
-            name of the parameter you wish to plot
-        module: str, optional
-            module you wish to use for the plotting
-        labels: list
-            list of analyses that you wish to include in the plot
-        colors: list
-            list of colors to use for each analysis
-        **kwargs: dict
-            all additional kwargs are passed to the
-            `_1d_comparison_histogram_plot` function
+        label: str
+            the label for the results file that you wish to plot
         """
-        module = importlib.import_module(
-            "pesummary.{}.plots.plot".format(module)
-        )
-        return getattr(module, "_1d_comparison_histogram_plot")(
-            parameter, [self[label][parameter] for label in labels],
-            colors, self.latex_labels[parameter], labels, **kwargs
+        if any(self.approximant[i] == {} for i in self.labels):
+            return
+
+        self._waveform_comparison_fd_plot(
+            self.savedir, self.maxL_samples, self.labels, self.colors,
+            self.preliminary_comparison_pages, self.checkpoint
         )
 
-    def _base_triangle(self, parameters, labels="all"):
-        """Check that the parameters are valid for the different triangle
-        plots available
+    @staticmethod
+    def _waveform_comparison_fd_plot(
+        savedir, maxL_samples, labels, colors, preliminary=False,
+        checkpoint=False
+    ):
+        """Generate a plot to compare the frequency domain waveforms
 
         Parameters
         ----------
-        parameters: list
-            list of parameters they wish to study
+        savedir: str
+            the directory you wish to save the plot in
+        maxL_samples: dict
+            dictionary of maximum likelihood samples for each result file
         labels: list
-            list of analyses that you wish to include in the plot
+            list of labels to distinguish each result file
+        colors: list
+            list of colors to be used to distinguish different result files
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
-        samples = [self[label] for label in labels]
-        if len(parameters) > 2:
-            raise ValueError("Function is only 2d")
-        condition = set(
-            label for num, label in enumerate(labels) for param in parameters if
-            param not in samples[num].keys()
-        )
-        if len(condition):
-            raise ValueError(
-                "{} and {} are not available for the following "
-                " analyses: {}".format(
-                    parameters[0], parameters[1], ", ".join(condition)
-                )
-            )
-        return samples
+        filename = os.path.join(savedir, "compare_waveforms.png")
+        if os.path.isfile(filename) and checkpoint:
+            return
+        samples = [maxL_samples[i] for i in labels]
+        fig = gw._waveform_comparison_plot(samples, colors, labels)
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
+        )
 
-    def _triangle(self, parameters, labels="all", **kwargs):
-        """Wrapper for the `pesummary.core.plots.publication.triangle_plot`
-        function
+    def waveform_comparison_td_plot(self, label):
+        """Generate a plot to compare the time domain waveform
 
         Parameters
         ----------
-        parameters: list
-            list of parameters they wish to study
-        labels: list
-            list of analyses that you wish to include in the plot
-        **kwargs: dict
-            all additional kwargs are passed to the `triangle_plot` function
+        label: str
+            the label for the results file that you wish to plot
         """
-        from pesummary.core.plots.publication import triangle_plot
+        if any(self.approximant[i] == {} for i in self.labels):
+            return
 
-        samples = self._base_triangle(parameters, labels=labels)
-        return triangle_plot(
-            [_samples[parameters[0]] for _samples in samples],
-            [_samples[parameters[1]] for _samples in samples],
-            xlabel=self.latex_labels[parameters[0]],
-            ylabel=self.latex_labels[parameters[1]], labels=labels, **kwargs
+        self._waveform_comparison_fd_plot(
+            self.savedir, self.maxL_samples, self.labels, self.colors,
+            self.preliminary_comparison_pages, self.checkpoint
         )
 
-    def _reverse_triangle(self, parameters, labels="all", **kwargs):
-        """Wrapper for the `pesummary.core.plots.publication.reverse_triangle_plot`
-        function
+    @staticmethod
+    def _waveform_comparison_td_plot(
+        savedir, maxL_samples, labels, colors, preliminary=False,
+        checkpoint=False
+    ):
+        """Generate a plot to compare the time domain waveforms
 
         Parameters
         ----------
-        parameters: list
-            list of parameters they wish to study
+        savedir: str
+            the directory you wish to save the plot in
+        maxL_samples: dict
+            dictionary of maximum likelihood samples for each result file
         labels: list
-            list of analyses that you wish to include in the plot
-        **kwargs: dict
-            all additional kwargs are passed to the `triangle_plot` function
-        """
-        from pesummary.core.plots.publication import reverse_triangle_plot
-
-        samples = self._base_triangle(parameters, labels=labels)
-        return reverse_triangle_plot(
-            [_samples[parameters[0]] for _samples in samples],
-            [_samples[parameters[1]] for _samples in samples],
-            xlabel=self.latex_labels[parameters[0]],
-            ylabel=self.latex_labels[parameters[1]], labels=labels, **kwargs
-        )
-
-    def _violin(
-        self, parameter, labels="all", priors=None, latex_labels=GWlatex_labels,
-        **kwargs
-    ):
-        """Wrapper for the `pesummary.gw.plots.publication.violin_plots`
-        function
+            list of labels to distinguish each result file
+        colors: list
+            list of colors to be used to distinguish different result files
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        filename = os.path.join(savedir, "compare_time_domain_waveforms.png")
+        if os.path.isfile(filename) and checkpoint:
+            return
+        samples = [maxL_samples[i] for i in labels]
+        fig = gw._time_domainwaveform_comparison_plot(samples, colors, labels)
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
+        )
+
+    def twod_comparison_contour_plot(self, label):
+        """Generate 2d comparison contour plots
 
         Parameters
         ----------
-        parameter: str, optional
-            name of the parameter you wish to generate a violin plot for
-        labels: list
-            list of analyses that you wish to include in the plot
-        priors: MultiAnalysisSamplesDict, optional
-            prior samples for each analysis. If provided, the right hand side
-            of each violin will show the prior
-        latex_labels: dict, optional
-            dictionary containing the latex label associated with parameter
-        **kwargs: dict
-            all additional kwargs are passed to the `violin_plots` function
-        """
-        from pesummary.gw.plots.publication import violin_plots
-
-        _labels = [label for label in labels if parameter in self[label].keys()]
-        if not len(_labels):
-            raise ValueError(
-                "{} is not in any of the posterior samples tables. Please "
-                "choose another parameter to plot".format(parameter)
-            )
-        elif len(_labels) != len(labels):
-            no = list(set(labels) - set(_labels))
-            logger.warn(
-                "Unable to generate a violin plot for {} because {} is not "
-                "in their posterior samples table".format(
-                    " or ".join(no), parameter
-                )
-            )
-        samples = [self[label][parameter] for label in _labels]
-        if priors is not None and not all(
-                label in priors.keys() for label in _labels
-        ):
-            raise ValueError("Please provide prior samples for all labels")
-        elif priors is not None and not all(
-                parameter in priors[label].keys() for label in _labels
-        ):
-            raise ValueError(
-                "Please provide prior samples for {} for all labels".format(
-                    parameter
+        label: str
+            the label for the results file that you wish to plot
+        """
+        error_message = (
+            "Failed to generate a 2d contour plot for %s because {}"
+        )
+        twod_plots = [
+            ["mass_ratio", "chi_eff"], ["mass_1", "mass_2"],
+            ["luminosity_distance", "chirp_mass_source"],
+            ["mass_1_source", "mass_2_source"],
+            ["theta_jn", "luminosity_distance"],
+            ["network_optimal_snr", "chirp_mass_source"]
+        ]
+        gridsize = (
+            int(self.publication_kwargs["gridsize"]) if "gridsize" in
+            self.publication_kwargs.keys() else 100
+        )
+        for plot in twod_plots:
+            if not all(
+                all(
+                    i in self.samples[j].keys() for i in plot
+                ) for j in self.labels
+            ):
+                logger.warning(
+                    "Failed to generate 2d contour plots for {} because {} are not "
+                    "common in all result files".format(
+                        " and ".join(plot), " and ".join(plot)
+                    )
                 )
+                continue
+            samples = [[self.samples[i][j] for j in plot] for i in self.labels]
+            arguments = [
+                self.savedir, plot, samples, self.labels, latex_labels,
+                self.colors, self.linestyles, gridsize,
+                self.preliminary_comparison_pages, self.checkpoint
+            ]
+            self._try_to_make_a_plot(
+                arguments, self._twod_comparison_contour_plot,
+                error_message % (" and ".join(plot))
             )
-        elif priors is not None:
-            from pesummary.gw.plots.violin import split_dataframe
 
-            priors = [priors[label][parameter] for label in _labels]
-            samples = split_dataframe(samples, priors, _labels)
-            palette = kwargs.get("palette", None)
-            left, right = "color: white", "pastel"
-            if palette is not None and not isinstance(palette, dict):
-                right = palette
-            elif palette is not None and all(
-                    side in palette.keys() for side in ["left", "right"]
-            ):
-                left, right = palette["left"], palette["right"]
-            kwargs.update(
-                {
-                    "split": True, "x": "label", "y": "data", "hue": "side",
-                    "palette": {"right": right, "left": left}
-                }
+    @staticmethod
+    def _twod_comparison_contour_plot(
+        savedir, plot_parameters, samples, labels, latex_labels, colors,
+        linestyles, gridsize, preliminary=False, checkpoint=False
+    ):
+        """Generate a 2d comparison contour plot for a given set of samples
+
+        Parameters
+        ----------
+        savedir: str
+            the directory you wish to save the plot in
+        plot_parameters: list
+            list of parameters to use for the 2d contour plot
+        samples: list
+            list of samples for each parameter
+        labels: list
+            list of labels used to distinguish each result file
+        latex_labels: dict
+            dictionary containing the latex labels for each parameter
+        gridsize: int
+            the number of points to use when estimating the KDE
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        filename = os.path.join(
+            savedir, "publication", "2d_contour_plot_{}.png".format(
+                "_and_".join(plot_parameters)
             )
-        return violin_plots(
-            parameter, samples, _labels, latex_labels, **kwargs
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        fig = publication.twod_contour_plots(
+            plot_parameters, samples, labels, latex_labels, colors=colors,
+            linestyles=linestyles, gridsize=gridsize
+        )
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
         )
 
-    def _corner(self, module="core", labels="all", parameters=None, **kwargs):
-        """Wrapper for the `pesummary.core.plots.plot._make_comparison_corner_plot`
-        or `pesummary.gw.plots.plot._make_comparison_corner_plot` function
+    def violin_plot(self, label):
+        """Generate violin plot to compare certain parameters in all result
+        files
 
         Parameters
         ----------
-        module: str, optional
-            module you wish to use for the plotting
-        labels: list
-            list of analyses that you wish to include in the plot
-        **kwargs: dict
-            all additional kwargs are passed to the `_make_comparison_corner_plot`
-            function
-        """
-        module = importlib.import_module(
-            "pesummary.{}.plots.plot".format(module)
-        )
-        _samples = {label: self[label] for label in labels}
-        _parameters = None
-        if parameters is not None:
-            _parameters = [
-                param for param in parameters if all(
-                    param in posterior for posterior in _samples.values()
+        label: str
+            the label for the results file that you wish to plot
+        """
+        error_message = (
+            "Failed to generate a violin plot for %s because {}"
+        )
+        violin_plots = ["mass_ratio", "chi_eff", "chi_p", "luminosity_distance"]
+
+        for plot in violin_plots:
+            injection = [self.injection_data[label][plot] for label in self.labels]
+            if not all(plot in self.samples[j].keys() for j in self.labels):
+                logger.warning(
+                    "Failed to generate violin plots for {} because {} is not "
+                    "common in all result files".format(plot, plot)
                 )
+            samples = [self.samples[i][plot] for i in self.labels]
+            arguments = [
+                self.savedir, plot, samples, self.labels, latex_labels[plot],
+                injection, self.preliminary_comparison_pages, self.checkpoint
             ]
-            if not len(_parameters):
-                raise ValueError(
-                    "None of the chosen parameters are in all of the posterior "
-                    "samples tables. Please choose other parameters to plot"
-                )
-        return getattr(module, "_make_comparison_corner_plot")(
-            _samples, self.latex_labels, corner_parameters=_parameters, **kwargs
-        )
+            self._try_to_make_a_plot(
+                arguments, self._violin_plot, error_message % (plot)
+            )
 
-    def js_divergence(self, parameter, decimal=5):
-        """Return the JS divergence between the posterior samples for
-        a given parameter
+    @staticmethod
+    def _violin_plot(
+        savedir, plot_parameter, samples, labels, latex_label, inj_values=None,
+        preliminary=False, checkpoint=False, kde=ReflectionBoundedKDE,
+        default_bounds=True
+    ):
+        """Generate a violin plot for a given set of samples
 
         Parameters
         ----------
-        parameter: str
-            name of the parameter you wish to return the gelman rubin statistic
-            for
-        decimal: int
-            number of decimal places to keep when rounding
+        savedir: str
+            the directory you wish to save the plot in
+        plot_parameter: str
+            name of the parameter you wish to generate a violin plot for
+        samples: list
+            list of samples for each parameter
+        labels: list
+            list of labels used to distinguish each result file
+        latex_label: str
+             latex_label correspondig to parameter
+        inj_value: list
+             list of injected values for each sample
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
         """
-        from pesummary.utils.utils import jensen_shannon_divergence
-
-        return jensen_shannon_divergence(
-            self.samples(parameter), decimal=decimal
+        filename = os.path.join(
+            savedir, "publication", "violin_plot_{}.png".format(plot_parameter)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        xlow, xhigh = None, None
+        if default_bounds:
+            xlow, xhigh = gw._return_bounds(
+                plot_parameter, samples, comparison=True
+            )
+        fig = publication.violin_plots(
+            plot_parameter, samples, labels, latex_labels, kde=kde,
+            kde_kwargs={"xlow": xlow, "xhigh": xhigh}, inj_values=inj_values
+        )
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
         )
 
-    def ks_statistic(self, parameter, decimal=5):
-        """Return the KS statistic between the posterior samples for
-        a given parameter
+    def spin_dist_plot(self, label):
+        """Generate a spin disk plot to compare spins in all result
+        files
 
         Parameters
         ----------
-        parameter: str
-            name of the parameter you wish to return the gelman rubin statistic
-            for
-        decimal: int
-            number of decimal places to keep when rounding
+        label: str
+            the label for the results file that you wish to plot
         """
-        from pesummary.utils.utils import kolmogorov_smirnov_test
-
-        return kolmogorov_smirnov_test(
-            self.samples(parameter), decimal=decimal
+        error_message = (
+            "Failed to generate a spin disk plot for %s because {}"
         )
+        parameters = ["a_1", "a_2", "cos_tilt_1", "cos_tilt_2"]
+        for num, label in enumerate(self.labels):
+            if not all(i in self.samples[label].keys() for i in parameters):
+                logger.warning(
+                    "Failed to generate spin disk plots because {} are not "
+                    "common in all result files".format(
+                        " and ".join(parameters)
+                    )
+                )
+                continue
+            samples = [self.samples[label][i] for i in parameters]
+            arguments = [
+                self.savedir, parameters, samples, label, self.colors[num],
+                self.preliminary_comparison_pages, self.checkpoint
+            ]
 
+            self._try_to_make_a_plot(
+                arguments, self._spin_dist_plot, error_message % (label)
+            )
 
-class Array(np.ndarray):
-    """Class to add extra functions and methods to np.ndarray
-
-    Parameters
-    ----------
-    input_aray: list/array
-        input list/array
-
-    Attributes
-    ----------
-    median: float
-        median of the input array
-    mean: float
-        mean of the input array
-    """
-    __slots__ = [
-        "standard_deviation", "minimum", "maximum", "maxL", "maxP", "weights"
-    ]
-
-    def __new__(cls, input_array, likelihood=None, prior=None, weights=None):
-        obj = np.asarray(input_array).view(cls)
-        obj.standard_deviation = np.std(obj)
-        obj.minimum = np.min(obj)
-        obj.maximum = np.max(obj)
-        obj.maxL = cls._maxL(obj, likelihood)
-        obj.maxP = cls._maxP(obj, log_likelihood=likelihood, log_prior=prior)
-        obj.weights = weights
-        return obj
+    @staticmethod
+    def _spin_dist_plot(
+        savedir, parameters, samples, label, color, preliminary=False,
+        checkpoint=False
+    ):
+        """Generate a spin disk plot for a given set of samples
 
-    def __reduce__(self):
-        pickled_state = super(Array, self).__reduce__()
-        new_state = pickled_state[2] + tuple(
-            [getattr(self, i) for i in self.__slots__]
+        Parameters
+        ----------
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        filename = os.path.join(
+            savedir, "publication", "spin_disk_plot_{}.png".format(label)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        fig = publication.spin_distribution_plots(
+            parameters, samples, label, color=color
+        )
+        _PlotGeneration.save(
+            fig, filename, preliminary=preliminary
         )
-        return (pickled_state[0], pickled_state[1], new_state)
-
-    def __setstate__(self, state):
-        self.standard_deviation = state[-6]
-        self.minimum = state[-5]
-        self.maximum = state[-4]
-        self.maxL = state[-3]
-        self.maxP = state[-2]
-        self.weights = state[-1]
-        super(Array, self).__setstate__(state[0:-6])
 
-    def average(self, type="mean"):
-        """Return the average of the array
+    def pepredicates_plot(self, label):
+        """Generate plots with the PEPredicates package
 
         Parameters
         ----------
-        type: str
-            the method to average the array
+        label: str
+            the label for the results file that you wish to plot
         """
-        if type == "mean":
-            return self._mean(self, weights=self.weights)
-        elif type == "median":
-            return self._median(self, weights=self.weights)
+        if self.mcmc_samples:
+            samples = self.samples[label].combine
         else:
-            return None
+            samples = self.samples[label]
+        self._pepredicates_plot(
+            self.savedir, samples, label,
+            self.pepredicates_probs[label]["default"], population_prior=False,
+            preliminary=self.preliminary_pages[label], checkpoint=self.checkpoint
+        )
+        self._pepredicates_plot(
+            self.savedir, samples, label,
+            self.pepredicates_probs[label]["population"], population_prior=True,
+            preliminary=self.preliminary_pages[label], checkpoint=self.checkpoint
+        )
 
     @staticmethod
-    def _mean(array, weights=None):
-        """Compute the mean from a set of weighted samples
+    @no_latex_plot
+    def _pepredicates_plot(
+        savedir, samples, label, probabilities, population_prior=False,
+        preliminary=False, checkpoint=False
+    ):
+        """Generate a plot with the PEPredicates package for a given set of
+        samples
 
         Parameters
         ----------
-        array: np.ndarray
-            input array
-        weights: np.ndarray, optional
-            list of weights associated with each sample
-        """
-        if weights is None:
-            return np.mean(array)
-        weights = np.array(weights).flatten() / float(sum(weights))
-        return float(np.dot(np.array(array), weights))
+        savedir: str
+            the directory you wish to save the plot in
+        samples: dict
+            dictionary of samples for each parameter
+        label: str
+            the label corresponding to the result file
+        probabilities: dict
+            dictionary of classification probabilities
+        population_prior: Bool, optional
+            if True, the samples will be reweighted according to a population
+            prior
+        preliminary: Bool, optional
+            if True, add a preliminary watermark to the plot
+        """
+        from pesummary.gw.classification import PEPredicates
+
+        if not population_prior:
+            filename = os.path.join(
+                savedir, "{}_default_pepredicates.png".format(label)
+            )
+        else:
+            filename = os.path.join(
+                savedir, "{}_population_pepredicates.png".format(label)
+            )
 
-    @staticmethod
-    def _median(array, weights=None):
-        """Compute the median from a set of weighted samples
+        _pepredicates = PEPredicates(samples)
+        if os.path.isfile(filename) and checkpoint:
+            pass
+        else:
+            fig = _pepredicates.plot(
+                type="pepredicates", population=population_prior,
+                probabilities=probabilities
+            )
+            _PlotGeneration.save(
+                fig, filename, preliminary=preliminary
+            )
+
+        if not population_prior:
+            filename = os.path.join(
+                savedir, "{}_default_pepredicates_bar.png".format(label)
+            )
+        else:
+            filename = os.path.join(
+                savedir, "{}_population_pepredicates_bar.png".format(label)
+            )
+        if os.path.isfile(filename) and checkpoint:
+            pass
+        else:
+            fig = _pepredicates.plot(
+                type="bar", probabilities=probabilities,
+                population=population_prior
+            )
+            _PlotGeneration.save(
+                fig, filename, preliminary=preliminary
+            )
+
+    def psd_plot(self, label):
+        """Generate a psd plot for a given result file
 
         Parameters
         ----------
-        array: np.ndarray
-            input array
-        weights: np.ndarray, optional
-            list of weights associated with each sample
+        label: str
+            the label corresponding to the result file
         """
-        if weights is None:
-            return np.median(array)
-        return Array.percentile(array, weights=weights, percentile=0.5)
+        error_message = (
+            "Failed to generate a PSD plot for %s because {}"
+        )
+
+        fmin = None
+
+        for num, label in enumerate(self.labels):
+            if list(self.psd[label].keys()) == [None]:
+                return
+            if list(self.psd[label].keys()) == []:
+                return
+            if "f_low" in list(self.file_kwargs[label]["sampler"].keys()):
+                fmin = self.file_kwargs[label]["sampler"]["f_low"]
+            labels = list(self.psd[label].keys())
+            frequencies = [np.array(self.psd[label][i]).T[0] for i in labels]
+            strains = [np.array(self.psd[label][i]).T[1] for i in labels]
+            arguments = [
+                self.savedir, frequencies, strains, fmin, labels, label,
+                self.checkpoint
+            ]
+
+            self._try_to_make_a_plot(
+                arguments, self._psd_plot, error_message % (label)
+            )
 
     @staticmethod
-    def _maxL(array, likelihood=None):
-        """Return the maximum likelihood value of the array
+    def _psd_plot(
+        savedir, frequencies, strains, fmin, psd_labels, label, checkpoint=False
+    ):
+        """Generate a psd plot for a given set of samples
 
         Parameters
         ----------
-        array: np.ndarray
-            input array
-        likelihood: np.ndarray, optional
-            likelihoods associated with each sample
+        savedir: str
+            the directory you wish to save the plot in
+        frequencies: list
+            list of psd frequencies for each IFO
+        strains: list
+            list of psd strains for each IFO
+        fmin: float
+            frequency to start the psd plotting
+        psd_labels: list
+            list of IFOs used
+        label: str
+            the label used to distinguish the result file
         """
-        if likelihood is not None:
-            likelihood = list(likelihood)
-            ind = likelihood.index(np.max(likelihood))
-            return array[ind]
-        return None
+        filename = os.path.join(savedir, "{}_psd_plot.png".format(label))
+        if os.path.isfile(filename) and checkpoint:
+            return
+        fig = gw._psd_plot(
+            frequencies, strains, labels=psd_labels, fmin=fmin
+        )
+        _PlotGeneration.save(fig, filename)
 
-    @staticmethod
-    def _maxP(array, log_likelihood=None, log_prior=None):
-        """Return the maximum posterior value of the array
+    def calibration_plot(self, label):
+        """Generate a calibration plot for a given result file
 
         Parameters
         ----------
-        array: np.ndarray
-            input array
-        log_likelihood: np.ndarray, optional
-            log likelihoods associated with each sample
-        log_prior: np.ndarray, optional
-            log prior associated with each sample
-        """
-        if any(param is None for param in [log_likelihood, log_prior]):
-            return None
-        likelihood = np.array(log_likelihood)
-        prior = np.array(log_prior)
-        posterior = likelihood + prior
-        ind = np.argmax(posterior)
-        return array[ind]
+        label: str
+            the label corresponding to the result file
+        """
+        import numpy as np
+
+        error_message = (
+            "Failed to generate calibration plot for %s because {}"
+        )
+        frequencies = np.arange(20., 1024., 1. / 4)
+
+        for num, label in enumerate(self.labels):
+            if list(self.calibration[label].keys()) == [None]:
+                return
+            if list(self.calibration[label].keys()) == []:
+                return
+
+            ifos = list(self.calibration[label].keys())
+            calibration_data = [
+                self.calibration[label][i] for i in ifos
+            ]
+            if "calibration" in self.priors.keys():
+                prior = [self.priors["calibration"][label][i] for i in ifos]
+            else:
+                prior = None
+            arguments = [
+                self.savedir, frequencies, calibration_data, ifos, prior,
+                label, self.checkpoint
+            ]
+            self._try_to_make_a_plot(
+                arguments, self._calibration_plot, error_message % (label)
+            )
 
     @staticmethod
-    def percentile(array, weights=None, percentile=None):
-        """Compute the Nth percentile of a set of weighted samples
+    def _calibration_plot(
+        savedir, frequencies, calibration_data, calibration_labels, prior, label,
+        checkpoint=False
+    ):
+        """Generate a calibration plot for a given set of samples
 
         Parameters
         ----------
-        array: np.ndarray
-            input array
-        weights: np.ndarray, optional
-            list of weights associated with each sample
-        percentile: float, list
-            list of percentiles to compute
+        savedir: str
+            the directory you wish to save the plot in
+        frequencies: list
+            list of frequencies used to interpolate the calibration data
+        calibration_data: list
+            list of calibration data for each IFO
+        calibration_labels: list
+            list of IFOs used
+        prior: list
+            list containing the priors used for each IFO
+        label: str
+            the label used to distinguish the result file
         """
-        if weights is None:
-            return np.percentile(array, percentile)
-
-        array, weights = np.array(array), np.array(weights)
-        _type = percentile
-        if not isinstance(percentile, (list, np.ndarray)):
-            percentile = np.array([float(percentile)])
-        percentile = np.array([float(i) for i in percentile])
-        ind_sorted = np.argsort(array)
-        sorted_data = array[ind_sorted]
-        sorted_weights = weights[ind_sorted]
-        Sn = 100 * sorted_weights.cumsum() / sorted_weights.sum()
-        data = np.zeros_like(percentile)
-        for num, p in enumerate(percentile):
-            inds = np.argwhere(Sn >= p)[0]
-            data[num] = np.interp(percentile, Sn[inds], sorted_data[inds])[0]
-
-        if isinstance(_type, (int, float, np.int, np.float64, np.float32)):
-            return float(data[0])
-        return data
+        filename = os.path.join(
+            savedir, "{}_calibration_plot.png".format(label)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            return
+        fig = gw._calibration_envelope_plot(
+            frequencies, calibration_data, calibration_labels, prior=prior
+        )
+        _PlotGeneration.save(fig, filename)
 
-    def confidence_interval(self, percentile=None):
-        """Return the confidence interval of the array
+    @staticmethod
+    def _interactive_corner_plot(
+        savedir, label, samples, latex_labels, checkpoint=False
+    ):
+        """Generate an interactive corner plot for a given set of samples
 
         Parameters
         ----------
-        percentile: int/list, optional
-            Percentile or sequence of percentiles to compute, which must be
-            between 0 and 100 inclusive
+        savedir: str
+            the directory you wish to save the plot in
+        label: str
+            the label corresponding to the results file
+        samples: dict
+            dictionary containing PESummary.utils.utils.Array objects that
+            contain samples for each parameter
+        latex_labels: str
+            latex labels for each parameter in samples
         """
-        if percentile is not None:
-            if isinstance(percentile, int):
-                return self.percentile(self, self.weights, percentile)
-            return np.array(
-                [self.percentile(self, self.weights, i) for i in percentile]
-            )
-        return np.array(
-            [self.percentile(self, self.weights, i) for i in [5, 95]]
+        filename = os.path.join(
+            savedir, "corner", "{}_interactive_source.html".format(label)
         )
+        if os.path.isfile(filename) and checkpoint:
+            pass
+        else:
+            source_parameters = [
+                "luminosity_distance", "mass_1_source", "mass_2_source",
+                "total_mass_source", "chirp_mass_source", "redshift"
+            ]
+            parameters = [i for i in samples.keys() if i in source_parameters]
+            data = [samples[parameter] for parameter in parameters]
+            labels = [latex_labels[parameter] for parameter in parameters]
+            _ = interactive.corner(
+                data, labels, write_to_html_file=filename,
+                dimensions={"width": 900, "height": 900}
+            )
 
-    def __array_finalize__(self, obj):
-        if obj is None:
-            return
-        self.standard_deviation = getattr(obj, 'standard_deviation', None)
-        self.minimum = getattr(obj, 'minimum', None)
-        self.maximum = getattr(obj, 'maximum', None)
-        self.maxL = getattr(obj, 'maxL', None)
-        self.maxP = getattr(obj, 'maxP', None)
-        self.weights = getattr(obj, 'weights', None)
+        filename = os.path.join(
+            savedir, "corner", "{}_interactive_extrinsic.html".format(label)
+        )
+        if os.path.isfile(filename) and checkpoint:
+            pass
+        else:
+            extrinsic_parameters = ["luminosity_distance", "psi", "ra", "dec"]
+            parameters = [i for i in samples.keys() if i in extrinsic_parameters]
+            data = [samples[parameter] for parameter in parameters]
+            labels = [latex_labels[parameter] for parameter in parameters]
+            _ = interactive.corner(
+                data, labels, write_to_html_file=filename
+            )
```

### Comparing `pesummary-0.9.1/pesummary/utils/utils.py` & `pesummary-1.0.0/pesummary/utils/utils.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,21 +1,8 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
 import sys
 import logging
 import contextlib
 import time
 import copy
@@ -24,20 +11,28 @@
 import numpy as np
 from scipy.integrate import cumtrapz
 from scipy.interpolate import interp1d
 from scipy import stats
 import h5py
 from pesummary import conf
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 try:
     from coloredlogs import ColoredFormatter as LogFormatter
 except ImportError:
     LogFormatter = logging.Formatter
 
-CACHE_DIR = os.path.expanduser(os.path.join("~", ".cache", "pesummary"))
+CACHE_DIR = os.path.join(
+    os.getenv(
+        "XDG_CACHE_HOME",
+        os.path.expanduser(os.path.join("~", ".cache")),
+    ),
+    "pesummary",
+)
 STYLE_CACHE = os.path.join(CACHE_DIR, "style")
 LOG_CACHE = os.path.join(CACHE_DIR, "log")
 
 
 def resample_posterior_distribution(posterior, nsamples):
     """Randomly draw nsamples from the posterior distribution
 
@@ -121,15 +116,15 @@
     f.close()
 
 
 def make_dir(path):
     if os.path.isdir(os.path.expanduser(path)):
         pass
     else:
-        os.makedirs(os.path.expanduser(path))
+        os.makedirs(os.path.expanduser(path), exist_ok=True)
 
 
 def guess_url(web_dir, host, user):
     """Guess the base url from the host name
 
     Parameters
     ----------
@@ -158,74 +153,101 @@
             url = "https://ldas-jobs.phys.uwm.edu/~{}".format(user)
         elif 'phy.syr.edu' in host:
             url = "https://sugar-jobs.phy.syr.edu/~{}".format(user)
         elif 'vulcan' in host:
             url = "https://galahad.aei.mpg.de/~{}".format(user)
         elif 'atlas' in host:
             url = "https://atlas1.atlas.aei.uni-hannover.de/~{}".format(user)
-        elif 'iucca' in host:
+        elif 'iucaa' in host:
             url = "https://ldas-jobs.gw.iucaa.in/~{}".format(user)
+        elif 'alice' in host:
+            url = "https://dumpty.alice.icts.res.in/~{}".format(user)
         elif 'hawk' in host:
             url = "https://ligo.gravity.cf.ac.uk/~{}".format(user)
         else:
             url = "https://{}/~{}".format(host, user)
         url += path
     else:
         url = "https://{}".format(web_dir)
     return url
 
 
+def map_parameter_names(dictionary, mapping):
+    """Modify keys in dictionary to use different names according to a map
+
+    Parameters
+    ----------
+    mapping: dict
+        dictionary mapping existing keys to new names.
+
+    Returns
+    -------
+    standard_dict: dict
+        dict object with new parameter names
+    """
+    standard_dict = {}
+    for key, item in dictionary.items():
+        if key not in mapping.keys():
+            standard_dict[key] = item
+            continue
+        standard_dict[mapping[key]] = item
+    return standard_dict
+
+
 def command_line_arguments():
     """Return the command line arguments
     """
     return sys.argv[1:]
 
 
 def command_line_dict():
     """Return a dictionary of command line arguments
     """
-    from pesummary.core.command_line import command_line
-    from pesummary.gw.command_line import insert_gwspecific_option_group
-
-    parser = command_line()
-    insert_gwspecific_option_group(parser)
+    from pesummary.gw.cli.parser import ArgumentParser
+    parser = ArgumentParser()
+    parser.add_all_known_options_to_parser()
     opts = parser.parse_args()
     return vars(opts)
 
 
 def gw_results_file(opts):
     """Determine if a GW results file is passed
     """
-    cond1 = hasattr(opts, "gw") and opts.gw
-    cond2 = hasattr(opts, "calibration") and opts.calibration
-    cond3 = hasattr(opts, "gracedb") and opts.gracedb
-    cond4 = hasattr(opts, "approximant") and opts.approximant
-    cond5 = hasattr(opts, "psd") and opts.psd
-    if cond1 or cond2 or cond3 or cond4 or cond5:
+    from pesummary.gw.cli.parser import ArgumentParser
+
+    attrs, defaults = ArgumentParser().gw_options
+    condition = any(
+        hasattr(opts, attr) and getattr(opts, attr) and getattr(opts, attr)
+        != default for attr, default in zip(attrs, defaults)
+    )
+    if condition:
         return True
-    else:
-        return False
+    return False
 
 
-def functions(opts):
+def functions(opts, gw=False):
     """Return a dictionary of functions that are either specific to GW results
     files or core.
     """
-    from pesummary.core.inputs import Input
-    from pesummary.gw.inputs import GWInput
+    from pesummary.core.cli.inputs import (
+        WebpagePlusPlottingPlusMetaFileInput as Input
+    )
+    from pesummary.gw.cli.inputs import (
+        WebpagePlusPlottingPlusMetaFileInput as GWInput
+    )
     from pesummary.core.file.meta_file import MetaFile
     from pesummary.gw.file.meta_file import GWMetaFile
     from pesummary.core.finish import FinishingTouches
     from pesummary.gw.finish import GWFinishingTouches
 
     dictionary = {}
-    dictionary["input"] = GWInput if gw_results_file(opts) else Input
-    dictionary["MetaFile"] = GWMetaFile if gw_results_file(opts) else MetaFile
+    dictionary["input"] = GWInput if gw_results_file(opts) or gw else Input
+    dictionary["MetaFile"] = GWMetaFile if gw_results_file(opts) or gw else MetaFile
     dictionary["FinishingTouches"] = \
-        GWFinishingTouches if gw_results_file(opts) else FinishingTouches
+        GWFinishingTouches if gw_results_file(opts) or gw else FinishingTouches
     return dictionary
 
 
 def _logger_format():
     return '%(asctime)s %(name)s %(levelname)-8s: %(message)s'
 
 
@@ -297,14 +319,16 @@
                 namespace.file_versions[i] = namespace.existing_file_version[i]
         if hasattr(namespace, "file_kwargs"):
             if i not in list(namespace.file_kwargs.keys()):
                 namespace.file_kwargs[i] = namespace.existing_file_kwargs[i]
         if hasattr(namespace, "config"):
             if namespace.existing_config[num] not in namespace.config:
                 namespace.config.append(namespace.existing_config[num])
+            elif namespace.existing_config[num] is None:
+                namespace.config.append(None)
         if hasattr(namespace, "priors"):
             if hasattr(namespace, "existing_priors"):
                 for key, item in namespace.existing_priors.items():
                     if key in namespace.priors.keys():
                         for label in item.keys():
                             if label not in namespace.priors[key].keys():
                                 namespace.priors[key][label] = item[label]
@@ -335,28 +359,30 @@
         if hasattr(namespace, "maxL_samples"):
             if i not in list(namespace.maxL_samples.keys()):
                 namespace.maxL_samples[i] = {
                     key: val.maxL for key, val in namespace.samples[i].items()
                 }
         if hasattr(namespace, "pepredicates_probs"):
             if i not in list(namespace.pepredicates_probs.keys()):
-                from pesummary.gw.pepredicates import get_classifications
-
-                namespace.pepredicates_probs[i] = get_classifications(
-                    namespace.existing_samples[i]
-                )
+                from pesummary.gw.classification import PEPredicates
+                try:
+                    namespace.pepredicates_probs[i] = PEPredicates(
+                        namespace.existing_samples[i]
+                    ).dual_classification()
+                except Exception:
+                    namespace.pepredicates_probs[i] = None
         if hasattr(namespace, "pastro_probs"):
             if i not in list(namespace.pastro_probs.keys()):
-                from pesummary.gw.p_astro import get_probabilities
-
-                em_bright = get_probabilities(namespace.existing_samples[i])
-                namespace.pastro_probs[i] = {
-                    "default": em_bright[0],
-                    "population": em_bright[1]
-                }
+                from pesummary.gw.classification import PAstro
+                try:
+                    namespace.pastro_probs[i] = PAstro(
+                        namespace.existing_samples[i]
+                    ).dual_classification()
+                except Exception:
+                    namespace.pastro_probs[i] = None
     if hasattr(namespace, "result_files"):
         number = len(namespace.labels)
         while len(namespace.result_files) < number:
             namespace.result_files.append(namespace.existing_metafile)
     parameters = [list(namespace.samples[i].keys()) for i in namespace.labels]
     namespace.same_parameters = list(
         set.intersection(*[set(l) for l in parameters])
@@ -503,18 +529,18 @@
     xhigh: float
         upper bound for grid to be used
     xN: int, optional
         Number of points to use within the grid
     N: int, optional
         Number of samples to generate
     """
-    from pesummary.core.plots.bounded_1d_kde import Bounded_1d_kde
+    from pesummary.core.plots.bounded_1d_kde import ReflectionBoundedKDE
 
-    prior_KDE = Bounded_1d_kde(prior_samples)
-    posterior_KDE = Bounded_1d_kde(posterior_samples)
+    prior_KDE = ReflectionBoundedKDE(prior_samples)
+    posterior_KDE = ReflectionBoundedKDE(posterior_samples)
 
     x = np.linspace(xlow, xhigh, xN)
     idx_nz = np.nonzero(posterior_KDE(x))
     pdf_ratio = prior_KDE(x)[idx_nz] / posterior_KDE(x)[idx_nz]
     M = 1.1 / min(pdf_ratio[np.where(pdf_ratio < 1)])
 
     indicies = []
@@ -525,32 +551,35 @@
         u = np.random.uniform()
         if u < posterior_KDE(x_i) / (M * prior_KDE(x_i)):
             indicies.append(idx_i)
             i += 1
     return indicies
 
 
-def unzip(zip_file, outdir=".", overwrite=False):
+def unzip(zip_file, outdir=None, overwrite=False):
     """Extract the data from a zipped file and save in outdir.
 
     Parameters
     ----------
     zip_file: str
         path to the file you wish to unzip
     outdir: str, optional
-        path to the directory where you wish to save the unzipped file.
+        path to the directory where you wish to save the unzipped file. Default
+        None which means that the unzipped file is stored in CACHE_DIR
     overwrite: Bool, optional
         If True, overwrite a file that has the same name
     """
     import gzip
     import shutil
     from pathlib import Path
 
     f = Path(zip_file)
     file_name = f.stem
+    if outdir is None:
+        outdir = CACHE_DIR
     out_file = os.path.join(outdir, file_name)
     if os.path.isfile(out_file) and not overwrite:
         raise FileExistsError(
             "The file '{}' already exists. Not overwriting".format(out_file)
         )
     with gzip.open(zip_file, 'rb') as input:
         with open(out_file, 'wb') as output:
@@ -597,25 +626,27 @@
             )
         except ImportError:
             return iterable
     else:
         return iterable
 
 
-def _check_latex_install():
+def _check_latex_install(force_tex=False):
     from matplotlib import rcParams
     from distutils.spawn import find_executable
 
     original = rcParams['text.usetex']
     if find_executable("latex") is not None:
         try:
             from matplotlib.texmanager import TexManager
 
             texmanager = TexManager()
             texmanager.make_dvi(r"$mass_{1}$", 12)
+            if force_tex:
+                original = True
             rcParams["text.usetex"] = original
         except RuntimeError:
             rcParams["text.usetex"] = False
     else:
         rcParams["text.usetex"] = False
 
 
@@ -746,15 +777,25 @@
         2d list containing the 2 PDFs that you wish to compare
     decimal: int
         number of decimal places to keep when rounding
     """
     return np.round(stats.ks_2samp(*samples)[1], decimal)
 
 
-def jensen_shannon_divergence(
+def jensen_shannon_divergence(*args, **kwargs):
+    import warnings
+    warnings.warn(
+        "The jensen_shannon_divergence function has changed its name to "
+        "jensen_shannon_divergence_from_samples. jensen_shannon_divergence "
+        "may not be supported in future releases. Please update"
+    )
+    return jensen_shannon_divergence_from_samples(*args, **kwargs)
+
+
+def jensen_shannon_divergence_from_samples(
     samples, kde=stats.gaussian_kde, decimal=5, base=np.e, **kwargs
 ):
     """Calculate the JS divergence between two sets of samples
 
     Parameters
     ----------
     samples: list
@@ -765,34 +806,78 @@
         number of decimal places to round the JS divergence to
     base: float, optional
         optional base to use for the scipy.stats.entropy function. Default
         np.e
     kwargs: dict
         all kwargs are passed to the kde function
     """
-    try:
-        kernel = [kde(i, **kwargs) for i in samples]
-    except np.linalg.LinAlgError:
+    pdfs = samples_to_kde(samples, kde=kde, **kwargs)
+    return jensen_shannon_divergence_from_pdfs(pdfs, decimal=decimal, base=base)
+
+
+def jensen_shannon_divergence_from_pdfs(pdfs, decimal=5, base=np.e):
+    """Calculate the JS divergence between two distributions
+
+    Parameters
+    ----------
+    pdfs: list
+        list of length 2 containing the distributions you wish to compare
+    decimal: int, float
+        number of decimal places to round the JS divergence to
+    base: float, optional
+        optional base to use for the scipy.stats.entropy function. Default
+        np.e
+    """
+    if any(np.isnan(_).any() for _ in pdfs):
         return float("nan")
-    x = np.linspace(
-        np.min([np.min(i) for i in samples]),
-        np.max([np.max(i) for i in samples]),
-        100
-    )
-    a, b = [k(x) for k in kernel]
+    a, b = pdfs
     a = np.asarray(a)
     b = np.asarray(b)
     a /= a.sum()
     b /= b.sum()
     m = 1. / 2 * (a + b)
     kl_forward = stats.entropy(a, qk=m, base=base)
     kl_backward = stats.entropy(b, qk=m, base=base)
     return np.round(kl_forward / 2. + kl_backward / 2., decimal)
 
 
+def samples_to_kde(samples, kde=stats.gaussian_kde, **kwargs):
+    """Generate KDE for a set of samples
+
+    Parameters
+    ----------
+    samples: list
+        list containing the samples to create a KDE for. samples can also
+        be a 2d list containing samples from multiple analyses.
+    kde: func
+        function to use when calculating the kde of the samples
+    """
+    _SINGLE_ANALYSIS = False
+    if not isinstance(samples[0], (np.ndarray, list, tuple)):
+        _SINGLE_ANALYSIS = True
+        _samples = [samples]
+    else:
+        _samples = samples
+    kernel = []
+    for i in _samples:
+        try:
+            kernel.append(kde(i, **kwargs))
+        except np.linalg.LinAlgError:
+            kernel.append(None)
+    x = np.linspace(
+        np.min([np.min(i) for i in _samples]),
+        np.max([np.max(i) for i in _samples]),
+        100
+    )
+    pdfs = [k(x) if k is not None else float('nan') for k in kernel]
+    if _SINGLE_ANALYSIS:
+        return pdfs[0]
+    return pdfs
+
+
 def make_cache_style_file(style_file):
     """Make a cache directory which stores the style file you wish to use
     when plotting
 
     Parameters
     ----------
     style_file: str
@@ -857,15 +942,15 @@
     else:
         filename = default_filename.format(label)
     return filename
 
 
 def check_filename(
     default_filename="pesummary_{}.dat", outdir="./", label=None, filename=None,
-    overwrite=False
+    overwrite=False, delete_existing=False
 ):
     """Check to see if a file exists. If no filename is provided, a default
     filename is checked
 
     Parameters
     ----------
     default_filename: str, optional
@@ -886,17 +971,88 @@
     _file = os.path.join(outdir, filename)
     if os.path.isfile(_file) and not overwrite:
         raise FileExistsError(
             "The file '{}' already exists in the directory {}".format(
                 filename, outdir
             )
         )
+    if os.path.isfile(_file) and delete_existing:
+        os.remove(_file)
     return _file
 
 
+def string_match(string, substring):
+    """Return True if a string matches a substring. This substring may include
+    wildcards
+
+    Parameters
+    ----------
+    string: str
+        string you wish to match
+    substring: str
+        string you wish to match against
+    """
+    import re
+    import sre_constants
+
+    try:
+        match = re.match(re.compile(substring), string)
+        if match:
+            return True
+        return False
+    except sre_constants.error:
+        import fnmatch
+        return string_match(string, fnmatch.translate(substring))
+
+
+def glob_directory(base):
+    """Return a list of files matching base
+
+    Parameters
+    ----------
+    base: str
+        string you wish to match e.g. "./", "./*.py"
+    """
+    import glob
+    if "*" not in base:
+        base = os.path.join(base, "*")
+    return glob.glob(base)
+
+
+def list_match(list_to_match, substring, return_true=True, return_false=False):
+    """Match a list of strings to a substring. This substring may include
+    wildcards
+
+    Parameters
+    ----------
+    list_to_match: list
+        list of string you wish to match
+    substring: str, list
+        string you wish to match against or a list of string you wish to match
+        against
+    return_true: Bool, optional
+        if True, return a sublist containing only the parameters that match the
+        substring. Default True
+    """
+    match = np.ones(len(list_to_match), dtype=bool)
+    if isinstance(substring, str):
+        substring = [substring]
+
+    for _substring in substring:
+        match *= np.array(
+            [string_match(item, _substring) for item in list_to_match],
+            dtype=bool
+        )
+    if return_false:
+        return np.array(list_to_match)[~match]
+    elif return_true:
+        return np.array(list_to_match)[match]
+    return match
+
+
 class Empty(object):
     """Define an empty class which simply returns the input
     """
     def __new__(self, *args):
         return args[0]
 
 
@@ -921,14 +1077,32 @@
     }
     if command_line is not None:
         _dict["command_line"] = (
             "Generated by running the following script: {}".format(
                 command_line
             )
         )
+    else:
+        _dict["command_line"] = " ".join(sys.argv)
     if program is not None:
         _dict["program"] = program
     return _dict
 
 
+def mute_logger():
+    """Mute the PESummary logger
+    """
+    _logger = logging.getLogger('PESummary')
+    _logger.setLevel(logging.CRITICAL + 10)
+    return
+
+
+def unmute_logger():
+    """Unmute the PESummary logger
+    """
+    _logger = logging.getLogger('PESummary')
+    _logger.setLevel(logging.INFO)
+    return
+
+
 _, LOG_FILE = setup_logger()
 logger = logging.getLogger('PESummary')
```

### Comparing `pesummary-0.9.1/pesummary/utils/decorators.py` & `pesummary-1.0.0/pesummary/utils/decorators.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,27 +1,16 @@
-# Copyright (C) 2020  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import functools
 import copy
 import numpy as np
 from pesummary.utils.utils import logger
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 def open_config(index=0):
     """Open a configuration file. The function first looks for a config file
     stored as the keyword argument 'config'. If no kwarg found, one must specify
     the argument index which corresponds to the config file. Default is the 0th
     argument.
 
@@ -67,14 +56,67 @@
                 _safe_read(config, args[index])
                 args[index] = config
             return func(*args, **kwargs)
         return wrapper_function
     return decorator
 
 
+def bound_samples(minimum=-np.inf, maximum=np.inf, logger_level="debug"):
+    """Bound samples to be within a specified range. If any samples lie
+    outside of this range, we set these invalid samples to equal the value at
+    the boundary.
+
+    Parameters
+    ----------
+    minimum: float
+        lower boundary. Default -np.inf
+    maximum: float
+        upper boundary. Default np.inf
+    logger_level: str
+        level to use for any logger messages
+
+    Examples
+    --------
+    @bound_samples(minimum=-1., maximum=1., logger_level="info")
+    def random_samples():
+        return np.random.uniform(-2, 2, 10000)
+
+    >>> random_samples()
+    PESummary INFO    : 2576/10000 (25.76%) samples lie outside of the specified
+    range for the function random_samples (< -1.0). Truncating these samples to
+    -1.0.
+    PESummary INFO    : 2495/10000 (24.95%) samples lie outside of the specified
+    range for the function random_samples (> 1.0). Truncating these samples to
+    1.0.
+    """
+    def decorator(func):
+        @functools.wraps(func)
+        def wrapper_function(*args, **kwargs):
+            value = np.atleast_1d(func(*args, **kwargs))
+            _minimum_inds = np.argwhere(value < minimum)
+            _maximum_inds = np.argwhere(value > maximum)
+            zipped = zip([_minimum_inds, _maximum_inds], [minimum, maximum])
+            for invalid, bound in zipped:
+                if len(invalid):
+                    getattr(logger, logger_level)(
+                        "{}/{} ({}%) samples lie outside of the specified "
+                        "range for the function {} ({} {}). Truncating these "
+                        "samples to {}.".format(
+                            len(invalid), len(value),
+                            np.round(len(invalid) / len(value) * 100, 2),
+                            func.__name__, "<" if bound == minimum else ">",
+                            bound, bound
+                        )
+                    )
+                    value[invalid] = bound
+            return value
+        return wrapper_function
+    return decorator
+
+
 def no_latex_plot(func):
     """Turn off latex plotting for a given function
     """
     @functools.wraps(func)
     def wrapper_function(*args, **kwargs):
         from matplotlib import rcParams
 
@@ -115,21 +157,23 @@
     def wrapper_function(*args, **kwargs):
         import tempfile
         import os
 
         current_dir = os.getcwd()
         with tempfile.TemporaryDirectory(dir="./") as path:
             os.chdir(path)
-            value = func(*args, **kwargs)
-            os.chdir(current_dir)
+            try:
+                value = func(*args, **kwargs)
+            finally:
+                os.chdir(current_dir)
         return value
     return wrapper_function
 
 
-def array_input(func):
+def array_input(ignore_args=None, ignore_kwargs=None, force_return_array=False):
     """Convert the input into an np.ndarray and return either a float or a
     np.ndarray depending on what was input.
 
     Examples
     --------
     >>> @array_input
     >>> def total_mass(mass_1, mass_2):
@@ -137,44 +181,53 @@
     ...    return total_mass
     ...
     >>> print(total_mass(30, 10))
     40.0
     >>> print(total_mass([30, 3], [10, 1]))
     [40 4]
     """
-    @functools.wraps(func)
-    def wrapper_function(*args, **kwargs):
-        new_args = list(copy.deepcopy(args))
-        new_kwargs = kwargs.copy()
-        return_float = False
-        for num, arg in enumerate(args):
-            if isinstance(arg, (float, int)):
-                new_args[num] = np.array([arg])
-                return_float = True
-            elif isinstance(arg, (list, np.ndarray)):
-                new_args[num] = np.array(arg)
-            else:
-                pass
-        for key, item in kwargs.items():
-            if isinstance(item, (float, int)):
-                new_kwargs[key] = np.array([item])
-            elif isinstance(item, (list, np.ndarray)):
-                new_kwargs[key] = np.array(item)
-        value = np.array(func(*new_args, **new_kwargs))
-        if return_float:
-            new_value = copy.deepcopy(value)
-            if len(new_value) > 1:
-                new_value = np.array([arg[0] for arg in value])
-            elif new_value.ndim == 2:
-                new_value = new_value[0]
-            else:
-                new_value = float(new_value)
-            return new_value
-        return value
-    return wrapper_function
+    def _array_input(func):
+        @functools.wraps(func)
+        def wrapper_function(*args, **kwargs):
+            new_args = list(copy.deepcopy(args))
+            new_kwargs = kwargs.copy()
+            return_float = False
+            for num, arg in enumerate(args):
+                if ignore_args is not None and num in ignore_args:
+                    pass
+                elif isinstance(arg, (float, int)):
+                    new_args[num] = np.array([arg])
+                    return_float = True
+                elif isinstance(arg, (list, np.ndarray)):
+                    new_args[num] = np.array(arg)
+                else:
+                    pass
+            for key, item in kwargs.items():
+                if ignore_kwargs is not None and key in ignore_kwargs:
+                    pass
+                elif isinstance(item, (float, int)):
+                    new_kwargs[key] = np.array([item])
+                elif isinstance(item, (list, np.ndarray)):
+                    new_kwargs[key] = np.array(item)
+            output = func(*new_args, **new_kwargs)
+            if isinstance(output, dict):
+                return output
+            value = np.array(output)
+            if return_float and not force_return_array:
+                new_value = copy.deepcopy(value)
+                if len(new_value) > 1:
+                    new_value = np.array([arg[0] for arg in value])
+                elif new_value.ndim == 2:
+                    new_value = new_value[0]
+                else:
+                    new_value = float(new_value)
+                return new_value
+            return value
+        return wrapper_function
+    return _array_input
 
 
 def docstring_subfunction(*args):
     """Edit the docstring of a function to show the docstrings of subfunctions
     """
     def wrapper_function(func):
         import importlib
@@ -203,14 +256,24 @@
                 )
             )
         func.__doc__ = original_docstring
         return func
     return wrapper_function
 
 
-def deprecation(warning):
+def set_docstring(docstring):
     def wrapper_function(func):
-        import warnings
-
-        warnings.warn(warning)
+        func.__doc__ = docstring
         return func
     return wrapper_function
+
+
+def deprecation(warning):
+    def decorator(func):
+        @functools.wraps(func)
+        def wrapper_function(*args, **kwargs):
+            import warnings
+
+            warnings.warn(warning)
+            return func(*args, **kwargs)
+        return wrapper_function
+    return decorator
```

### Comparing `pesummary-0.9.1/pesummary/cli/summaryrecreate.py` & `pesummary-1.0.0/pesummary/cli/summaryrecreate.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,35 +1,24 @@
 #! /usr/bin/env python
 
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
-from pesummary.core.file.formats.pesummary import PESummary
-from pesummary.core.command_line import DelimiterSplitAction
+from pesummary.core.cli.parser import ArgumentParser as _ArgumentParser
+from pesummary.core.cli.actions import DelimiterSplitAction
 from pesummary.gw.file.read import read
 from pesummary.utils.utils import logger, make_dir
+from pesummary.utils.dict import edit_dictionary, paths_to_key
 from pesummary.utils.exceptions import InputError
+from pesummary.io import write
 
 import subprocess
 import os
-import argparse
 import copy
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 __doc__ = """This executable is used to recreate the analysis which was used
 to generate the posterior samples stored in the metafile"""
 
 
 def launch_subprocess(command_line):
     """Run a command line
 
@@ -39,49 +28,59 @@
         the command line you wish to run
     """
     logger.info("Running '{}'".format(command_line))
     process = subprocess.Popen(command_line, shell=True)
     process.wait()
 
 
-def command_line():
-    """Generate an Argument Parser object to control the command line options
-    """
-    parser = argparse.ArgumentParser(description=__doc__)
-    parser.add_argument(
-        "-s", "--samples", dest="samples", required=True,
-        help=("Path to the PESummary file which stores the analysis you wish "
-              "to recreate.")
-    )
-    parser.add_argument(
-        "--labels", dest="labels", nargs='+', default=None,
-        help="The label/labels of the analysis you wish to recreate"
-    )
-    parser.add_argument(
-        "-r", "--rundir", dest="rundir", default="./",
-        help="The run directory of the analysis"
-    )
-    parser.add_argument(
-        "-c", "--code", dest="code", default="lalinference",
-        help="The sampling software you wish to run"
-    )
-    parser.add_argument(
-        "--delimiter", dest="delimiter", default=":",
-        help="Delimiter used to seperate the existing and new quantity"
-    )
-    parser.add_argument(
-        "--config_override", action=DelimiterSplitAction, nargs='+',
-        dest="config_override", help=(
-            "Changes you wish to make to the configuration file. Must be in the "
-            "form `key:item` where key is the entry in the config file you wish "
-            "to modify, ':' is the default delimiter, and item is the string you "
-            "wish to replace with."
-        ), default={}
-    )
-    return parser
+class ArgumentParser(_ArgumentParser):
+    def _pesummary_options(self):
+        options = super(ArgumentParser, self)._pesummary_options()
+        options.update(
+            {
+                "--samples": {
+                    "short": "-s",
+                    "required": True,
+                    "help": (
+                        "Path to the PESummary file which stores the analysis "
+                        "you wish to recreate."
+                    )
+                },
+                "--rundir": {
+                    "default": "./",
+                    "help": "The run directory of the analysis",
+                    "short": "-r",
+                },
+                "--code": {
+                    "default": "lalinference",
+                    "short": "-c",
+                    "help": "The sampling software you wish to run"
+                },
+                "--delimiter": {
+                    "default": ":",
+                    "help": (
+                        "Delimiter used to seperate the existing and new "
+                        "quantity"
+                    )
+                },
+                "--config_override": {
+                    "action": DelimiterSplitAction,
+                    "nargs": "+",
+                    "default": {},
+                    "help": (
+                        "Changes you wish to make to the configuration file. "
+                        "Must be in the form `key:item` where key is the entry "
+                        "in the config file you wish to modify, ':' is the "
+                        "default delimiter, and item is the string you wish to "
+                        "replace with."
+                    )
+                },
+            }
+        )
+        return options
 
 
 class LALInference(object):
     """Class to create a LALInference analysis
     """
     @staticmethod
     def pipe(rundir, config, **kwargs):
@@ -265,26 +264,25 @@
             )
         config_files = {}
         for label in self.labels:
             config_data = copy.deepcopy(self.samples.config[label])
             if self.settings_to_change is not None:
                 for key, item in self.settings_to_change[label].items():
                     try:
-                        path, = self.samples.paths_to_key(key, config_data)
-                        config_data = self.samples.edit_dictionary(
-                            config_data, path, item
-                        )
+                        path, = paths_to_key(key, config_data)
+                        config_data = edit_dictionary(config_data, path, item)
                     except ValueError:
                         logger.warning(
                             "Unable to change '{}' to '{}' in the config "
                             "file".format(key, item)
                         )
             outdir = os.path.join(self.rundir, label)
-            PESummary.save_config_dictionary_to_file(
-                config_data, outdir=outdir, filename="config.ini"
+            write(
+                config_data, outdir=outdir, filename="config.ini",
+                file_format="ini"
             )
             logger.info("Writing the configuration file to: {}".format(outdir))
             config_files[label] = os.path.join(outdir, "config.ini")
         return config_files
 
 
 class Input(_Input):
@@ -311,15 +309,21 @@
                 self.settings_to_change[label][name] = item
         self.config = self.write_config_file()
 
 
 def main(args=None):
     """The main function for the `summaryrecreate` executable
     """
-    parser = command_line()
+    parser = ArgumentParser(description=__doc__)
+    parser.add_known_options_to_parser(
+        [
+            "--samples", "--labels", "--rundir", "--code", "--delimiter",
+            "--config_override"
+        ]
+    )
     opts = parser.parse_args(args=args)
     inputs = Input(opts)
     for label in inputs.labels:
         inputs.code.pipe(os.path.join(inputs.rundir, label), inputs.config[label])
 
 
 if __name__ == "__main__":
```

### Comparing `pesummary-0.9.1/pesummary/cli/summarymodify.py` & `pesummary-1.0.0/pesummary/gw/file/formats/lalinference.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,540 +1,523 @@
-#! /usr/bin/env python
-
-# Copyright (C) 2020  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
-import numpy as np
-import math
-import argparse
-import json
-import h5py
-from pathlib import Path
-
-from pesummary.utils.utils import logger, check_file_exists_and_rename
-from pesummary.utils.exceptions import InputError
-from pesummary.core.command_line import DelimiterSplitAction
-from pesummary.gw.inputs import _GWInput
-from pesummary.gw.file.meta_file import _GWMetaFile
-
 
-__doc__ = """This executable is used to modify a PESummary metafile from the
-command line"""
-
-
-class _Input(_GWInput):
-    """Super class to handle the command line arguments
-    """
-    @property
-    def labels(self):
-        return self._labels
-
-    @labels.setter
-    def labels(self, labels):
-        self._labels = labels
-        if labels is not None and isinstance(labels, dict):
-            self._labels = labels
-        elif labels is not None:
-            raise InputError(
-                "Please provide an existing labels and the label you wish "
-                "to replace it with `--labels existing:new`."
-            )
-
-    @property
-    def kwargs(self):
-        return self._kwargs
-
-    @kwargs.setter
-    def kwargs(self, kwargs):
-        self._kwargs = kwargs
-        if kwargs is not None and isinstance(kwargs, dict):
-            self._kwargs = kwargs
-        elif kwargs is not None:
-            raise InputError(
-                "Please provide the label, kwarg and value with '--kwargs "
-                "label:kwarg:value`"
-            )
-
-    @property
-    def replace_posterior(self):
-        return self._replace_posterior
-
-    @replace_posterior.setter
-    def replace_posterior(self, replace_posterior):
-        self._replace_posterior = replace_posterior
-        if replace_posterior is not None and isinstance(replace_posterior, dict):
-            self._replace_posterior = replace_posterior
-        elif replace_posterior is not None:
-            raise InputError(
-                "Please provide the label, posterior and file path with "
-                "value with '--replace_posterior "
-                "label;posterior:/path/to/posterior.dat where ';' is the chosen "
-                "delimiter and provided with '--delimiter ;`"
-            )
-
-    @property
-    def remove_posterior(self):
-        return self._remove_posterior
-
-    @remove_posterior.setter
-    def remove_posterior(self, remove_posterior):
-        self._remove_posterior = remove_posterior
-        if remove_posterior is not None and isinstance(remove_posterior, dict):
-            self._remove_posterior = remove_posterior
-        elif remove_posterior is not None:
-            raise InputError(
-                "Please provide the label and posterior with '--remove_posterior "
-                "label:posterior`"
-            )
-
-    @property
-    def store_skymap(self):
-        return self._store_skymap
-
-    @store_skymap.setter
-    def store_skymap(self, store_skymap):
-        self._store_skymap = store_skymap
-        if store_skymap is not None and isinstance(store_skymap, dict):
-            self._store_skymap = store_skymap
-        elif store_skymap is not None:
-            raise InputError(
-                "Please provide the label and path to skymap with '--store_skymap "
-                "label:path/to/skymap.fits`"
-            )
-
-    @property
-    def samples(self):
-        return self._samples
-
-    @samples.setter
-    def samples(self, samples):
-        if samples is None:
-            raise InputError(
-                "Please provide a result file that you wish to modify"
-            )
-        if len(samples) > 1:
-            raise InputError(
-                "Only a single result file can be passed"
-            )
-        samples = samples[0]
-        if not self.is_pesummary_metafile(samples):
-            raise InputError(
-                "Please provide a PESummary metafile to this executable"
-            )
-        self._samples = samples
-
-    @property
-    def data(self):
-        return self._data
-
-    @data.setter
-    def data(self, data):
-        extension = Path(self.samples).suffix
-        if extension == ".h5" or extension == ".hdf5":
-            from pesummary.core.file.formats.pesummary import PESummary
-            from pandas import DataFrame
-
-            with h5py.File(self.samples, "r") as f:
-                data = PESummary._convert_hdf5_to_dict(f)
-                for label in data.keys():
-                    try:
-                        data[label]["posterior_samples"] = DataFrame(
-                            data[label]["posterior_samples"]
-                        ).to_records(index=False, column_dtypes=np.float)
-                    except KeyError:
-                        pass
-                    except Exception:
-                        parameters = data[label]["posterior_samples"]["parameter_names"]
-                        if isinstance(parameters[0], bytes):
-                            parameters = [
-                                parameter.decode("utf-8") for parameter in parameters
-                            ]
-                        samples = np.array([
-                            j for j in data[label]["posterior_samples"]["samples"]
-                        ].copy())
-                        data[label]["posterior_samples"] = DataFrame.from_dict(
-                            {
-                                param: samples.T[num] for num, param in
-                                enumerate(parameters)
-                            }
-                        ).to_records(index=False, column_dtypes=np.float)
-                self._data = data
-        elif extension == ".json":
-            with open(self.samples, "r") as f:
-                self._data = json.load(f)
-        else:
-            raise InputError(
-                "The extension '{}' is not recognised".format(extension)
-            )
-
-
-class Input(_Input):
-    """Class to handle the command line arguments
-
-    Parameters
-    ----------
-    opts: argparse.Namespace
-        Namespace object containing the command line options
-
-    Attributes
-    ----------
-    samples: str
-        path to a PESummary meta file that you wish to modify
-    labels: dict
-        dictionary of labels that you wish to modify. Key is the existing label
-        and item is the new label
-    """
-    def __init__(self, opts, ignore_copy=False):
-        logger.info("Command line arguments: %s" % (opts))
-        self.opts = opts
-        self.existing = None
-        self.webdir = self.opts.webdir
-        self.samples = self.opts.samples
-        self.labels = self.opts.labels
-        self.kwargs = self.opts.kwargs
-        self.replace_posterior = self.opts.replace_posterior
-        self.remove_posterior = self.opts.remove_posterior
-        self.store_skymap = self.opts.store_skymap
-        self.hdf5 = not self.opts.save_to_json
-        self.overwrite = self.opts.overwrite
-        self.force_replace = self.opts.force_replace
-        self.data = None
-
-
-def command_line():
-    """Generate an Argument Parser object to control the command line options
-    """
-    parser = argparse.ArgumentParser(description=__doc__)
-    parser.add_argument(
-        "--labels", dest="labels", nargs='+', action=DelimiterSplitAction,
-        help=("labels you wish to modify. Syntax: `--labels existing:new` "
-              "where ':' is the default delimiter"),
-        default=None
-    )
-    parser.add_argument(
-        "-s", "--samples", dest="samples", default=None, nargs='+',
-        help="Path to PESummary meta file you wish to modify"
-    )
-    parser.add_argument(
-        "-w", "--webdir", dest="webdir", default="./", metavar="DIR",
-        help="Directory to write the output file"
-    )
-    parser.add_argument(
-        "--save_to_json", action="store_true", default=False,
-        help="save the modified data in json format"
-    )
-    parser.add_argument(
-        "--delimiter", dest="delimiter", default=":",
-        help="Delimiter used to seperate the existing and new quantity"
-    )
-    parser.add_argument(
-        "--kwargs", dest="kwargs", nargs='+', action=DelimiterSplitAction,
-        help=("kwargs you wish to modify. Syntax: `--kwargs label/kwarg:item` "
-              "where '/' is a delimiter of your choosing (it cannot be ':'), "
-              "kwarg is the kwarg name and item is the value of the kwarg"),
-        default=None
-    )
-    parser.add_argument(
-        "--overwrite", action="store_true", default=False,
-        help=("Overwrite the supplied PESummary meta file with the modified "
-              "version")
-    )
-    parser.add_argument(
-        "--replace_posterior", nargs='+', action=DelimiterSplitAction,
-        help=("Replace the posterior for a given label. Syntax: "
-              "--replace_posterior label;a:/path/to/posterior.dat where "
-              "';' is a delimiter of your choosing (it cannot be '/' or ':'), "
-              "a is the posterior you wish to replace and item is a path "
-              "to a one column ascii file containing the posterior samples "
-              "(/path/to/posterior.dat)"),
-        default=None
-    )
-    parser.add_argument(
-        "--remove_posterior", nargs='+', action=DelimiterSplitAction,
-        help=("Remove a posterior distribution for a given label. Syntax: "
-              "--remove_posterior label:a where a is the posterior you wish to remove"),
-        default=None
-    )
-    parser.add_argument(
-        "--store_skymap", nargs='+', action=DelimiterSplitAction,
-        help=("Store the contents of a fits file in the metafile. Syntax: "
-              "--store_skymap label:path/to/skymap.fits"),
-        default=None
-    )
-    parser.add_argument(
-        "--force_replace", action="store_true", default=False,
-        help=("Override the ValueError raised if the data is already stored in the "
-              "result file")
-    )
-    return parser
-
-
-def _check_label(data, label, message, logger_level="warn"):
-    """Check that a given label is stored in the data. If it is not stored
-    print a warning message
+import h5py
+import numpy as np
 
-    Parameters
-    ----------
-    data: dict
-        dictionary containing the data
-    label: str
-        name of the label you wish to check
-    message: str
-        message you wish to print in logger when the label is not stored
-    logger_level: str, optional
-        the logger level of the message
-    """
-    if label not in data.keys():
-        getattr(logger, logger_level)(message)
-        return False
-    return True
+from pesummary.gw.file.formats.base_read import GWRead, GWSingleAnalysisRead
+from pesummary.utils.utils import logger
+from pesummary.utils.decorators import open_config
+from pesummary import conf
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+SAMPLER_KWARGS = {
+    "log_bayes_factor": conf.log_bayes_factor,
+    "bayes_factor": conf.bayes_factor,
+    "log_evidence": conf.log_evidence,
+    "evidence": conf.evidence,
+    "log_prior_volume": conf.log_prior_volume,
+    "sampleRate": "sample_rate",
+    "segmentLength": "segment_length"
+}
+
+META_DATA = {
+    "flow": "f_low",
+    "f_low": "f_low",
+    "fref": "f_ref",
+    "f_ref": "f_ref",
+    "LAL_PNORDER": "pn_order",
+    "LAL_APPROXIMANT": "approximant",
+    "number_of_live_points": "number_of_live_points",
+    "segmentLength": "segment_length",
+    "segmentStart": "segment_start",
+    "sampleRate": "sample_rate",
+}
 
 
-def _modify_labels(data, labels=None):
-    """Modify the existing labels in the data
+def open_lalinference(path):
+    """Grab the parameters and samples in a lalinference file
 
     Parameters
     ----------
-    data: dict
-        dictionary containing the data
-    labels: dict
-        dictionary of labels showing the existing label, key, and the new
-        label, item
+    path: str
+        path to the result file you wish to read in
     """
-    for existing, new in labels.items():
-        if existing not in data.keys():
-            logger.warning(
-                "Unable to find label '{}' in the root of the metafile. "
-                "Checking inside the groups".format(existing)
-            )
-            for key in data.keys():
-                if existing in data[key].keys():
-                    data[key][new] = data[key].pop(existing)
-        else:
-            data[new] = data.pop(existing)
-    return data
+    from pesummary.core.file.formats.hdf5 import _read_hdf5_with_h5py
+    lalinference_names, samples, _dataset = _read_hdf5_with_h5py(
+        path, return_posterior_dataset=True
+    )
+    if "logdistance" in lalinference_names:
+        lalinference_names.append("luminosity_distance")
+        for num, i in enumerate(samples):
+            samples[num].append(
+                np.exp(i[lalinference_names.index("logdistance")]))
+    if "costheta_jn" in lalinference_names:
+        lalinference_names.append("theta_jn")
+        for num, i in enumerate(samples):
+            samples[num].append(
+                np.arccos(i[lalinference_names.index("costheta_jn")]))
+    extra_kwargs = LALInference.grab_extra_kwargs(path)
+    extra_kwargs["sampler"]["nsamples"] = len(samples)
+    extra_kwargs["sampler"]["pe_algorithm"] = "lalinference"
+    try:
+        version = _dataset.attrs["VERSION"].decode("utf-8")
+    except Exception as e:
+        version = None
+    return {
+        "parameters": lalinference_names,
+        "samples": samples,
+        "injection": None,
+        "version": version,
+        "kwargs": extra_kwargs
+    }
 
 
-def _modify_kwargs(data, kwargs=None):
-    """Modify kwargs that are stored in the data
+class LALInference(GWSingleAnalysisRead):
+    """PESummary wrapper of `lalinference`
+    (https://git.ligo.org/lscsoft/lalsuite/lalinference).
 
     Parameters
     ----------
-    data: dict
-        dictionary containing the data
-    kwargs: dict
-        dictionary of kwargs showing the label as key and kwarg:value as the
-        item
-    """
-    from pesummary.core.file.formats.base_read import Read
-
-    def add_to_meta_data(data, label, string):
-        kwarg, value = string.split(":")
-        try:
-            _group, = Read.paths_to_key(kwarg, data[label]["meta_data"])
-            group = _group[0]
-        except ValueError:
-            group = "other"
-        if group == "other" and group not in data[label]["meta_data"].keys():
-            data[label]["meta_data"]["other"] = {}
-        data[label]["meta_data"][group][kwarg] = value
-        return data
-
-    message = "Unable to find label '{}' in the metafile. Unable to modify kwargs"
-    for label, item in kwargs.items():
-        check = _check_label(data, label, message.format(label))
-        if check:
-            if isinstance(item, list):
-                for _item in item:
-                    data = add_to_meta_data(data, label, _item)
-            else:
-                data = add_to_meta_data(data, label, item)
-    return data
-
+    path_to_results_file: str
+        path to the results file you wish to load in with `LALInference`
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
 
-def _modify_posterior(data, kwargs=None):
-    """Replace a posterior distribution that is stored in the data
-
-    Parameters
+    Attributes
     ----------
-    data: dict
-        dictionary containing the data
-    kwargs: dict
-        dictionary of kwargs showing the label as key and posterior:path as the
-        item
-    """
-    def _replace_posterior(data, string):
-        posterior, path = string.split(":")
-        _data = np.genfromtxt(path, usecols=0)
-        if math.isnan(_data[0]):
-            _data = np.genfromtxt(path, names=True, usecols=0)
-            _data = _data[_data.dtype.names[0]]
-        if posterior in data[label]["posterior_samples"].dtype.names:
-            data[label]["posterior_samples"][posterior] = _data
-        else:
-            from numpy.lib.recfunctions import append_fields
-
-            data[label]["posterior_samples"] = append_fields(
-                data[label]["posterior_samples"], posterior, _data, usemask=False
-            )
-        return data
-
-    message = "Unable to find label '{}' in the metafile. Unable to modify posterior"
-    for label, item in kwargs.items():
-        check = _check_label(data, label, message.format(label))
-        if check:
-            if isinstance(item, list):
-                for _item in item:
-                    data = _replace_posterior(data, _item)
-            else:
-                data = _replace_posterior(data, item)
-    return data
-
+    parameters: list
+        list of parameters stored in the result file
+    converted_parameters: list
+        list of parameters that have been derived from the sampled distributions
+    samples: 2d list
+        list of samples stored in the result file
+    samples_dict: dict
+        dictionary of samples stored in the result file keyed by parameters
+    input_version: str
+        version of the result file passed.
+    extra_kwargs: dict
+        dictionary of kwargs that were extracted from the result file
+    converted_parameters: list
+        list of parameters that have been added
+    pe_algorithm: str
+        name of the algorithm used to generate the posterior samples
+
+    Methods
+    -------
+    to_dat:
+        save the posterior samples to a .dat file
+    to_latex_table:
+        convert the posterior samples to a latex table
+    generate_latex_macros:
+        generate a set of latex macros for the stored posterior samples
+    to_lalinference:
+        convert the posterior samples to a lalinference result file
+    generate_all_posterior_samples:
+        generate all posterior distributions that may be derived from
+        sampled distributions
+    """
+    def __init__(self, path_to_results_file, injection_file=None, **kwargs):
+        super(LALInference, self).__init__(path_to_results_file, **kwargs)
+        self.load(self._grab_data_from_lalinference_file)
+
+    @classmethod
+    def load_file(cls, path, injection_file=None, **kwargs):
+        if injection_file and not os.path.isfile(injection_file):
+            raise IOError("%s does not exist" % (path))
+        return super(LALInference, cls).load_file(
+            path, injection_file=injection_file, **kwargs
+        )
 
-def _remove_posterior(data, kwargs=None):
-    """Remove a posterior distribution that is stored in the data
+    @staticmethod
+    def guess_path_to_sampler(path):
+        """Guess the path to the sampler group in a LALInference results file
+
+        Parameters
+        ----------
+        path: str
+            path to the LALInference results file
+        """
+        def _find_name(name):
+            c1 = "lalinference_nest" in name or "lalinference_mcmc" in name
+            c2 = "lalinference_nest/" not in name and "lalinference_mcmc/" not in name
+            if c1 and c2:
+                return name
+
+        f = h5py.File(path, 'r')
+        _path = f.visit(_find_name)
+        f.close()
+        return _path
+
+    @staticmethod
+    def _parameters_in_lalinference_file(path):
+        """Return the parameter names stored in the LALInference results file
+
+        Parameters
+        ----------
+        """
+        f = h5py.File(path, 'r')
+        path_to_samples = GWRead.guess_path_to_samples(path)
+        parameters = list(f[path_to_samples].dtype.names)
+        f.close()
+        return parameters
+
+    @staticmethod
+    def _samples_in_lalinference_file(path):
+        """
+        """
+        f = h5py.File(path, 'r')
+        path_to_samples = GWRead.guess_path_to_samples(path)
+        samples = [list(i) for i in f[path_to_samples]]
+        return samples
 
-    Parameters
-    ----------
-    data: dict
-        dictionary containing the data
-    kwargs: dict
-        dictionary of kwargs showing the label as key and posterior as the item
-    """
-    def _rmfield(array, *fieldnames_to_remove):
-        return array[
-            [name for name in array.dtype.names if name not in fieldnames_to_remove]
-        ]
-
-    message = "Unable to find label '{}' in the metafile. Unable to remove posterior"
-    for label, item in kwargs.items():
-        check = _check_label(data, label, message.format(label))
-        if check:
-            group = "posterior_samples"
-            if isinstance(item, list):
-                for _item in item:
-                    data[label][group] = _rmfield(data[label][group], _item)
+    @property
+    def calibration_spline_posterior(self):
+        if not any("_spcal_amp" in i for i in self.parameters):
+            return super(LALInference, self).calibration_spline_posterior
+        keys_amp = np.sort(
+            [param for param in self.parameters if "_spcal_amp" in param]
+        )
+        keys_phase = np.sort(
+            [param for param in self.parameters if "_spcal_phase" in param]
+        )
+        IFOs = np.unique(
+            [
+                param.split("_")[0] for param in self.parameters if
+                "_spcal_" in param
+            ]
+        )
+        log_frequencies = {ifo: [] for ifo in IFOs}
+        for key, value in self.extra_kwargs["other"].items():
+            if "_spcal_logfreq" in key:
+                cond = (
+                    key.replace("logfreq", "freq") not in
+                    self.extra_kwargs["other"].keys()
+                )
+                if cond:
+                    log_frequencies[key.split("_")[0]].append(float(value))
+            elif "_spcal_freq" in key:
+                log_frequencies[key.split("_")[0]].append(np.log(float(value)))
+        amp_params = {ifo: [] for ifo in IFOs}
+        phase_params = {ifo: [] for ifo in IFOs}
+        zipped = zip(
+            [keys_amp, keys_phase], [amp_params, phase_params]
+        )
+        _samples = self.samples_dict
+        for keys, dictionary in zipped:
+            for key in keys:
+                ifo = key.split("_")[0]
+                ind = self.parameters.index(key)
+                dictionary[ifo].append(_samples[key])
+        return log_frequencies, amp_params, phase_params
+
+    @staticmethod
+    def grab_extra_kwargs(path):
+        """Grab any additional information stored in the lalinference file
+        """
+        kwargs = {"sampler": {}, "meta_data": {}, "other": {}}
+        path_to_samples = GWRead.guess_path_to_samples(path)
+        path_to_sampler = LALInference.guess_path_to_sampler(path)
+        f = h5py.File(path, 'r')
+
+        attributes = dict(f[path_to_sampler].attrs.items())
+        for kwarg, item in attributes.items():
+            if kwarg in list(SAMPLER_KWARGS.keys()) and kwarg == "evidence":
+                kwargs["sampler"][conf.log_evidence] = np.round(np.log(item), 2)
+            elif kwarg in list(SAMPLER_KWARGS.keys()) and kwarg == "bayes_factor":
+                kwargs["sampler"][conf.log_bayes_factor] = np.round(
+                    np.log(item), 2
+                )
+            elif kwarg in list(SAMPLER_KWARGS.keys()):
+                kwargs["sampler"][SAMPLER_KWARGS[kwarg]] = np.round(item, 2)
             else:
-                data[label][group] = _rmfield(data[label][group], item)
-    return data
+                kwargs["other"][kwarg] = item
 
-
-def _store_skymap(data, kwargs=None, replace=False):
-    """Store a skymap in the metafile
-
-    Parameters
-    ----------
-    data: dict
-        dictionary containing the data
-    kwargs: dict
-        dictionary of kwargs showing the label as key and posterior as the item
-    replace: dict
-        replace a skymap already stored in the result file
-    """
-    from pesummary.io import read
-
-    message = "Unable to find label '{}' in the metafile. Unable to store skymap"
-    for label, path in kwargs.items():
-        check = _check_label(data, label, message.format(label))
-        if check:
-            skymap = read(path, skymap=True)
-            if "skymap" not in data[label].keys():
-                data[label]["skymap"] = {}
-            if "meta_data" not in data[label]["skymap"].keys():
-                data[label]["skymap"]["meta_data"] = {}
-            if "data" in data[label]["skymap"].keys() and not replace:
-                raise ValueError(
-                    "Skymap already found in result file for {}. If you wish to replace "
-                    "the skymap, add the command line argument '--force_replace".format(
-                        label
+        attributes = dict(f[path_to_samples].attrs.items())
+        for kwarg, item in attributes.items():
+            if kwarg in list(META_DATA.keys()) and kwarg == "LAL_APPROXIMANT":
+                try:
+                    from lalsimulation import GetStringFromApproximant
+
+                    kwargs["meta_data"]["approximant"] = \
+                        GetStringFromApproximant(
+                            int(attributes["LAL_APPROXIMANT"])
                     )
-                )
-            elif "data" in data[label]["skymap"].keys():
-                logger.warning("Replacing skymap data for {}".format(label))
-            data[label]["skymap"]["data"] = skymap
-            for key in skymap.meta_data:
-                data[label]["skymap"]["meta_data"][key] = skymap.meta_data[key]
-    return data
-
-
-def modify(data, function, **kwargs):
-    """Modify the data according to a given function
+                except Exception:
+                    kwargs["meta_data"]["approximant"] = \
+                        int(attributes["LAL_APPROXIMANT"])
+            elif kwarg in list(META_DATA.keys()):
+                kwargs["meta_data"][META_DATA[kwarg]] = item
+            else:
+                kwargs["other"][kwarg] = item
+        f.close()
+        return kwargs
+
+    @staticmethod
+    def _grab_data_from_lalinference_file(path):
+        """
+        """
+        return open_lalinference(path)
+
+    def add_fixed_parameters_from_config_file(self, config_file):
+        """Search the conifiguration file and add fixed parameters to the
+        list of parameters and samples
+
+        Parameters
+        ----------
+        config_file: str
+            path to the configuration file
+        """
+        self._add_fixed_parameters_from_config_file(
+            config_file, self._add_fixed_parameters)
+
+    def add_marginalized_parameters_from_config_file(self, config_file):
+        """Search the configuration file and add the marginalized parameters
+        to the list of parameters and samples
+
+        Parameters
+        ----------
+        config_file: str
+            path to the configuration file
+        """
+        self._add_marginalized_parameters_from_config_file(
+            config_file, self._add_marginalized_parameters)
+
+    @staticmethod
+    @open_config(index=2)
+    def _add_fixed_parameters(parameters, samples, config_file):
+        """Open a LALInference configuration file and add the fixed parameters
+        to the list of parameters and samples
+
+        Parameters
+        ----------
+        parameters: list
+            list of existing parameters
+        samples: list
+            list of existing samples
+        config_file: str
+            path to the configuration file
+        """
+        from pesummary.gw.file.standard_names import standard_names
+
+        config = config_file
+        if not config.error:
+            fixed_data = None
+            if "engine" in config.sections():
+                fixed_data = {
+                    key.split("fix-")[1]: item for key, item in
+                    config.items("engine") if "fix" in key}
+            if fixed_data is not None:
+                for i in fixed_data.keys():
+                    fixed_parameter = i
+                    fixed_value = fixed_data[i]
+                    try:
+                        param = standard_names[fixed_parameter]
+                        if param in parameters:
+                            pass
+                        else:
+                            parameters.append(param)
+                            for num in range(len(samples)):
+                                samples[num].append(float(fixed_value))
+                    except Exception:
+                        if fixed_parameter == "logdistance":
+                            if "luminosity_distance" not in parameters:
+                                parameters.append(standard_names["distance"])
+                                for num in range(len(samples)):
+                                    samples[num].append(float(fixed_value))
+                        if fixed_parameter == "costheta_jn":
+                            if "theta_jn" not in parameters:
+                                parameters.append(standard_names["theta_jn"])
+                                for num in range(len(samples)):
+                                    samples[num].append(float(fixed_value))
+        return parameters, samples
+
+    @staticmethod
+    @open_config(index=2)
+    def _add_marginalized_parameters(parameters, samples, config_file):
+        """Open a LALInference configuration file and add the marginalized
+        parameters to the list of parameters and samples
+
+        Parameters
+        ----------
+        parameters: list
+            list of existing parameters
+        samples: list
+            list of existing samples
+        config_file: str
+            path to the configuration file
+        """
+        config = config_file
+        if not config.error:
+            fixed_data = None
+            if "engine" in config.sections():
+                marg_par = {
+                    key.split("marg")[1]: item for key, item in
+                    config.items("engine") if "marg" in key}
+            for i in marg_par.keys():
+                if "time" in i and "geocent_time" not in parameters:
+                    if "marginalized_geocent_time" in parameters:
+                        ind = parameters.index("marginalized_geocent_time")
+                        parameters.remove(parameters[ind])
+                        parameters.append("geocent_time")
+                        for num, j in enumerate(samples):
+                            samples[num].append(float(j[ind]))
+                            del j[ind]
+                    else:
+                        logger.warning("You have marginalized over time and "
+                                       "there are no time samples. Manually "
+                                       "setting time to 100000s")
+                        parameters.append("geocent_time")
+                        for num, j in enumerate(samples):
+                            samples[num].append(float(100000))
+                if "phi" in i and "phase" not in parameters:
+                    if "marginalized_phase" in parameters:
+                        ind = parameters.index("marginalized_phase")
+                        parameters.remove(parameters[ind])
+                        parameters.append("phase")
+                        for num, j in enumerate(samples):
+                            samples[num].append(float(j[ind]))
+                            del j[ind]
+                    else:
+                        logger.warning("You have marginalized over phase and "
+                                       "there are no phase samples. Manually "
+                                       "setting the phase to be 0")
+                        parameters.append("phase")
+                        for num, j in enumerate(samples):
+                            samples[num].append(float(0))
+                if "dist" in i and "luminosity_distance" not in parameters:
+                    if "marginalized_distance" in parameters:
+                        ind = parameters.index("marginalized_distance")
+                        parameters.remove(parameters[ind])
+                        parameters.append("luminosity_distance")
+                        for num, j in enumerate(samples):
+                            samples[num].append(float(j[ind]))
+                            del j[ind]
+                    else:
+                        logger.warning("You have marginalized over distance and "
+                                       "there are no distance samples. Manually "
+                                       "setting distance to 100Mpc")
+                        parameters.append("luminosity_distance")
+                        for num, j in enumerate(samples):
+                            samples[num].append(float(100.0))
+            return parameters, samples
+        return parameters, samples
+
+
+def _write_lalinference(
+    parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
+    sampler="lalinference_nest", dat=False, **kwargs
+):
+    """Write a set of samples in LALInference file format
 
     Parameters
     ----------
-    data: dict
-        dictionary containing the data
-    function:
-        function you wish to use to modify the data
-    kwargs: dict
-        dictionary of kwargs for function
-    """
-    func_map = {
-        "labels": _modify_labels,
-        "kwargs": _modify_kwargs,
-        "add_posterior": _modify_posterior,
-        "rm_posterior": _remove_posterior,
-        "skymap": _store_skymap,
-    }
-    return func_map[function](data, **kwargs)
-
-
-def _main(opts):
-    """
-    """
-    args = Input(opts)
-    if not args.overwrite:
-        meta_file = os.path.join(
-            args.webdir, "modified_posterior_samples.{}".format(
-                "h5" if args.hdf5 else "json"
+    parameters: list
+        list of parameters
+    samples: 2d list
+        list of samples. Columns correspond to a given parameter
+    outdir: str
+        The directory where you would like to write the lalinference file
+    label: str
+        The label of the analysis. This is used in the filename if a filename
+        if not specified
+    filename: str
+        The name of the file that you wish to write
+    overwrite: Bool
+        If True, an existing file of the same name will be overwritten
+    sampler: str
+        The sampler which you wish to store in the result file. This may either
+        be 'lalinference_nest' or 'lalinference_mcmc'
+    dat: Bool
+        If True, a LALInference dat file is produced
+    """
+    from pesummary.gw.file.standard_names import lalinference_map
+    from pesummary.utils.samples_dict import SamplesDict
+    import copy
+
+    _samples = copy.deepcopy(samples)
+    _parameters = copy.deepcopy(parameters)
+    _samples = SamplesDict(_parameters, np.array(_samples).T.tolist())
+    if not filename and not label:
+        from time import time
+
+        label = round(time())
+    if not filename:
+        extension = "dat" if dat else "hdf5"
+        filename = "lalinference_{}.{}".format(label, extension)
+
+    if os.path.isfile(os.path.join(outdir, filename)) and not overwrite:
+        raise FileExistsError(
+            "The file '{}' already exists in the directory {}".format(
+                filename, outdir
             )
         )
-        check_file_exists_and_rename(meta_file)
-    else:
-        meta_file = args.samples
-    if args.labels is not None:
-        modified_data = modify(args.data, "labels", labels=args.labels)
-    if args.kwargs is not None:
-        modified_data = modify(args.data, "kwargs", kwargs=args.kwargs)
-    if args.replace_posterior is not None:
-        modified_data = modify(args.data, "add_posterior", kwargs=args.replace_posterior)
-    if args.remove_posterior is not None:
-        modified_data = modify(args.data, "rm_posterior", kwargs=args.remove_posterior)
-    if args.store_skymap is not None:
-        modified_data = modify(
-            args.data, "skymap", kwargs=args.store_skymap, replace=args.force_replace
+    reverse_map = {item: key for key, item in lalinference_map.items()}
+    no_key = []
+    for param in _parameters:
+        if param in reverse_map.keys() and reverse_map[param] in _parameters:
+            logger.warning(
+                "The LALInference name for '{}' is '{}'. '{}' already found "
+                "in the posterior table. Keeping both entries".format(
+                    param, reverse_map[param], reverse_map[param]
+                )
+            )
+        elif param in reverse_map.keys():
+            _samples[reverse_map[param]] = _samples.pop(param)
+        elif param not in lalinference_map.keys():
+            no_key.append(param)
+    if len(no_key):
+        logger.info(
+            "Unable to find a LALInference name for the parameters: {}. "
+            "Keeping the PESummary name.".format(", ".join(no_key))
         )
-    logger.info(
-        "Saving the modified data to '{}'".format(meta_file)
-    )
-    if args.hdf5:
-        _GWMetaFile.save_to_hdf5(
-            modified_data, list(modified_data.keys()), None, meta_file,
-            no_convert=True
+    lalinference_samples = _samples.to_structured_array()
+    if dat:
+        np.savetxt(
+            os.path.join(outdir, filename), lalinference_samples,
+            delimiter="\t", comments="",
+            header="\t".join(lalinference_samples.dtype.names)
         )
     else:
-        _GWMetaFile.save_to_json(modified_data, meta_file)
-
+        with h5py.File(os.path.join(outdir, filename), "w") as f:
+            lalinference = f.create_group("lalinference")
+            sampler = lalinference.create_group(sampler)
+            samples = sampler.create_dataset(
+                "posterior_samples", data=lalinference_samples
+            )
 
-def main(args=None):
-    """
-    """
-    parser = command_line()
-    opts = parser.parse_args(args=args)
-    return _main(opts)
 
+def write_lalinference(
+    parameters, samples, outdir="./", label=None, filename=None, overwrite=False,
+    sampler="lalinference_nest", dat=False, **kwargs
+):
+    """Write a set of samples in LALInference file format
 
-if __name__ == "__main__":
-    main()
+    Parameters
+    ----------
+    parameters: list
+        list of parameters
+    samples: 2d list
+        list of samples. Columns correspond to a given parameter
+    outdir: str
+        The directory where you would like to write the lalinference file
+    label: str
+        The label of the analysis. This is used in the filename if a filename
+        if not specified
+    filename: str
+        The name of the file that you wish to write
+    overwrite: Bool
+        If True, an existing file of the same name will be overwritten
+    sampler: str
+        The sampler which you wish to store in the result file. This may either
+        be 'lalinference_nest' or 'lalinference_mcmc'
+    dat: Bool
+        If True, a LALInference dat file is produced
+    """
+    from pesummary.io.write import _multi_analysis_write
+
+    _multi_analysis_write(
+        _write_lalinference, parameters, samples, outdir=outdir, label=label,
+        filename=filename, overwrite=overwrite, sampler=sampler, dat=dat,
+        file_format="lalinference", **kwargs
+    )
```

### Comparing `pesummary-0.9.1/pesummary/cli/summaryclassification.py` & `pesummary-1.0.0/pesummary/cli/summaryclassification.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,93 +1,91 @@
 #! /usr/bin/env python
 
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
 import pesummary
+from pesummary.core.cli.inputs import _Input
 from pesummary.gw.file.read import read as GWRead
-from pesummary.gw.pepredicates import PEPredicates
-from pesummary.gw.p_astro import PAstro
+from pesummary.gw.classification import PEPredicates, PAstro
 from pesummary.utils.utils import make_dir, logger
 from pesummary.utils.exceptions import InputError
-import argparse
-
+from pesummary.core.cli.parser import ArgumentParser as _ArgumentParser
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 __doc__ = """This executable is used to generate a txt file containing the
 source classification probailities"""
 
 
-def command_line():
-    """Generate an Argument Parser object to control the command line options
-    """
-    parser = argparse.ArgumentParser(description=__doc__)
-    parser.add_argument("-w", "--webdir", dest="webdir",
-                        help="make page and plots in DIR", metavar="DIR",
-                        default=None)
-    parser.add_argument("-s", "--samples", dest="samples",
-                        help="Posterior samples hdf5 file", nargs='+',
-                        default=None)
-    parser.add_argument("--labels", dest="labels",
-                        help="labels used to distinguish runs", nargs='+',
-                        default=None)
-    parser.add_argument("--prior", dest="prior",
-                        choices=["population", "default", "both"],
-                        default="both",
-                        help=("Prior to use when calculating source "
-                              "classification probabilities"))
-    parser.add_argument("--plot", dest="plot",
-                        help="name of the plot you wish to make",
-                        default="bar", choices=["bar", "mass_1_mass_2"])
-    return parser
+class ArgumentParser(_ArgumentParser):
+    def _pesummary_options(self):
+        options = super(ArgumentParser, self)._pesummary_options()
+        options.update(
+            {
+                "--prior": {
+                    "choices": ["population", "default", "both"],
+                    "default": "both",
+                    "help": (
+                        "Prior to use when calculating source classification "
+                        "probabilities"
+                    )
+                },
+                "--plot": {
+                    "choices": ["bar", "mass_1_mass_2"],
+                    "default": "bar",
+                    "help": "Name of the plot you wish to make",
+                },
+            }
+        )
+        return options
 
 
-def generate_probabilities(result_files, prior="both"):
+def generate_probabilities(result_files, prior="both", seed=123456789):
     """Generate the classification probabilities
 
     Parameters
     ----------
     result_files: list
         list of result files
     prior: str
         prior you wish to reweight your samples too
     """
     classifications = []
+    if prior == "both":
+        _func = "dual_classification"
+        _kwargs = {}
+    else:
+        _func = "classification"
+        _kwargs = {"population": True if prior == "population" else False}
 
     for num, i in enumerate(result_files):
         mydict = {}
-        f = GWRead(i)
-        if not isinstance(f, pesummary.gw.file.formats.pesummary.PESummary):
-            f.generate_all_posterior_samples()
-            mydict["default"], mydict["population"] = \
-                PEPredicates.classifications(f.samples, f.parameters)
-            em_bright = PAstro.classifications(f.samples_dict)
+        if not _Input.is_pesummary_metafile(i):
+            mydict = getattr(
+                PEPredicates, "{}_from_file".format(_func)
+            )(i, seed=seed, **_kwargs)
+            em_bright = getattr(
+                PAstro, "{}_from_file".format(_func)
+            )(i, seed=seed, **_kwargs)
         else:
+            f = GWRead(i)
             label = f.labels[0]
-            mydict["default"], mydict["population"] = \
-                PEPredicates.classifications(f.samples[0], f.parameters[0])
-            em_bright = PAstro.classifications(f.samples_dict[label])
-        mydict["default"].update(em_bright[0])
-        mydict["population"].update(em_bright[1])
+            mydict = getattr(
+                 PEPredicates(f.samples_dict[label]), _func
+            )(seed=seed, **_kwargs)
+            em_bright = getattr(
+                PAstro(f.samples_dict[label]), _func
+            )(seed=seed, **_kwargs)
+        if prior == "both":
+            mydict["default"].update(em_bright["default"])
+            mydict["population"].update(em_bright["population"])
+        else:
+            mydict.update(em_bright)
         classifications.append(mydict)
-    if prior == "both":
-        return classifications
-    return [{prior: i[prior]} for i in classifications]
+    return classifications
 
 
 def save_classifications(savedir, classifications, labels):
     """Read and return a list of parameters and samples stored in the result
     files
 
     Parameters
@@ -177,24 +175,29 @@
                     )
                 )
 
 
 def main(args=None):
     """Top level interface for `summarypublication`
     """
-    parser = command_line()
-    opts = parser.parse_args(args=args)
+    parser = ArgumentParser(description=__doc__)
+    parser.add_known_options_to_parser(
+        ["--webdir", "--samples", "--labels", "--prior", "--plot", "--seed"]
+    )
+    opts, _ = parser.parse_known_args(args=args)
     if opts.webdir:
         make_dir(opts.webdir)
     else:
         logger.warning(
             "No webdir given so plots will not be generated and "
             "classifications will be shown in stdout rather than saved to file"
         )
-    classifications = generate_probabilities(opts.samples, prior=opts.prior)
+    classifications = generate_probabilities(
+        opts.samples, prior=opts.prior, seed=opts.seed
+    )
     if opts.labels is None:
         opts.labels = []
         for i in opts.samples:
             f = GWRead(i)
             if hasattr(f, "labels"):
                 opts.labels.append(f.labels[0])
             else:
```

### Comparing `pesummary-0.9.1/pesummary/cli/summaryjscompare.py` & `pesummary-1.0.0/pesummary/cli/summaryjscompare.py`

 * *Files 10% similar despite different names*

```diff
@@ -2,40 +2,82 @@
 """
 Interface to generate JS test comparison between two results files.
 
 This plots the difference in CDF between two sets of samples and adds the JS test
 statistic with uncertainty estimated by bootstrapping.
 """
 
-import argparse
 from collections import namedtuple
-
 import numpy as np
 import os
 import pandas as pd
 from scipy.stats import binom
 import matplotlib
 import matplotlib.style
 import matplotlib.pyplot as plt
 
 from pesummary.io import read
-from pesummary.core.parser import parser as pesummary_parser
-from pesummary.core.plots.bounded_1d_kde import Bounded_1d_kde
+from pesummary.core.cli.parser import ArgumentParser as _ArgumentParser
+from pesummary.core.plots.bounded_1d_kde import ReflectionBoundedKDE
 from pesummary.core.plots.figure import figure
 from pesummary.gw.plots.bounds import default_bounds
 from pesummary.gw.plots.latex_labels import GWlatex_labels
 from pesummary.core.plots.latex_labels import latex_labels
 from pesummary.utils.tqdm import tqdm
 from pesummary.utils.utils import jensen_shannon_divergence
 from pesummary.utils.utils import _check_latex_install, get_matplotlib_style_file, logger
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 matplotlib.style.use(get_matplotlib_style_file())
 _check_latex_install()
 
 
+class ArgumentParser(_ArgumentParser):
+    def _pesummary_options(self):
+        options = super(ArgumentParser, self)._pesummary_options()
+        options.update(
+            {
+                "--ntests": {
+                    "type": int,
+                    "default": 100,
+                    "help": "Number of iterations for bootstrapping",
+                },
+                "--main_keys": {
+                    "nargs": "+",
+                    "default": [
+                        "theta_jn", "chirp_mass", "mass_ratio", "tilt_1",
+                        "tilt_2", "luminosity_distance", "ra", "dec", "a_1",
+                        "a_2"
+                    ],
+                    "help": "List of parameter names"
+                },
+                "--event": {
+                    "type": str,
+                    "required": True,
+                    "help": "Label, e.g. the event name"
+                },
+                "--samples": {
+                    "short": "-s",
+                    "type": str,
+                    "nargs": 2,
+                    "help": "Paths to a pair of results files to compare."
+                },
+                "--labels": {
+                    "short": "-l",
+                    "type": str,
+                    "nargs": 2,
+                    "help": "Pair of labels for each result file",
+                },
+            }
+        )
+        options["--nsamples"]["default"] = 10000
+        return options
+
+
 def load_data(data_file):
     """ Read in a data file and return samples dictionary """
     f = read(data_file, package="gw", disable_prior=True)
     return f.samples_dict
 
 
 def js_bootstrap(key, resultA, resultB, nsamples, ntests):
@@ -68,15 +110,15 @@
 
     js_array = np.zeros(ntests)
 
     for j in tqdm(range(ntests)):
         bootA = np.random.choice(samplesA, size=nsamples, replace=False)
         bootB = np.random.choice(samplesB, size=nsamples, replace=False)
         js_array[j] = np.nan_to_num(
-            jensen_shannon_divergence([bootA, bootB], kde=Bounded_1d_kde, xlow=xlow, xhigh=xhigh)
+            jensen_shannon_divergence([bootA, bootB], kde=ReflectionBoundedKDE, xlow=xlow, xhigh=xhigh)
         )
     return js_array
 
 
 def calc_median_error(jsvalues, quantiles=(0.16, 0.84)):
     quants_to_compute = np.array([quantiles[0], 0.5, quantiles[1]])
     quants = np.percentile(jsvalues, quants_to_compute * 100)
@@ -185,59 +227,30 @@
     ax.set_title(r"{} N samples={:.0f}".format(event, nsamples))
     ax.grid()
     fig.tight_layout()
     plt.savefig(os.path.join(webdir, "{}-comparison-{}-{}.png".format(event, labelA, labelB)))
 
 
 def parse_cmd_line(args=None):
-    parser = argparse.ArgumentParser(description=__doc__, formatter_class=argparse.ArgumentDefaultsHelpFormatter)
-    parser.add_argument("--event", type=str, required=True, help="Label, e.g. the event name")
-    parser.add_argument(
-        "-r", "--samples", type=str, nargs=2, help=("Paths to a pair of results files to compare."),
+    _parser = ArgumentParser(description=__doc__)
+    _parser.add_known_options_to_parser(
+        [
+            "--seed", "--nsamples", "--webdir", "--ntests", "--main_keys",
+            "--labels", "--samples", "--event"
+        ]
     )
-    parser.add_argument("-l", "--labels", type=str, nargs=2, help="Pair of labels for each result file")
-    parser.add_argument(
-        "--main_keys",
-        nargs="+",
-        default=[
-            "theta_jn",
-            "chirp_mass",
-            "mass_ratio",
-            "tilt_1",
-            "tilt_2",
-            "luminosity_distance",
-            "ra",
-            "dec",
-            "a_1",
-            "a_2",
-        ],
-        required=False,
-        help="List of parameter names",
-    )
-    parser.add_argument(
-        "--ntests", type=int, default=100, required=False, help="Number of iteration for bootstrapping",
-    )
-    parser.add_argument(
-        "--nsamples", type=int, default=10000, required=False, help="Number of samples to use",
-    )
-    parser.add_argument("--random-seed", type=int, default=150914)
-    parser.add_argument(
-        "--webdir", type=str, default=".", required=False, help="Path to webdirectory where plots will be saved"
-    )
-
-    _parser = pesummary_parser(existing_parser=parser)
     args, unknown = _parser.parse_known_args(args=args)
     return args
 
 
 def main(args=None):
     args = parse_cmd_line(args=args)
 
     # Set random seed
-    np.random.seed(seed=args.random_seed)
+    np.random.seed(seed=args.seed)
 
     # Read in the keys to apply
     main_keys = args.main_keys
 
     # Read in the results and labels
     resultA = load_data(args.samples[0])
     labelA = args.labels[0]
```

### Comparing `pesummary-0.9.1/pesummary/cli/summarypages.py` & `pesummary-1.0.0/pesummary/cli/summarypages.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,27 +1,16 @@
 #! /usr/bin/env python
 
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from pesummary.utils.utils import logger, gw_results_file
-from pesummary.core.inputs import PostProcessing
-from pesummary.gw.inputs import GWPostProcessing
+import pesummary.core.cli.inputs
+import pesummary.gw.cli.inputs
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 class WebpageGeneration(object):
     """Wrapper class for _GWWebpageGeneration and _CoreWebpageGeneration
 
     Parameters
     ----------
@@ -46,177 +35,222 @@
             object = _GWWebpageGeneration(self.inputs, colors=self.colors)
         else:
             object = _CoreWebpageGeneration(self.inputs, colors=self.colors)
         object.generate_webpages()
         logger.info("Finished generating webpages")
 
 
-class _CoreWebpageGeneration(PostProcessing):
+class _CoreWebpageGeneration(object):
     """Class to generate all webpages for all result files with the Core module
 
     Parameters
     ----------
     inputs: argparse.Namespace
         Namespace object containing the command line options
     colors: list, optional
         colors that you wish to use to distinguish different result files
     """
     def __init__(self, inputs, colors="default"):
         from pesummary.core.webpage.main import _WebpageGeneration
-
-        super(_CoreWebpageGeneration, self).__init__(inputs, colors)
+        key_data = inputs.grab_key_data_from_result_files()
         self.webpage_object = _WebpageGeneration(
-            webdir=self.webdir, samples=self.samples, labels=self.labels,
-            publication=self.publication, user=self.user, config=self.config,
-            same_parameters=self.same_parameters, base_url=self.baseurl,
-            file_versions=self.file_version, hdf5=self.hdf5, colors=self.colors,
-            custom_plotting=self.custom_plotting,
-            existing_labels=self.existing_labels,
-            existing_config=self.existing_config,
-            existing_file_version=self.existing_file_version,
-            existing_injection_data=self.existing_injection_data,
-            existing_samples=self.existing_samples,
-            existing_metafile=self.existing,
-            existing_file_kwargs=self.existing_file_kwargs,
-            existing_weights=self.existing_weights,
-            add_to_existing=self.add_to_existing, notes=self.notes,
-            disable_comparison=self.disable_comparison,
-            disable_interactive=self.disable_interactive,
-            package_information=self.package_information,
-            mcmc_samples=self.mcmc_samples,
-            external_hdf5_links=self.external_hdf5_links
+            webdir=inputs.webdir, samples=inputs.samples, labels=inputs.labels,
+            publication=inputs.publication, user=inputs.user, config=inputs.config,
+            same_parameters=inputs.same_parameters, base_url=inputs.baseurl,
+            file_versions=inputs.file_version, hdf5=inputs.hdf5, colors=inputs.colors,
+            custom_plotting=inputs.custom_plotting,
+            existing_labels=inputs.existing_labels,
+            existing_config=inputs.existing_config,
+            existing_file_version=inputs.existing_file_version,
+            existing_injection_data=inputs.existing_injection_data,
+            existing_samples=inputs.existing_samples,
+            existing_metafile=inputs.existing,
+            existing_file_kwargs=inputs.existing_file_kwargs,
+            existing_weights=inputs.existing_weights,
+            add_to_existing=inputs.add_to_existing, notes=inputs.notes,
+            disable_comparison=inputs.disable_comparison,
+            disable_interactive=inputs.disable_interactive,
+            package_information=inputs.package_information,
+            mcmc_samples=inputs.mcmc_samples,
+            external_hdf5_links=inputs.external_hdf5_links, key_data=key_data,
+            existing_plot=inputs.existing_plot, disable_expert=inputs.disable_expert,
+            analytic_priors=inputs.analytic_prior_dict
         )
 
     def generate_webpages(self):
         """Generate all webpages within the Core module
         """
         self.webpage_object.generate_webpages()
 
 
-class _GWWebpageGeneration(GWPostProcessing):
+class _GWWebpageGeneration(object):
     """Class to generate all webpages for all result files with the GW module
 
     Parameters
     ----------
     inputs: argparse.Namespace
         Namespace object containing the command line options
     colors: list, optional
         colors that you wish to use to distinguish different result files
     """
     def __init__(self, inputs, colors="default"):
         from pesummary.gw.webpage.main import _WebpageGeneration
-
-        super(_GWWebpageGeneration, self).__init__(inputs, colors)
-        key_data = self.grab_key_data_from_result_files()
+        key_data = inputs.grab_key_data_from_result_files()
         self.webpage_object = _WebpageGeneration(
-            webdir=self.webdir, samples=self.samples, labels=self.labels,
-            publication=self.publication, user=self.user, config=self.config,
-            same_parameters=self.same_parameters, base_url=self.baseurl,
-            file_versions=self.file_version, hdf5=self.hdf5, colors=self.colors,
-            custom_plotting=self.custom_plotting, gracedb=self.gracedb,
-            pepredicates_probs=self.pepredicates_probs,
-            approximant=self.approximant, key_data=key_data,
-            file_kwargs=self.file_kwargs, existing_labels=self.existing_labels,
-            existing_config=self.existing_config,
-            existing_file_version=self.existing_file_version,
-            existing_injection_data=self.existing_injection_data,
-            existing_samples=self.existing_samples,
-            existing_metafile=self.existing,
-            add_to_existing=self.add_to_existing,
-            existing_file_kwargs=self.existing_file_kwargs,
-            existing_weights=self.existing_weights,
-            result_files=self.result_files, notes=self.notes,
-            disable_comparison=self.disable_comparison,
-            disable_interactive=self.disable_interactive,
-            pastro_probs=self.pastro_probs, gwdata=self.gwdata,
-            publication_kwargs=self.publication_kwargs,
-            no_ligo_skymap=self.no_ligo_skymap,
-            psd=self.psd, priors=self.priors,
-            package_information=self.package_information,
-            mcmc_samples=self.mcmc_samples,
-            external_hdf5_links=self.external_hdf5_links
+            webdir=inputs.webdir, samples=inputs.samples, labels=inputs.labels,
+            publication=inputs.publication, user=inputs.user, config=inputs.config,
+            same_parameters=inputs.same_parameters, base_url=inputs.baseurl,
+            file_versions=inputs.file_version, hdf5=inputs.hdf5, colors=inputs.colors,
+            custom_plotting=inputs.custom_plotting, gracedb=inputs.gracedb,
+            pepredicates_probs=inputs.pepredicates_probs,
+            approximant=inputs.approximant, key_data=key_data,
+            file_kwargs=inputs.file_kwargs, existing_labels=inputs.existing_labels,
+            existing_config=inputs.existing_config,
+            existing_file_version=inputs.existing_file_version,
+            existing_injection_data=inputs.existing_injection_data,
+            existing_samples=inputs.existing_samples,
+            existing_metafile=inputs.existing,
+            add_to_existing=inputs.add_to_existing,
+            existing_file_kwargs=inputs.existing_file_kwargs,
+            existing_weights=inputs.existing_weights,
+            result_files=inputs.result_files, notes=inputs.notes,
+            disable_comparison=inputs.disable_comparison,
+            disable_interactive=inputs.disable_interactive,
+            pastro_probs=inputs.pastro_probs, gwdata=inputs.gwdata,
+            publication_kwargs=inputs.publication_kwargs,
+            no_ligo_skymap=inputs.no_ligo_skymap,
+            psd=inputs.psd, priors=inputs.priors,
+            package_information=inputs.package_information,
+            mcmc_samples=inputs.mcmc_samples, existing_plot=inputs.existing_plot,
+            external_hdf5_links=inputs.external_hdf5_links,
+            preliminary_pages=inputs.preliminary_pages,
+            disable_expert=inputs.disable_expert,
+            analytic_priors=inputs.analytic_prior_dict
         )
 
     def generate_webpages(self):
         """Generate all webpages within the Core module
         """
         self.webpage_object.generate_webpages()
 
 
-class _PublicGWWebpageGeneration(GWPostProcessing):
+class _PublicGWWebpageGeneration(object):
     """Class to generate all webpages for all result files with the GW module
 
     Parameters
     ----------
     inputs: argparse.Namespace
         Namespace object containing the command line options
     colors: list, optional
         colors that you wish to use to distinguish different result files
     """
     def __init__(self, inputs, colors="default"):
         from pesummary.gw.webpage.public import _PublicWebpageGeneration
-
-        super(_PublicGWWebpageGeneration, self).__init__(inputs, colors)
-        key_data = self.grab_key_data_from_result_files()
+        key_data = inputs.grab_key_data_from_result_files()
         self.webpage_object = _PublicWebpageGeneration(
-            webdir=self.webdir, samples=self.samples, labels=self.labels,
-            publication=self.publication, user=self.user, config=self.config,
-            same_parameters=self.same_parameters, base_url=self.baseurl,
-            file_versions=self.file_version, hdf5=self.hdf5, colors=self.colors,
-            custom_plotting=self.custom_plotting, gracedb=self.gracedb,
-            pepredicates_probs=self.pepredicates_probs,
-            approximant=self.approximant, key_data=key_data,
-            file_kwargs=self.file_kwargs, existing_labels=self.existing_labels,
-            existing_config=self.existing_config,
-            existing_file_version=self.existing_file_version,
-            existing_injection_data=self.existing_injection_data,
-            existing_samples=self.existing_samples,
-            existing_metafile=self.existing,
-            add_to_existing=self.add_to_existing,
-            existing_file_kwargs=self.existing_file_kwargs,
-            existing_weights=self.existing_weights,
-            result_files=self.result_files, notes=self.notes,
-            disable_comparison=self.disable_comparison,
-            disable_interactive=self.disable_interactive,
-            pastro_probs=self.pastro_probs, gwdata=self.gwdata,
-            publication_kwargs=self.publication_kwargs,
-            no_ligo_skymap=self.no_ligo_skymap,
-            psd=self.psd, priors=self.priors,
-            package_information=self.package_information,
-            mcmc_samples=self.mcmc_samples,
-            external_hdf5_links=self.external_hdf5_links
+            webdir=inputs.webdir, samples=inputs.samples, labels=inputs.labels,
+            publication=inputs.publication, user=inputs.user, config=inputs.config,
+            same_parameters=inputs.same_parameters, base_url=inputs.baseurl,
+            file_versions=inputs.file_version, hdf5=inputs.hdf5, colors=inputs.colors,
+            custom_plotting=inputs.custom_plotting, gracedb=inputs.gracedb,
+            pepredicates_probs=inputs.pepredicates_probs,
+            approximant=inputs.approximant, key_data=key_data,
+            file_kwargs=inputs.file_kwargs, existing_labels=inputs.existing_labels,
+            existing_config=inputs.existing_config,
+            existing_file_version=inputs.existing_file_version,
+            existing_injection_data=inputs.existing_injection_data,
+            existing_samples=inputs.existing_samples,
+            existing_metafile=inputs.existing,
+            add_to_existing=inputs.add_to_existing,
+            existing_file_kwargs=inputs.existing_file_kwargs,
+            existing_weights=inputs.existing_weights,
+            result_files=inputs.result_files, notes=inputs.notes,
+            disable_comparison=inputs.disable_comparison,
+            disable_interactive=inputs.disable_interactive,
+            pastro_probs=inputs.pastro_probs, gwdata=inputs.gwdata,
+            publication_kwargs=inputs.publication_kwargs,
+            no_ligo_skymap=inputs.no_ligo_skymap,
+            psd=inputs.psd, priors=inputs.priors,
+            package_information=inputs.package_information,
+            mcmc_samples=inputs.mcmc_samples, existing_plot=inputs.existing_plot,
+            external_hdf5_links=inputs.external_hdf5_links,
+            preliminary_pages=inputs.preliminary_pages,
+            disable_expert=inputs.disable_expert,
+            analytic_priors=inputs.analytic_prior_dict
         )
 
     def generate_webpages(self):
         """Generate all webpages within the Core module
         """
         self.webpage_object.generate_webpages()
 
 
-def main(args=None):
+def main(
+    args=None,
+    _parser=None,
+    _core_input_cls=pesummary.core.cli.inputs.WebpagePlusPlottingPlusMetaFileInput,
+    _gw_input_cls=pesummary.gw.cli.inputs.WebpagePlusPlottingPlusMetaFileInput
+):
     """Top level interface for `summarypages`
     """
-    from pesummary.gw.parser import parser
-    from pesummary.utils import functions, history_dictionary
-
-    _parser = parser()
-    opts, unknown = _parser.parse_known_args(args=args)
-    func = functions(opts)
-    args = func["input"](opts)
+    from pesummary.utils import history_dictionary
     from .summaryplots import PlotGeneration
 
-    plotting_object = PlotGeneration(args, gw=gw_results_file(opts))
-    WebpageGeneration(args, gw=gw_results_file(opts))
+    if _parser is None:
+        from pesummary.gw.cli.parser import ArgumentParser
+        _parser = ArgumentParser()
+        _parser.add_all_groups_to_parser()
+
+    opts, unknown = _parser.parse_known_args(args=args)
+    _gw = False
+    if opts.restart_from_checkpoint:
+        from pesummary import conf
+        import os
+        if opts.webdir is None:
+            raise ValueError(
+                "In order to restart from checkpoint please provide a webdir"
+            )
+        resume_file_dir = conf.checkpoint_dir(opts.webdir)
+        resume_file = conf.resume_file
+        state = pesummary.core.cli.inputs.load_current_state(
+            os.path.join(resume_file_dir, resume_file)
+        )
+        if state is not None:
+            _gw = state.gw
+        input_args = (opts,)
+        input_kwargs = {"checkpoint": state}
+    else:
+        if opts.gw or gw_results_file(opts):
+            _gw = True
+        input_args = (opts,)
+        input_kwargs = {}
+
+    if _gw:
+        from pesummary.gw.file.meta_file import GWMetaFile
+        from pesummary.gw.finish import GWFinishingTouches
+        input_cls = _gw_input_cls
+        meta_file_cls = GWMetaFile
+        finish_cls = GWFinishingTouches
+    else:
+        from pesummary.core.file.meta_file import MetaFile
+        from pesummary.core.finish import FinishingTouches
+        input_cls = _core_input_cls
+        meta_file_cls = MetaFile
+        finish_cls = FinishingTouches
+
+    args = input_cls(*input_args, **input_kwargs)
+    plotting_object = PlotGeneration(args, gw=args.gw)
+    WebpageGeneration(args, gw=args.gw)
     _history = history_dictionary(
         program=_parser.prog, creator=args.user,
         command_line=_parser.command_line
     )
-    func["MetaFile"](args, history=_history)
+    meta_file_cls(args, history=_history)
     if gw_results_file(opts):
         kwargs = dict(ligo_skymap_PID=plotting_object.ligo_skymap_PID)
     else:
         kwargs = {}
-    func["FinishingTouches"](args, **kwargs)
+    finish_cls(args, **kwargs)
 
 
 if __name__ == "__main__":
     main()
```

### Comparing `pesummary-0.9.1/pesummary/cli/summaryplots.py` & `pesummary-1.0.0/pesummary/gw/file/meta_file.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,349 +1,326 @@
-#! /usr/bin/env python
+# Licensed under an MIT style license -- see LICENSE.md
 
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-
-from pesummary.utils.utils import logger, make_dir
-from pesummary.core.inputs import PostProcessing
-from pesummary.gw.inputs import GWPostProcessing
-from pesummary.gw.plots.latex_labels import GWlatex_labels
-from pesummary.core.plots.latex_labels import latex_labels
-from pesummary.gw.plots.main import _PlotGeneration
-from pesummary.core.command_line import DictionaryAction
-from pesummary.gw.file.read import read
+import numpy as np
+from pesummary.utils.utils import logger
+from pesummary.core.file.meta_file import (
+    _MetaFile, DEFAULT_HDF5_KEYS as CORE_HDF5_KEYS
+)
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+DEFAULT_HDF5_KEYS = CORE_HDF5_KEYS
 
-class PlotGeneration(object):
-    """Wrapper class for _GWPlotGeneration and _CorePlotGeneration
-
-    Parameters
-    ----------
-    inputs: argparse.Namespace
-        Namespace object containing the command line options
-    colors: list, optional
-        colors that you wish to use to distinguish different result files
-    """
-    def __init__(self, inputs, colors="default", gw=False):
-        self.inputs = inputs
-        self.colors = colors
-        self.gw = gw
-        self.generate_plots()
-
-    def generate_plots(self):
-        """Generate all plots for all result files passed
-        """
-        logger.info("Starting to generate plots")
-        if self.gw and self.inputs.public:
-            object = _PublicGWPlotGeneration(self.inputs, colors=self.colors)
-            self.ligo_skymap_PID = object.ligo_skymap_PID
-        elif self.gw:
-            object = _GWPlotGeneration(self.inputs, colors=self.colors)
-        else:
-            object = _CorePlotGeneration(self.inputs, colors=self.colors)
-        object.generate_plots()
-        if self.gw:
-            self.ligo_skymap_PID = object.ligo_skymap_PID
-        logger.info("Finished generating plots")
 
+class _GWMetaFile(_MetaFile):
+    """This class handles the creation of a meta file storing all information
+    from the analysis
 
-class _CorePlotGeneration(PostProcessing):
-    """Class to generate all plots associated with the Core module
-
-    Parameters
+    Attributes
     ----------
-    inputs: argparse.Namespace
-        Namespace object containing the command line options
-    colors: list, optional
-        colors that you wish to use to distinguish different result files
+    meta_file: str
+        name of the meta file storing all information
     """
-    def __init__(self, inputs, colors="default"):
-        from pesummary.core.plots.main import _PlotGeneration
-
-        super(_CorePlotGeneration, self).__init__(inputs, colors)
-        self.plotting_object = _PlotGeneration(
-            webdir=self.webdir, labels=self.labels,
-            samples=self.samples, kde_plot=self.kde_plot,
-            existing_labels=self.existing_labels,
-            existing_injection_data=self.existing_injection_data,
-            existing_samples=self.existing_samples,
-            same_parameters=self.same_parameters,
-            injection_data=self.injection_data,
-            colors=self.colors, custom_plotting=self.custom_plotting,
-            add_to_existing=self.add_to_existing, priors=self.priors,
-            include_prior=self.include_prior, weights=self.weights,
-            disable_comparison=self.disable_comparison,
-            linestyles=self.linestyles,
-            disable_interactive=self.disable_interactive,
-            disable_corner=self.disable_corner,
-            multi_process=self.multi_process, mcmc_samples=self.mcmc_samples,
-            corner_params=self.corner_params
+    def __init__(
+        self, samples, labels, config, injection_data, file_versions,
+        file_kwargs, calibration=None, psd=None, approximant=None, webdir=None,
+        result_files=None, hdf5=False, existing_version=None,
+        existing_label=None, existing_samples=None, existing_psd=None,
+        existing_calibration=None, existing_approximant=None,
+        existing_config=None, existing_injection=None,
+        existing_metadata=None, priors={}, outdir=None, existing=None,
+        existing_priors={}, existing_metafile=None, package_information={},
+        mcmc_samples=False, skymap=None, existing_skymap=None,
+        filename=None, external_hdf5_links=False, hdf5_compression=None,
+        history=None, descriptions=None, gwdata=None
+    ):
+        self.calibration = calibration
+        self.psds = psd
+        self.approximant = approximant
+        self.existing_psd = existing_psd
+        self.existing_calibration = existing_calibration
+        self.existing_approximant = existing_approximant
+        self.skymap = skymap
+        self.existing_skymap = existing_skymap
+        self.gwdata = gwdata
+        super(_GWMetaFile, self).__init__(
+            samples, labels, config, injection_data, file_versions,
+            file_kwargs, webdir=webdir, result_files=result_files, hdf5=hdf5,
+            priors=priors, existing_version=existing_version,
+            existing_label=existing_label, existing_samples=existing_samples,
+            existing_injection=existing_injection,
+            existing_metadata=existing_metadata,
+            existing_config=existing_config, existing_priors=existing_priors,
+            outdir=outdir, package_information=package_information,
+            existing=existing, existing_metafile=existing_metafile,
+            mcmc_samples=mcmc_samples, filename=filename,
+            external_hdf5_links=external_hdf5_links,
+            hdf5_compression=hdf5_compression, history=history,
+            descriptions=descriptions
         )
+        if self.calibration is None:
+            self.calibration = {label: {} for label in self.labels}
+        if self.psds is None:
+            self.psds = {label: {} for label in self.labels}
 
-    def generate_plots(self):
-        """Generate all plots within the Core module
+    def _make_dictionary(self):
+        """Generate a single dictionary which stores all information
         """
-        self.plotting_object.generate_plots()
-
-
-class _GWPlotGeneration(GWPostProcessing):
-    """Class to generate all plots associated with the GW module
-
-    Parameters
-    ----------
-    inputs: argparse.Namespace
-        Namespace object containing the command line options
-    colors: list, optional
-        colors that you wish to use to distinguish different result files
-    """
-    def __init__(self, inputs, colors="default"):
-        from pesummary.gw.plots.main import _PlotGeneration
-
-        super(_GWPlotGeneration, self).__init__(inputs, colors)
-        self.plotting_object = _PlotGeneration(
-            webdir=self.webdir, labels=self.labels,
-            samples=self.samples, kde_plot=self.kde_plot,
-            existing_labels=self.existing_labels,
-            existing_injection_data=self.existing_injection_data,
-            existing_samples=self.existing_samples,
-            existing_file_kwargs=self.existing_file_kwargs,
-            existing_approximant=self.existing_approximant,
-            existing_metafile=self.existing_metafile,
-            same_parameters=self.same_parameters,
-            injection_data=self.injection_data,
-            result_files=self.result_files,
-            file_kwargs=self.file_kwargs,
-            colors=self.colors, custom_plotting=self.custom_plotting,
-            add_to_existing=self.add_to_existing, priors=self.priors,
-            no_ligo_skymap=self.no_ligo_skymap,
-            nsamples_for_skymap=self.nsamples_for_skymap,
-            detectors=self.detectors, maxL_samples=self.maxL_samples,
-            gwdata=self.gwdata, calibration=self.calibration,
-            psd=self.psd, approximant=self.approximant,
-            multi_threading_for_skymap=self.multi_threading_for_skymap,
-            pepredicates_probs=self.pepredicates_probs,
-            include_prior=self.include_prior, publication=self.publication,
-            existing_psd=self.existing_psd,
-            existing_calibration=self.existing_calibration, weights=self.weights,
-            linestyles=self.linestyles,
-            disable_comparison=self.disable_comparison,
-            disable_interactive=self.disable_interactive,
-            disable_corner=self.disable_corner,
-            publication_kwargs=self.publication_kwargs,
-            multi_process=self.multi_process, mcmc_samples=self.mcmc_samples,
-            skymap=self.skymap, existing_skymap=self.existing_skymap,
-            corner_params=self.corner_params
-        )
-        self.ligo_skymap_PID = self.plotting_object.ligo_skymap_PID
-
-    def generate_plots(self):
-        """Generate all plots within the GW module
+        super(_GWMetaFile, self)._make_dictionary()
+        for num, label in enumerate(self.labels):
+            cond = all(self.calibration[label] != j for j in [{}, None])
+            if self.calibration != {} and cond:
+                self.data[label]["calibration_envelope"] = {
+                    key: item for key, item in self.calibration[label].items()
+                    if item is not None
+                }
+            else:
+                self.data[label]["calibration_envelope"] = {}
+            if self.psds != {} and all(self.psds[label] != j for j in [{}, None]):
+                self.data[label]["psds"] = {
+                    key: item for key, item in self.psds[label].items() if item
+                    is not None
+                }
+            else:
+                self.data[label]["psds"] = {}
+            if self.approximant is not None and self.approximant[label] is not None:
+                self.data[label]["approximant"] = self.approximant[label]
+            else:
+                self.data[label]["approximant"] = {}
+            if self.skymap is not None and len(self.skymap):
+                if self.skymap[label] is not None:
+                    self.data[label]["skymap"] = {
+                        "data": self.skymap[label],
+                        "meta_data": {
+                            key: item for key, item in
+                            self.skymap[label].meta_data.items()
+                        }
+                    }
+        if self.gwdata is not None and len(self.gwdata):
+            try:
+                from gwpy.types.io.hdf5 import format_index_array_attrs
+                from pesummary.utils.dict import Dict
+                self.data["strain"] = {}
+                for key, item in self.gwdata.items():
+                    if item is None:
+                        continue
+                    _name = item.name if item.name is not None else "unknown_name"
+                    self.data["strain"][key] = Dict(
+                        {_name: item.value},
+                        extra_kwargs=format_index_array_attrs(item)
+                    )
+            except Exception:
+                logger.warning(
+                    "Failed to store the gravitational wave strain data"
+                )
+
+    @staticmethod
+    def save_to_hdf5(
+        data, labels, samples, meta_file, no_convert=False, mcmc_samples=False,
+        external_hdf5_links=False, compression=None, _class=None, gwdata=None
+    ):
+        """Save the metafile as a hdf5 file
         """
-        self.plotting_object.generate_plots()
-
+        if gwdata is not None and len(gwdata):
+            extra_keys = CORE_HDF5_KEYS + ["strain"]
+        else:
+            extra_keys = CORE_HDF5_KEYS
+        _MetaFile.save_to_hdf5(
+            data, labels, samples, meta_file, no_convert=no_convert,
+            extra_keys=extra_keys, mcmc_samples=mcmc_samples, _class=_class,
+            external_hdf5_links=external_hdf5_links, compression=compression
+        )
 
-class _PublicGWPlotGeneration(GWPostProcessing):
-    """Class to generate all plots associated with the GW module
 
-    Parameters
-    ----------
-    inputs: argparse.Namespace
-        Namespace object containing the command line options
-    colors: list, optional
-        colors that you wish to use to distinguish different result files
+class _TGRMetaFile(_GWMetaFile):
+    """Class to create a single file to contain TGR data
     """
-    def __init__(self, inputs, colors="default"):
-        from pesummary.gw.plots.public import _PlotGeneration
-
-        super(_PublicGWPlotGeneration, self).__init__(inputs, colors)
-        self.plotting_object = _PlotGeneration(
-            webdir=self.webdir, labels=self.labels,
-            samples=self.samples, kde_plot=self.kde_plot,
-            existing_labels=self.existing_labels,
-            existing_injection_data=self.existing_injection_data,
-            existing_samples=self.existing_samples,
-            existing_file_kwargs=self.existing_file_kwargs,
-            existing_approximant=self.existing_approximant,
-            existing_metafile=self.existing_metafile,
-            same_parameters=self.same_parameters,
-            injection_data=self.injection_data,
-            result_files=self.result_files,
-            file_kwargs=self.file_kwargs,
-            colors=self.colors, custom_plotting=self.custom_plotting,
-            add_to_existing=self.add_to_existing, priors=self.priors,
-            no_ligo_skymap=self.no_ligo_skymap,
-            nsamples_for_skymap=self.nsamples_for_skymap,
-            detectors=self.detectors, maxL_samples=self.maxL_samples,
-            gwdata=self.gwdata, calibration=self.calibration,
-            psd=self.psd, approximant=self.approximant,
-            multi_threading_for_skymap=self.multi_threading_for_skymap,
-            pepredicates_probs=self.pepredicates_probs,
-            include_prior=self.include_prior, publication=self.publication,
-            existing_psd=self.existing_psd,
-            existing_calibration=self.existing_calibration, weights=self.weights,
-            linestyles=self.linestyles,
-            disable_comparison=self.disable_comparison,
-            disable_interactive=self.disable_interactive,
-            disable_corner=self.disable_corner,
-            publication_kwargs=self.publication_kwargs,
-            multi_process=self.multi_process, mcmc_samples=self.mcmc_samples,
-            skymap=self.skymap, existing_skymap=self.existing_skymap,
-            corner_params=self.corner_params
+    def __init__(
+        self, samples, labels, imrct_data=None, webdir=None, outdir=None,
+        file_kwargs={}
+    ):
+        super(_TGRMetaFile, self).__init__(
+            samples, labels, None, None, None, None, webdir=webdir,
+            outdir=outdir, filename="tgr_samples.h5", hdf5=True
         )
-        self.ligo_skymap_PID = self.plotting_object.ligo_skymap_PID
-
-    def generate_plots(self):
-        """Generate all plots within the GW module
+        self.tgr_data = {"imrct": imrct_data}
+        self.file_kwargs = file_kwargs
+        for key in self.tgr_data.keys():
+            if key not in self.file_kwargs:
+                self.file_kwargs[key] = {}
+
+    @staticmethod
+    def convert_posterior_samples_to_numpy(labels, samples, mcmc_samples=False):
+        """Convert a dictionary of multiple posterior samples from a
+        column-major dictionary to a row-major numpy array
+
+        Parameters
+        ----------
+        labels: list
+            list of unique labels for each analysis
+        samples: MultiAnalysisSamplesDict
+            dictionary of multiple posterior samples to convert to a numpy
+            array.
+        mcmc_samples: Bool, optional
+            if True, the dictionary contains seperate mcmc chains
+
+        Examples
+        --------
+        >>> dictionary = MultiAnalysisSamplesDict(
+        ...     {"label": {"mass_1": [1,2,3], "mass_2": [1,2,3]}}
+        ... )
+        >>> dictionary = _Metafile.convert_posterior_samples_to_numpy(
+        ...     dictionary.keys(), dictionary
+        ... )
+        >>> print(dictionary)
+        ... {"label": rec.array([(1., 1.), (2., 2.), (3., 3.)],
+        ...           dtype=[('mass_1', '<f4'), ('mass_2', '<f4')])}
         """
-        self.plotting_object.generate_plots()
-
-
-def command_line():
-    """Generate an Argument Parser object to control the command line options
-    """
-    import argparse
-
-    parser = argparse.ArgumentParser(description=__doc__)
-    parser.add_argument("-w", "--webdir", dest="webdir",
-                        help="make page and plots in DIR", metavar="DIR",
-                        default="./")
-    parser.add_argument("-s", "--samples", dest="samples", nargs='+',
-                        help="Path to PESummary metafile", default=None)
-    parser.add_argument("--labels", dest="labels",
-                        help="labels used to distinguish runs", nargs='+',
-                        default=None)
-    parser.add_argument("--plot", dest="plot",
-                        help=("name of the publication plot you wish to "
-                              "produce"), default="2d_contour",
-                        choices=["1d_histogram", "sample_evolution",
-                                 "autocorrelation", "skymap"])
-    parser.add_argument("--parameters", dest="parameters", nargs="+",
-                        help=("parameters of the 2d contour plot you wish to "
-                              "make"), default=None)
-    parser.add_argument("--plot_kwargs", help="Optional plotting kwargs",
-                        action=DictionaryAction, nargs="+", default={})
-    parser.add_argument("--inj", help="Injected value", default=None)
-    parser.add_argument("--kde_plot", action="store_true",
-                        help="plot a kde rather than a histogram",
-                        default=False)
-    parser.add_argument("--burnin", dest="burnin",
-                        help="Number of samples to remove as burnin",
-                        default=None)
-    parser.add_argument("--disable_comparison", action="store_true", default=False,
-                        help="Whether to make comparison plots when multiple "
-                             "results are passed.")
-    return parser
-
-
-def check_inputs(opts):
-    """Check that the inputs are compatible with `summaryplots`
-    """
-    from pesummary.utils.exceptions import InputError
-
-    base = "Please provide {} for each result file"
-    if opts.inj is None:
-        opts.inj = [float("nan")] * len(opts.samples)
-    if opts.labels is None:
-        opts.labels = [i for i in range(len(opts.samples))]
-    if len(opts.samples) != len(opts.labels):
-        raise InputError(base.format("a label"))
-    if len(opts.samples) != len(opts.inj):
-        raise InputError(base.format("the injected value"))
-    if opts.burnin is not None:
-        opts.burnin = int(opts.burnin)
-    return opts
-
-
-def read_input_file(path_to_file):
-    """Use PESummary to read a result file
-
-    Parameters
-    ----------
-    path_to_file: str
-        path to the results file
-    """
-    from pesummary.gw.file.read import read
-
-    f = read(path_to_file)
-    return f
-
-
-def oned_histogram_plot(opts):
-    """Make a 1d histogram plot
-    """
-    for num, samples in enumerate(opts.samples):
-        data = read(samples)
-        for parameter in opts.parameters:
-            _PlotGeneration._oned_histogram_plot(
-                opts.webdir, opts.labels[num], parameter,
-                data.samples_dict[parameter][opts.burnin:],
-                latex_labels[parameter], opts.inj[num], kde=opts.kde_plot
-            )
-
-
-def sample_evolution_plot(opts):
-    """Make a sample evolution plot
-    """
-    for num, samples in enumerate(opts.samples):
-        data = read(samples)
-        for parameter in opts.parameters:
-            _PlotGeneration._sample_evolution_plot(
-                opts.webdir, opts.labels[num], parameter,
-                data.samples_dict[parameter][opts.burnin:],
-                latex_labels[parameter], opts.inj[num]
-            )
-
+        _convert_function = _GWMetaFile._convert_posterior_samples_to_numpy
+        converted_samples = {label: {} for label in labels}
+        for label in labels:
+            for key in ["inspiral", "postinspiral"]:
+                if "{}:{}".format(label, key) in samples.keys():
+                    _samples_key = "{}:{}".format(label, key)
+                else:
+                    _samples_key = key
+                converted_samples[label][key] = _convert_function(
+                    samples[_samples_key], mcmc_samples=False
+                )
+        return converted_samples
+
+    @staticmethod
+    def save_to_hdf5(*args, **kwargs):
+        """Save the metafile as a hdf5 file
+        """
+        return _GWMetaFile.save_to_hdf5(*args, _class=_TGRMetaFile, **kwargs)
 
-def autocorrelation_plot(opts):
-    """Make an autocorrelation plot
-    """
-    for num, samples in enumerate(opts.samples):
-        data = read(samples)
-        for parameter in opts.parameters:
-            _PlotGeneration._autocorrelation_plot(
-                opts.webdir, opts.labels[num], parameter,
-                data.samples_dict[parameter][opts.burnin:]
+    def _make_dictionary(self):
+        """Generate a single dictionary which stores all information
+        """
+        dictionary = self._dictionary_structure
+        dictionary.update(
+            {
+                label: {key: {} for key in self.tgr_data.keys()} for label in
+                self.labels
+            }
+        )
+        for label in self.labels:
+            dictionary[label]["posterior_samples"] = {}
+            if self.tgr_data["imrct"] is not None:
+                for analysis in ["inspiral", "postinspiral"]:
+                    try:
+                        _samples = self.samples["{}:{}".format(label, analysis)]
+                    except KeyError:
+                        _samples = self.samples[analysis]
+                    parameters = _samples.keys()
+                    samples = np.array([_samples[i] for i in parameters]).T
+                    dictionary[label]["posterior_samples"][analysis] = {
+                        "parameter_names": list(parameters),
+                        "samples": samples.tolist()
+                    }
+                deviations = "final_mass_final_spin_deviations"
+                _imrct_data = self.tgr_data["imrct"][label][deviations]
+                dictionary[label]["imrct"] = {
+                    "final_mass_deviation": _imrct_data.x,
+                    "final_spin_deviation": _imrct_data.y,
+                    "pdf": _imrct_data.probs,
+                }
+                _kwargs = self.file_kwargs["imrct"]
+                if label in _kwargs.keys():
+                    _kwargs = _kwargs[label]
+                dictionary[label]["imrct"]["meta_data"] = _kwargs
+        self.data = dictionary
+
+
+class GWMetaFile(object):
+    """This class handles the creation of a metafile storing all information
+    from the analysis
+    """
+    def __init__(self, inputs, history=None):
+        logger.info("Starting to generate the meta file")
+        if inputs.add_to_existing:
+            existing = inputs.existing
+            existing_metafile = inputs.existing_metafile
+            existing_samples = inputs.existing_samples
+            existing_labels = inputs.existing_labels
+            existing_psd = inputs.existing_psd
+            existing_calibration = inputs.existing_calibration
+            existing_config = inputs.existing_config
+            existing_approximant = inputs.existing_approximant
+            existing_injection = inputs.existing_injection_data
+            existing_version = inputs.existing_file_version
+            existing_metadata = inputs.existing_file_kwargs
+            existing_priors = inputs.existing_priors
+        else:
+            existing_metafile = None
+            existing_samples = None
+            existing_labels = None
+            existing_psd = None
+            existing_calibration = None
+            existing_config = None
+            existing_approximant = None
+            existing_injection = None
+            existing_version = None
+            existing_metadata = None
+            existing = None
+            existing_priors = {}
+
+        meta_file = _GWMetaFile(
+            inputs.samples, inputs.labels, inputs.config, inputs.injection_data,
+            inputs.file_version, inputs.file_kwargs, calibration=inputs.calibration,
+            psd=inputs.psd, hdf5=inputs.hdf5, webdir=inputs.webdir,
+            result_files=inputs.result_files, existing_version=existing_version,
+            existing_label=existing_labels, existing_samples=existing_samples,
+            existing_psd=existing_psd, existing_calibration=existing_calibration,
+            existing_approximant=existing_approximant,
+            existing_injection=existing_injection,
+            existing_metadata=existing_metadata,
+            existing_config=existing_config, priors=inputs.priors,
+            existing_priors=existing_priors, existing=existing,
+            existing_metafile=existing_metafile, approximant=inputs.approximant,
+            package_information=inputs.package_information,
+            mcmc_samples=inputs.mcmc_samples, skymap=inputs.skymap,
+            existing_skymap=inputs.existing_skymap, filename=inputs.filename,
+            external_hdf5_links=inputs.external_hdf5_links,
+            hdf5_compression=inputs.hdf5_compression, history=history,
+            gwdata=inputs.gwdata, descriptions=inputs.descriptions
+        )
+        meta_file.make_dictionary()
+        if not inputs.hdf5:
+            meta_file.save_to_json(meta_file.data, meta_file.meta_file)
+        else:
+            meta_file.save_to_hdf5(
+                meta_file.data, meta_file.labels, meta_file.samples,
+                meta_file.meta_file, mcmc_samples=meta_file.mcmc_samples,
+                external_hdf5_links=meta_file.external_hdf5_links,
+                compression=meta_file.hdf5_compression,
+                gwdata=meta_file.gwdata
             )
-
-
-def skymap_plot(opts):
-    """Make a skymap plot
-    """
-    for num, samples in enumerate(opts.samples):
-        data = read(samples)
-        _PlotGeneration._skymap_plot(
-            opts.webdir, data.samples_dict["ra"][opts.burnin:],
-            data.samples_dict["dec"][opts.burnin:], opts.labels[num]
+        meta_file.save_to_dat()
+        meta_file.write_marginalized_posterior_to_dat()
+        logger.info(
+            "Finishing generating the meta file. The meta file can be viewed "
+            "here: {}".format(meta_file.meta_file)
         )
 
 
-def main(args=None):
-    """The main interface for `summaryplots`
+class TGRMetaFile(_GWMetaFile):
+    """Class to create a single file to contain TGR data
     """
-    latex_labels.update(GWlatex_labels)
-    parser = command_line()
-    opts = parser.parse_args(args=args)
-    opts = check_inputs(opts)
-    make_dir(opts.webdir)
-    func_map = {
-        "1d_histogram": oned_histogram_plot,
-        "sample_evolution": sample_evolution_plot,
-        "autocorrelation": autocorrelation_plot,
-        "skymap": skymap_plot
-    }
-    func_map[opts.plot](opts)
-
-
-if __name__ == "__main__":
-    main()
+    def __init__(
+        self, samples, labels, imrct_data=None, webdir=None, outdir=None,
+        file_kwargs={}
+    ):
+        logger.info("Starting to generate the meta file")
+        meta_file = _TGRMetaFile(
+            samples, labels, imrct_data=imrct_data, webdir=webdir,
+            outdir=outdir, file_kwargs=file_kwargs
+        )
+        meta_file.make_dictionary()
+        meta_file.save_to_hdf5(
+            meta_file.data, meta_file.labels, meta_file.samples,
+            meta_file.meta_file, mcmc_samples=False
+        )
+        logger.info(
+            "Finished generating the meta file. The meta file can be viewed "
+            "here: {}".format(meta_file.meta_file)
+        )
```

### Comparing `pesummary-0.9.1/pesummary/cli/summarypublication.py` & `pesummary-1.0.0/pesummary/cli/summarypublication.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,79 +1,61 @@
 #! /usr/bin/env python
 
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import pesummary
 from pesummary.gw.file.read import read as GWRead
 from pesummary.gw.plots.latex_labels import GWlatex_labels
 from pesummary.gw.plots import publication as pub
 from pesummary.core.plots import population as pop
 from pesummary.core.plots.latex_labels import latex_labels
-from pesummary.utils.utils import make_dir, logger
-from pesummary.core.command_line import DictionaryAction
-import argparse
+from pesummary.utils.utils import make_dir, logger, _check_latex_install
+from pesummary.core.cli.parser import ArgumentParser as _ArgumentParser
+from pesummary.core.cli.actions import DictionaryAction
 import seaborn
 import numpy as np
 
-
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 __doc__ = """This executable is used to generate publication quality plots given
 result files"""
+_check_latex_install()
 
 
-def command_line():
-    """Generate an Argument Parser object to control the command line options
-    """
-    parser = argparse.ArgumentParser(description=__doc__)
-    parser.add_argument("-w", "--webdir", dest="webdir",
-                        help="make page and plots in DIR", metavar="DIR",
-                        default=None)
-    parser.add_argument("-s", "--samples", dest="samples",
-                        help="Posterior samples hdf5 file", nargs='+',
-                        default=None)
-    parser.add_argument("--labels", dest="labels",
-
-                        help="labels used to distinguish runs", nargs='+',
-                        default=None)
-    parser.add_argument("--plot", dest="plot",
-                        help=("name of the publication plot you wish to "
-                              "produce"), default="2d_contour",
-                        choices=[
-                            "2d_contour", "violin", "spin_disk",
-                            "population_scatter", "population_scatter_error"
-                        ])
-    parser.add_argument("--parameters", dest="parameters", nargs="+",
-                        help=("parameters of the 2d contour plot you wish to "
-                              "make"), default=None)
-    parser.add_argument("--publication_kwargs",
-                        help="Optional kwargs for publication plots",
-                        action=DictionaryAction, nargs="+", default={})
-    parser.add_argument("--palette", dest="palette",
-                        help="Color palette to use to distinguish result files",
-                        default="colorblind")
-    parser.add_argument("--colors", dest="colors",
-                        help="Colors you wish to use to distinguish result files",
-                        nargs='+', default=None)
-    parser.add_argument("--linestyles", dest="linestyles",
-                        help=("Linestyles you wish to use to distinguish result "
-                              "files"),
-                        nargs='+', default=None)
-    return parser
+class ArgumentParser(_ArgumentParser):
+    def _pesummary_options(self):
+        options = super(ArgumentParser, self)._pesummary_options()
+        options.update(
+            {
+                "--plot": {
+                    "help": "name of the publication plot you wish to produce",
+                    "default": "2d_contour",
+                    "choices": [
+                        "2d_contour", "violin", "spin_disk",
+                        "population_scatter", "population_scatter_error"
+                    ],
+                },
+                "--parameters": {
+                    "nargs": "+",
+                    "help": "parameters of the 2d contour plot you wish to make",
+                },
+                "--publication_kwargs": {
+                    "help": "Optional kwargs for publication plots",
+                    "nargs": "+",
+                    "default": {},
+                    "action": DictionaryAction
+                },
+                "--levels": {
+                    "default": [0.9],
+                    "nargs": "+",
+                    "help": "Contour levels you wish to plot",
+                    "type": float
+                }
+            }
+        )
+        return options
 
 
 def draw_specific_samples(param, parameters, samples):
     """Return samples for a given parameter
 
     param: str
         parameter that you wish to get samples for
@@ -195,15 +177,16 @@
         twod_samples = [[j, k] for j, k in zip(samples1, samples2)]
         gridsize = (
             opts.publication_kwargs["gridsize"] if "gridsize" in
             opts.publication_kwargs.keys() else 100
         )
         fig, ax = pub.twod_contour_plots(
             i, twod_samples, opts.labels, latex_labels, colors=colors,
-            linestyles=linestyles, gridsize=gridsize, return_ax=True
+            linestyles=linestyles, gridsize=gridsize, levels=opts.levels,
+            return_ax=True
         )
         current_xlow, current_xhigh = ax.get_xlim()
         current_ylow, current_yhigh = ax.get_ylim()
         keys = opts.publication_kwargs.keys()
         if "xlow" in keys and "xhigh" in keys:
             ax.set_xlim(
                 [
@@ -257,28 +240,28 @@
         try:
             ind = [j.index(i) for j in parameters]
             samples = [[k[ind[num]] for k in l] for num, l in
                        enumerate(samples)]
             fig = pub.violin_plots(i, samples, opts.labels, latex_labels)
             fig.savefig("%s/violin_plot_%s.png" % (opts.webdir, i))
             fig.close()
-        except Exception:
-            logger.info("Failed to generate a violin plot for %s" % (i))
+        except Exception as e:
+            logger.info(
+                "Failed to generate a violin plot for %s because %s" % (i, e)
+            )
             continue
 
 
 def make_spin_disk_plot(opts):
     """Make a spin disk plot
     """
-    import seaborn
-
     colors, linestyles = get_colors_and_linestyles(opts)
     parameters, samples = read_samples(opts.samples)
 
-    required_parameters = ["a_1", "a_2", "tilt_1", "tilt_2"]
+    required_parameters = ["a_1", "a_2", "cos_tilt_1", "cos_tilt_2"]
     for num, i in enumerate(parameters):
         if not all(j in i for j in required_parameters):
             logger.info("Failed to generate spin disk plots for %s because "
                         "%s are not in the result file" % (
                             opts.labels[num],
                             " and ".join(required_parameters)))
             continue
@@ -350,15 +333,22 @@
         fig.close()
 
 
 def main(args=None):
     """Top level interface for `summarypublication`
     """
     latex_labels.update(GWlatex_labels)
-    parser = command_line()
+    parser = ArgumentParser(description=__doc__)
+    parser.add_known_options_to_parser(
+        [
+            "--webdir", "--samples", "--labels", "--plot", "--parameters",
+            "--publication_kwargs", "--colors", "--palette", "--linestyles",
+            "--levels"
+        ]
+    )
     opts = parser.parse_args(args=args)
     make_dir(opts.webdir)
     func_map = {"2d_contour": make_2d_contour_plot,
                 "violin": make_violin_plot,
                 "spin_disk": make_spin_disk_plot,
                 "population_scatter": make_population_scatter_plot,
                 "population_scatter_error": make_population_scatter_plot}
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/injection.py` & `pesummary-1.0.0/pesummary/utils/parameters.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,100 +1,108 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-
-import numpy as np
-from pesummary.core.file.formats.base_read import Read
-from pesummary.core.file.injection import Injection
+# Licensed under an MIT style license -- see LICENSE.md
 
+from .list import List
+from pesummary.gw.file.standard_names import descriptive_names
 
-class GWInjection(Injection):
-    """Class to handle injection information
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
+
+class Parameter(str):
+    """Class to handle a single parameter
 
     Parameters
     ----------
-    parameters: list
-        list of parameters
-    samples: nd list
-        list of samples for each parameter
-    conversion: Bool, optional
-        If True, convert all injection parameters
-    conversion_kwargs: dict, optional
-        kwargs that are passed to the
-        `pesummary.gw.file.conversions._Conversion` class
+    parameter: str
+        name of the parameter
+    description: str, optional
+        text describing what parameter represents. Default, look to see if
+        pesummary has a descriptive name for parameter else 'Unknown parameter
+        description
+
+    Attributes
+    ----------
+    description: str
+        return text describing what parameter represents
     """
-    def __init__(self, *args, conversion=True, conversion_kwargs={}, **kwargs):
-        super(GWInjection, self).__init__(*args, **kwargs)
-        if conversion:
-            from pesummary.gw.file.conversions import _Conversion
-
-            converted = _Conversion(self, **conversion_kwargs)
-            if conversion_kwargs.get("return_kwargs", False):
-                converted = converted[0]
-            for key, value in converted.items():
-                self[key] = value
-
-    @classmethod
-    def read(cls, path, conversion=True, conversion_kwargs={}, **kwargs):
-        """Read an injection file and initalize the Injection class
-
-        Parameters
-        ----------
-        path: str
-            Path to the injection file you wish to read
-        conversion: Bool, optional
-            If True, convert all injection parameters
-        conversion_kwargs: dict, optional
-            kwargs that are passed to the
-            `pesummary.gw.file.conversions._Conversion` class
-        **kwargs: dict
-            All kwargs passed to the format specific read function
-        """
-        if Read.extension_from_path(path) == "xml":
-            data = GWInjection._grab_injection_from_xml_file(path, **kwargs)
-            return cls(
-                data, conversion=conversion, conversion_kwargs=conversion_kwargs
-            )
+    def __new__(self, parameter, **kwargs):
+        return super(Parameter, self).__new__(self, parameter)
+
+    def __init__(self, parameter, description=None):
+        self._parameter = parameter
+        self._description = "Unknown parameter description"
+        if description is None:
+            if parameter in descriptive_names.keys():
+                self._description = descriptive_names[parameter]
         else:
-            return super(GWInjection, cls).read(
-                path, conversion=conversion, conversion_kwargs=conversion_kwargs
-            )
+            self._description = description
+
+    @property
+    def description(self):
+        return self._description
+
+    def __repr__(self):
+        return repr(self._parameter)
+
+
+class Parameters(List):
+    """Class to store the list of parameters
+
+    Parameters
+    ----------
+    args: tuple
+        all arguments are passed to the list class
+    **kwargs: dict
+        all kwargs are passed to the list class
 
-    @staticmethod
-    def _grab_injection_from_xml_file(
-        injection_file, format="ligolw", tablename="sim_inspiral", num=0
-    ):
-        """Grab the data from an xml injection file
-
-        Parameters
-        ----------
-        injection_file: str
-            Path to the injection file you wish to read
-        format: str, optional
-            The format of your xml. Default is 'ligolw'
-        tablename: str, optional
-            Name of the table you wish to load. Default is 'sim_inspiral'
-        num: int, optional
-            The injection row you wish to load. Default is 0
-        """
-        from gwpy.table import Table
-        from pesummary.gw.file.standard_names import standard_names
-
-        table = Table.read(
-            injection_file, format=format, tablename=tablename
-        )
-        injection = {
-            standard_names[key]: [table[key][num]] for key in table.colnames
-            if key in standard_names.keys()
-        }
-        return injection
+    Attributes
+    ----------
+    added: list
+        list of parameters that have been appended to the original list
+    """
+    def __init__(self, *args, cls=Parameter, **kwargs):
+        self._cls = cls
+        if len(args) == 1:
+            super(Parameters, self).__init__(*args, cls=Parameter, **kwargs)
+        else:
+            _args = list(args)
+            _args[2] = Parameter
+            super(Parameters, self).__init__(*_args, **kwargs)
+
+    def __iter__(self, *args, **kwargs):
+        _iter = super(Parameters, self).__iter__(*args, **kwargs)
+        for _value in _iter:
+            yield self._cls(_value)
+
+
+class MultiAnalysisParameters(List):
+    """Class to store a multidimensional list of parameters
+
+    Parameters
+    ----------
+    args: tuple
+        all arguments are passed to the list class
+    **kwargs: dict
+        all kwargs are passed to the list class
+
+    Attributes
+    ----------
+    added: list
+        list of parameters that have been appended to the original list
+    """
+    def __init__(self, *args, cls=Parameters, **kwargs):
+        self._cls = cls
+        if len(args) == 1:
+            super(MultiAnalysisParameters, self).__init__(
+                *args, cls=cls, **kwargs
+            )
+        else:
+            _args = list(args)
+            _args[2] = Parameters
+            super(MultiAnalysisParameters, self).__init__(*_args, **kwargs)
+
+    def __getitem__(self, *args, **kwargs):
+        self.cls = None
+        output = super(List, self).__getitem__(*args, **kwargs)
+        self.cls = self._cls
+        if isinstance(output[0], list):
+            return [self.cls(value) for value in output]
+        return self.cls(output)
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/formats/bilby.py` & `pesummary-1.0.0/pesummary/gw/file/formats/bilby.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,31 +1,49 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
-import os
 import numpy as np
 from pesummary.core.file.formats.bilby import Bilby as CoreBilby
-from pesummary.gw.file.formats.base_read import GWRead
+from pesummary.gw.file.formats.base_read import GWSingleAnalysisRead
 from pesummary.gw.plots.latex_labels import GWlatex_labels
 from pesummary.utils.utils import logger
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
-def prior_samples_from_file(path, cls="BBHPriorDict", nsamples=5000):
+
+def read_bilby(
+    path, disable_prior=False, latex_dict=GWlatex_labels,
+    complex_params=["matched_filter_snr", "optimal_snr"], **kwargs
+):
+    """Grab the parameters and samples in a bilby file
+
+    Parameters
+    ----------
+    path: str
+        path to the result file you wish to read in
+    disable_prior: Bool, optional
+        if True, do not collect prior samples from the `bilby` result file.
+        Default False
+    complex_params: list, optional
+        list of parameters stored in the bilby result file which are complex
+        and you wish to store the `amplitude` and `angle` as seperate
+        posterior distributions
+    latex_dict: dict, optional
+        list of latex labels for each parameter
+    """
+    from pesummary.core.file.formats.bilby import (
+        read_bilby as _read_bilby
+    )
+
+    return _read_bilby(
+        path, disable_prior=disable_prior, latex_dict=latex_dict,
+        complex_params=complex_params, _bilby_class=Bilby, **kwargs
+    )
+
+
+def prior_samples_from_file(path, cls="BBHPriorDict", nsamples=5000, **kwargs):
     """Return a dict of prior samples from a `bilby` prior file
 
     Parameters
     ----------
     path: str
         path to a `bilby` prior file
     cls: str, optional
@@ -36,18 +54,18 @@
     from pesummary.core.file.formats.bilby import (
         prior_samples_from_file as _prior_samples_from_file
     )
     from bilby.gw import prior
 
     if isinstance(cls, str):
         cls = getattr(prior, cls)
-    return _prior_samples_from_file(path, cls=cls, nsamples=nsamples)
+    return _prior_samples_from_file(path, cls=cls, nsamples=nsamples, **kwargs)
 
 
-class Bilby(GWRead):
+class Bilby(GWSingleAnalysisRead):
     """PESummary wrapper of `bilby` (https://git.ligo.org/lscsoft/bilby). The
     path_to_results_file argument will be passed directly to
     `bilby.core.result.read_in_result`. All functions therefore use `bilby`
     methods and requires `bilby` to be installed.
 
     Parameters
     ----------
@@ -55,32 +73,40 @@
         path to the results file that you wish to read in with `bilby`.
     disable_prior: Bool, optional
         if True, do not collect prior samples from the `bilby` result file.
         Default False
     disable_prior_conversion: Bool, optional
         if True, disable the conversion module from deriving alternative prior
         distributions. Default False
+    pe_algorithm: str
+        name of the algorithm used to generate the posterior samples
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
 
     Attributes
     ----------
     parameters: list
         list of parameters stored in the result file
+    converted_parameters: list
+        list of parameters that have been derived from the sampled distributions
     samples: 2d list
         list of samples stored in the result file
     samples_dict: dict
         dictionary of samples stored in the result file keyed by parameters
     input_version: str
         version of the result file passed.
     extra_kwargs: dict
         dictionary of kwargs that were extracted from the result file
     prior: dict
         dictionary of prior samples extracted from the bilby result file. The
         analytic priors are evaluated for 5000 samples
     injection_parameters: dict
         dictionary of injection parameters stored in the result file
+    converted_parameters: list
+        list of parameters that have been added
 
     Methods
     -------
     to_dat:
         save the posterior samples to a .dat file
     to_latex_table:
         convert the posterior samples to a latex table
@@ -92,39 +118,35 @@
         generate all posterior distributions that may be derived from
         sampled distributions
     """
     def __init__(self, path_to_results_file, **kwargs):
         super(Bilby, self).__init__(path_to_results_file, **kwargs)
         self.load(self._grab_data_from_bilby_file, **kwargs)
 
-    @classmethod
-    def load_file(cls, path, **kwargs):
-        if not os.path.isfile(path):
-            raise Exception("%s does not exist" % (path))
-        return cls(path, **kwargs)
-
     @staticmethod
     def grab_priors(bilby_object, nsamples=5000):
         """Draw samples from the prior functions stored in the bilby file
         """
-        from pesummary.utils.samples_dict import Array
+        from pesummary.utils.array import Array
 
         f = bilby_object
         try:
             samples = f.priors.sample(size=nsamples)
             priors = {key: Array(samples[key]) for key in samples}
         except Exception as e:
             logger.info("Failed to draw prior samples because {}".format(e))
             priors = {}
         return priors
 
     @staticmethod
     def grab_extra_kwargs(bilby_object):
-        """Grab any additional information stored in the lalinference file
+        """Grab any additional information stored in the bilby file
         """
+        from pesummary.core.file.formats.bilby import config_from_object
+
         f = bilby_object
         kwargs = CoreBilby.grab_extra_kwargs(bilby_object)
         try:
             kwargs["meta_data"]["f_ref"] = \
                 f.meta_data["likelihood"]["waveform_arguments"]["reference_frequency"]
         except Exception:
             pass
@@ -143,64 +165,76 @@
         except Exception:
             pass
         try:
             kwargs["meta_data"]["IFOs"] = \
                 " ".join(f.meta_data["likelihood"]["interferometers"].keys())
         except Exception:
             pass
+        _config = config_from_object(f)
+        if len(_config):
+            if "config" in _config.keys():
+                _config = _config["config"]
+            options = [
+                "minimum_frequency", "reference_frequency",
+                "waveform_approximant"
+            ]
+            pesummary_names = ["f_low", "f_ref", "approximant"]
+            for opt, name in zip(options, pesummary_names):
+                if opt in _config.keys():
+                    try:
+                        import ast
+                        _option = ast.literal_eval(_config[opt])
+                    except ValueError:
+                        _option = _config[opt]
+                    if isinstance(_option, dict):
+                        _value = np.min(list(_option.values()))
+                    else:
+                        _value = _option
+                    kwargs["meta_data"][name] = _value
         return kwargs
 
-    @staticmethod
-    def _check_for_calibration_data_in_bilby_file(path):
-        """
-        """
-        from bilby.core.result import read_in_result
-
-        bilby_object = read_in_result(filename=path)
-        parameters = bilby_object.search_parameter_keys
-        if any("recalib_" in i for i in parameters):
-            return True
-        return False
-
     @property
-    def calibration_data_in_results_file(self):
-        """
-        """
-        check = Bilby._check_for_calibration_data_in_bilby_file
-        grab = Bilby._grab_calibration_data_from_bilby_file
-        if self.check_for_calibration_data(check, self.path_to_results_file):
-            return self.grab_calibration_data(grab, self.path_to_results_file)
-        return None
-
-    @staticmethod
-    def _grab_calibration_data_from_bilby_file(path):
-        """
-        """
-        from bilby.core.result import read_in_result
-
-        bilby_object = read_in_result(filename=path)
-        posterior = bilby_object.posterior
-        parameters = list(posterior.keys())
+    def calibration_spline_posterior(self):
+        if not any("recalib_" in i for i in self.parameters):
+            return super(Bilby, self).calibration_spline_posterior
         ifos = np.unique(
-            [param.split('_')[1] for param in parameters if 'recalib_' in param])
-
+            [
+                param.split('_')[1] for param in self.parameters if 'recalib_'
+                in param and "non_reweighted" not in param
+            ]
+        )
         amp_params, phase_params, log_freqs = {}, {}, {}
         for ifo in ifos:
             amp_params[ifo], phase_params[ifo] = [], []
             freq_params = np.sort(
-                [param for param in parameters if 'recalib_%s_frequency_' % (ifo)
-                 in param])
-            log_freqs[ifo] = np.log([posterior[param].iloc[0] for param in freq_params])
+                [
+                    param for param in self.parameters if
+                    'recalib_%s_frequency_' % (ifo) in param
+                    and "non_reweighted" not in param
+                ]
+            )
+            posterior = self.samples_dict
+            log_freqs[ifo] = np.log(
+                [posterior[param][0] for param in freq_params]
+            )
             amp_parameters = np.sort(
-                [param for param in parameters if 'recalib_%s_amplitude_' % (ifo)
-                 in param])
+                [
+                    param for param in self.parameters if
+                    'recalib_%s_amplitude_' % (ifo) in param
+                    and "non_reweighted" not in param
+                ]
+            )
             amplitude = np.array([posterior[param] for param in amp_parameters])
             phase_parameters = np.sort(
-                [param for param in parameters if 'recalib_%s_phase_' % (ifo)
-                 in param])
+                [
+                    param for param in self.parameters if
+                    'recalib_%s_phase_' % (ifo) in param
+                    and "non_reweighted" not in param
+                ]
+            )
             phase = np.array([posterior[param] for param in phase_parameters])
             for num, i in enumerate(amplitude):
                 amp_params[ifo].append(i)
                 phase_params[ifo].append(phase[num])
         return log_freqs, amp_params, phase_params
 
     @staticmethod
@@ -241,20 +275,15 @@
     def _grab_data_from_bilby_file(path, disable_prior=False, **kwargs):
         """
         Load the results file using the `bilby` library
 
         Complex matched filter SNRs are stored in the result file.
         The amplitude and angle are extracted here.
         """
-        from pesummary.core.file.formats.bilby import read_bilby
-
-        return read_bilby(
-            path, disable_prior=disable_prior, latex_dict=GWlatex_labels,
-            complex_params=["matched_filter_snr", "optimal_snr"], **kwargs
-        )
+        return read_bilby(path, disable_prior=disable_prior, **kwargs)
 
     def add_marginalized_parameters_from_config_file(self, config_file):
         """Search the configuration file and add the marginalized parameters
         to the list of parameters and samples
 
         Parameters
         ----------
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/formats/base_read.py` & `pesummary-1.0.0/pesummary/cli/summarymodify.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,597 +1,689 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+#! /usr/bin/env python
 
+# Licensed under an MIT style license -- see LICENSE.md
+
+import os
 import numpy as np
-from scipy.interpolate import interp1d
-from pesummary.gw.file.standard_names import standard_names
-from pesummary.core.file.formats.base_read import Read
-from pesummary.utils.utils import logger
-from pesummary.utils.samples_dict import SamplesDict
-from pesummary.utils.decorators import open_config
-from pesummary.gw.file import conversions as con
-
-try:
-    from glue.ligolw import ligolw
-    from glue.ligolw import lsctables
-    from glue.ligolw import utils as ligolw_utils
-    GLUE = True
-except ImportError:
-    GLUE = False
+import math
+import json
+import h5py
+from pathlib import Path
+
+from pesummary.utils.utils import logger, check_file_exists_and_rename
+from pesummary.utils.dict import paths_to_key
+from pesummary.utils.exceptions import InputError
+from pesummary.core.cli.parser import ArgumentParser as _ArgumentParser
+from pesummary.core.cli.actions import DelimiterSplitAction
+from pesummary.gw.cli.inputs import _GWInput
+from pesummary.gw.file.meta_file import _GWMetaFile
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+__doc__ = """This executable is used to modify a PESummary metafile from the
+command line"""
+
+
+class _Input(_GWInput):
+    """Super class to handle the command line arguments
+    """
+    @property
+    def labels(self):
+        return self._labels
+
+    @labels.setter
+    def labels(self, labels):
+        self._labels = labels
+        if labels is not None and isinstance(labels, dict):
+            self._labels = labels
+        elif labels is not None:
+            raise InputError(
+                "Please provide an existing labels and the label you wish "
+                "to replace it with `--labels existing:new`."
+            )
+
+    @property
+    def config(self):
+        return self._config
+
+    @config.setter
+    def config(self, config):
+        self._config = config
+        if config is not None and isinstance(config, dict):
+            for key, item in config.items():
+                if not os.path.isfile(item):
+                    raise FileNotFoundError(
+                        "Unable to find the config file '{}'".format(item)
+                    )
+                self._config[key] = _GWMetaFile._grab_config_data_from_data_file(
+                    item
+                )
+        elif config is not None:
+            raise InputError(
+                "Please provide the label and config file with '--config "
+                "label:path'"
+            )
 
+    @property
+    def kwargs(self):
+        return self._kwargs
+
+    @kwargs.setter
+    def kwargs(self, kwargs):
+        self._kwargs = kwargs
+        if kwargs is not None and isinstance(kwargs, dict):
+            self._kwargs = kwargs
+        elif kwargs is not None:
+            raise InputError(
+                "Please provide the label, kwarg and value with '--kwargs "
+                "label:kwarg:value`"
+            )
 
-class GWRead(Read):
-    """Base class to read in a results file
+    @property
+    def replace_posterior(self):
+        return self._replace_posterior
+
+    @replace_posterior.setter
+    def replace_posterior(self, replace_posterior):
+        self._replace_posterior = replace_posterior
+        if replace_posterior is not None and isinstance(replace_posterior, dict):
+            self._replace_posterior = replace_posterior
+        elif replace_posterior is not None:
+            raise InputError(
+                "Please provide the label, posterior and file path with "
+                "value with '--replace_posterior "
+                "label;posterior:/path/to/posterior.dat where ';' is the chosen "
+                "delimiter and provided with '--delimiter ;`"
+            )
+
+    @property
+    def remove_posterior(self):
+        return self._remove_posterior
+
+    @remove_posterior.setter
+    def remove_posterior(self, remove_posterior):
+        self._remove_posterior = remove_posterior
+        if remove_posterior is not None and isinstance(remove_posterior, dict):
+            self._remove_posterior = remove_posterior
+        elif remove_posterior is not None:
+            raise InputError(
+                "Please provide the label and posterior with '--remove_posterior "
+                "label:posterior`"
+            )
+
+    @property
+    def store_skymap(self):
+        return self._store_skymap
+
+    @store_skymap.setter
+    def store_skymap(self, store_skymap):
+        self._store_skymap = store_skymap
+        if store_skymap is not None and isinstance(store_skymap, dict):
+            self._store_skymap = store_skymap
+        elif store_skymap is not None:
+            raise InputError(
+                "Please provide the label and path to skymap with '--store_skymap "
+                "label:path/to/skymap.fits`"
+            )
+
+    @property
+    def samples(self):
+        return self._samples
+
+    @samples.setter
+    def samples(self, samples):
+        if samples is None:
+            raise InputError(
+                "Please provide a result file that you wish to modify"
+            )
+        if len(samples) > 1:
+            raise InputError(
+                "Only a single result file can be passed"
+            )
+        samples = samples[0]
+        if not self.is_pesummary_metafile(samples):
+            raise InputError(
+                "Please provide a PESummary metafile to this executable"
+            )
+        self._samples = samples
+
+    @property
+    def data(self):
+        return self._data
+
+    @data.setter
+    def data(self, data):
+        extension = Path(self.samples).suffix
+        if extension == ".h5" or extension == ".hdf5":
+            from pesummary.core.file.formats.pesummary import PESummary
+            from pandas import DataFrame
+
+            with h5py.File(self.samples, "r") as f:
+                data = PESummary._convert_hdf5_to_dict(f)
+                for label in data.keys():
+                    try:
+                        data[label]["posterior_samples"] = DataFrame(
+                            data[label]["posterior_samples"]
+                        ).to_records(index=False, column_dtypes=float)
+                    except KeyError:
+                        pass
+                    except Exception:
+                        parameters = data[label]["posterior_samples"]["parameter_names"]
+                        if isinstance(parameters[0], bytes):
+                            parameters = [
+                                parameter.decode("utf-8") for parameter in parameters
+                            ]
+                        samples = np.array([
+                            j for j in data[label]["posterior_samples"]["samples"]
+                        ].copy())
+                        data[label]["posterior_samples"] = DataFrame.from_dict(
+                            {
+                                param: samples.T[num] for num, param in
+                                enumerate(parameters)
+                            }
+                        ).to_records(index=False, column_dtypes=float)
+                self._data = data
+        elif extension == ".json":
+            with open(self.samples, "r") as f:
+                self._data = json.load(f)
+        else:
+            raise InputError(
+                "The extension '{}' is not recognised".format(extension)
+            )
+
+
+class Input(_Input):
+    """Class to handle the command line arguments
 
     Parameters
     ----------
-    path_to_results_file: str
-        path to the results file you wish to load
+    opts: argparse.Namespace
+        Namespace object containing the command line options
 
     Attributes
     ----------
-    parameters: list
-        list of parameters stored in the result file
-    samples: 2d list
-        list of samples stored in the result file
-    samples_dict: dict
-        dictionary of samples stored in the result file keyed by parameters
-    input_version: str
-        version of the result file passed.
-    extra_kwargs: dict
-        dictionary of kwargs that were extracted from the result file
-
-    Methods
-    -------
-    to_dat:
-        save the posterior samples to a .dat file
-    to_latex_table:
-        convert the posterior samples to a latex table
-    generate_latex_macros:
-        generate a set of latex macros for the stored posterior samples
-    to_lalinference:
-        convert the posterior samples to a lalinference result file
-    generate_all_posterior_samples:
-        generate all posterior distributions that may be derived from
-        sampled distributions
-    """
-    def __init__(self, path_to_results_file, **kwargs):
-        super(GWRead, self).__init__(path_to_results_file, **kwargs)
-
-    def load(self, function, **kwargs):
-        """Load a results file according to a given function
-
-        Parameters
-        ----------
-        function: func
-            callable function that will load in your results file
-        """
-        data = self.load_from_function(
-            function, self.path_to_results_file, **kwargs)
-        if "mcmc_samples" in data.keys():
-            self.mcmc_samples = data["mcmc_samples"]
-        parameters, samples = self.translate_parameters(
-            data["parameters"], data["samples"]
+    samples: str
+        path to a PESummary meta file that you wish to modify
+    labels: dict
+        dictionary of labels that you wish to modify. Key is the existing label
+        and item is the new label
+    """
+    def __init__(self, opts, ignore_copy=False):
+        logger.info("Command line arguments: %s" % (opts))
+        self.opts = opts
+        self.existing = None
+        self.webdir = self.opts.webdir
+        self.samples = self.opts.samples
+        self.labels = self.opts.labels
+        self.kwargs = self.opts.kwargs
+        self.config = self.opts.config
+        self.replace_posterior = self.opts.replace_posterior
+        self.remove_label = self.opts.remove_label
+        self.remove_posterior = self.opts.remove_posterior
+        self.store_skymap = self.opts.store_skymap
+        self.hdf5 = not self.opts.save_to_json
+        self.overwrite = self.opts.overwrite
+        self.force_replace = self.opts.force_replace
+        self.data = None
+        self.stored_labels = [
+            key for key in self.data.keys() if key not in
+            ["history", "strain", "version"]
+        ]
+        if self.opts.descriptions is not None:
+            import copy
+            self._labels_copy = copy.deepcopy(self._labels)
+            self._labels = self.stored_labels
+            self.descriptions = self.opts.descriptions
+            self._labels = self._labels_copy
+        else:
+            self._descriptions = None
+
+
+class ArgumentParser(_ArgumentParser):
+    def _pesummary_options(self):
+        options = super(ArgumentParser, self)._pesummary_options()
+        options.update(
+            {
+                "--labels": {
+                    "nargs": "+",
+                    "action": DelimiterSplitAction,
+                    "help": (
+                        "labels you wish to modify. Syntax: `--labels "
+                        "existing:new` where ':' is the default delimiter"
+                    )
+                },
+                "--config": {
+                    "nargs": "+",
+                    "action": DelimiterSplitAction,
+                    "help": (
+                        "config data you wish to modify. Syntax `--config "
+                        "label:path` where label is the analysis you wish to "
+                        "change and path is the path to a new configuration "
+                        "file."
+                    )
+                },
+                "--delimiter": {
+                    "default": ":",
+                    "help": (
+                        "Delimiter used to seperate the existing and new "
+                        "quantity"
+                    )
+                },
+                "--kwargs": {
+                    "nargs": "+",
+                    "action": DelimiterSplitAction,
+                    "help": (
+                        "kwargs you wish to modify. Syntax: `--kwargs "
+                        "label/kwarg:item` where '/' is a delimiter of your "
+                        "choosing (it cannot be ':'), kwarg is the kwarg name "
+                        "and item is the value of the kwarg"
+                    )
+                },
+                "--overwrite": {
+                    "action": "store_true",
+                    "default": False,
+                    "help": (
+                        "Overwrite the supplied PESummary meta file with the "
+                        "modified version"
+                    )
+                },
+                "--replace_posterior": {
+                    "nargs": "+",
+                    "action": DelimiterSplitAction,
+                    "help": (
+                        "Replace the posterior for a given label. Syntax: "
+                        "--replace_posterior label;a:/path/to/posterior.dat "
+                        "where ';' is a delimiter of your choosing (it cannot "
+                        "be '/' or ':'), a is the posterior you wish to "
+                        "replace and item is a path to a one column ascii file "
+                        "containing the posterior samples "
+                        "(/path/to/posterior.dat)"
+                    )
+                },
+                "--remove_posterior": {
+                    "nargs": "+",
+                    "action": DelimiterSplitAction,
+                    "help": (
+                        "Remove a posterior distribution for a given label. "
+                        "Syntax: --remove_posterior label:a where a is the "
+                        "posterior you wish to remove"
+                    )
+                },
+                "--remove_label": {
+                    "nargs": "+",
+                    "help": "Remove an entire analysis from the input file"
+                },
+                "--store_skymap": {
+                    "nargs": "+",
+                    "action": DelimiterSplitAction,
+                    "help": (
+                        "Store the contents of a fits file in the metafile. "
+                        "Syntax: --store_skymap label:path/to/skymap.fits"
+                    )
+                },
+                "--force_replace": {
+                    "action": "store_true",
+                    "default": False,
+                    "help": (
+                        "Override the ValueError raised if the data is already "
+                        "stored in the result file"
+                    )
+                }
+            }
         )
-        _add_likelihood = kwargs.get("add_zero_likelihood", True)
-        if "log_likelihood" not in parameters and _add_likelihood:
+        options["--descriptions"]["action"] = DelimiterSplitAction
+        return options
+
+
+def _check_label(data, label, message, logger_level="warn"):
+    """Check that a given label is stored in the data. If it is not stored
+    print a warning message
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    label: str
+        name of the label you wish to check
+    message: str
+        message you wish to print in logger when the label is not stored
+    logger_level: str, optional
+        the logger level of the message
+    """
+    if label not in data.keys():
+        getattr(logger, logger_level)(message)
+        return False
+    return True
+
+
+def _modify_labels(data, labels=None):
+    """Modify the existing labels in the data
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    labels: dict
+        dictionary of labels showing the existing label, key, and the new
+        label, item
+    """
+    for existing, new in labels.items():
+        if existing not in data.keys():
             logger.warning(
-                "Failed to find 'log_likelihood' in result file. Setting "
-                "every sample to have log_likelihood 0"
+                "Unable to find label '{}' in the root of the metafile. "
+                "Checking inside the groups".format(existing)
             )
-            parameters.append("log_likelihood")
-            for num, i in enumerate(samples):
-                samples[num].append(0)
-        self.data = {
-            "parameters": parameters, "samples": samples
-        }
-        self.parameters = self.data["parameters"]
-        self.samples = self.data["samples"]
-        self.data["injection"] = data["injection"]
-        if "version" in data.keys() and data["version"] is not None:
-            self.input_version = data["version"]
+            for key in data.keys():
+                if existing in data[key].keys():
+                    data[key][new] = data[key].pop(existing)
         else:
-            self.input_version = "No version information found"
-        if "kwargs" in data.keys():
-            self.extra_kwargs = data["kwargs"]
-        else:
-            self.extra_kwargs = {"sampler": {}, "meta_data": {}}
-            self.extra_kwargs["sampler"]["nsamples"] = len(self.data["samples"])
-        if data["injection"] is not None:
-            self.injection_parameters = self.convert_injection_parameters(
-                data["injection"]
-            )
-        else:
-            self.injection_parameters = data["injection"]
-        if isinstance(self.injection_parameters, dict):
-            self.injection_parameters = {
-                key.decode("utf-8") if isinstance(key, bytes) else key: val
-                for key, val in self.injection_parameters.items()
-            }
-        elif isinstance(self.injection_parameters, list):
-            self.injection_parameters = [
-                {
-                    key.decode("utf-8") if isinstance(key, bytes) else
-                    key: val for key, val in i.items()
-                } for i in self.injection_parameters
-            ]
-        if "prior" in data.keys():
-            self.priors = data["prior"]
-            if data["prior"]["samples"] != {}:
-                priors = data["prior"]["samples"]
-                default_parameters = list(priors.keys())
-                default_samples = [
-                    [priors[parameter][i] for parameter in default_parameters]
-                    for i in range(len(priors[default_parameters[0]]))
-                ]
-                parameters, samples = self.translate_parameters(
-                    default_parameters, default_samples
-                )
-                if not kwargs.get("disable_prior_conversion", False):
-                    self.priors["samples"] = con._Conversion(
-                        parameters, samples, extra_kwargs=self.extra_kwargs
-                    )
-                else:
-                    self.priors["samples"] = SamplesDict(parameters, samples)
-        if "analytic" in data.keys():
-            self.analytic = data["analytic"]
-        if "weights" in self.data.keys():
-            self.weights = self.data["weights"]
-        else:
-            self.weights = self.check_for_weights(
-                self.data["parameters"], self.data["samples"]
-            )
+            data[new] = data.pop(existing)
+    return data
 
-    def convert_injection_parameters(self, data):
-        """
-        """
-        import math
-
-        if all(math.isnan(data[i]) for i in data.keys()):
-            return data
-        parameters = list(data.keys())
-        samples = [[data[i] for i in parameters]]
-        if "waveform_approximant" in parameters:
-            ind = parameters.index("waveform_approximant")
-            parameters.remove(parameters[ind])
-            samples[0].remove(samples[0][ind])
-        nan_inds = []
-        for num, i in enumerate(parameters):
-            if math.isnan(samples[0][num]):
-                nan_inds.append(num)
-        for i in nan_inds[::-1]:
-            parameters.remove(parameters[i])
-            samples[0].remove(samples[0][i])
-        inj_samples = con._Conversion(
-            parameters, samples, extra_kwargs=self.extra_kwargs
-        )
-        for i in self.parameters:
-            if i not in list(inj_samples.keys()):
-                inj_samples[i] = float("nan")
-        return inj_samples
-
-    def _grab_injection_parameters_from_file(self, injection_file, **kwargs):
-        from pesummary.gw.file.injection import GWInjection
-
-        inj_samples = GWInjection.read(injection_file, **kwargs).samples_dict
-        for i in self.parameters:
-            if i not in list(inj_samples.keys()):
-                inj_samples[i] = float("nan")
-        return inj_samples
-
-    def _unzip_injection_file(self, injection_file):
-        """Unzip the injection file and extract injection parameters from
-        the file.
-        """
-        from pesummary.utils.utils import unzip
-
-        out_file = unzip(injection_file)
-        return self._grab_injection_parameters_from_file(out_file)
-
-    def _grab_injection_data_from_xml_file(self, injection_file):
-        """Grab the data from an xml injection file
-        """
-        if GLUE:
-            xmldoc = ligolw_utils.load_filename(
-                injection_file, contenthandler=lsctables.use_in(
-                    ligolw.LIGOLWContentHandler))
-            try:
-                table = lsctables.SimInspiralTable.get_table(xmldoc)[0]
-            except Exception:
-                table = lsctables.SnglInspiralTable.get_table(xmldoc)[0]
-            injection_values = self._return_all_injection_parameters(
-                self.parameters, table)
-        else:
-            injection_values = [float("nan")] * len(self.parameters)
-        return {i: j for i, j in zip(self.parameters, injection_values)}
 
-    def _return_all_injection_parameters(self, parameters, table):
-        """Return tlhe full list of injection parameters
+def _modify_descriptions(data, descriptions={}):
+    """Modify the existing descriptions in the data
 
-        Parameters
-        ----------
-        parameters: list
-            full list of parameters being used in the analysis
-        table: glue.ligolw.lsctables.SnglInspiral
-            table containing the trigger values
-        """
-        func_map = {
-            "chirp_mass": lambda inj: inj.mchirp,
-            "luminosity_distance": lambda inj: inj.distance,
-            "mass_1": lambda inj: inj.mass1,
-            "mass_2": lambda inj: inj.mass2,
-            "dec": lambda inj: inj.latitude,
-            "spin_1x": lambda inj: inj.spin1x,
-            "spin_1y": lambda inj: inj.spin1y,
-            "spin_1z": lambda inj: inj.spin1z,
-            "spin_2x": lambda inj: inj.spin2x,
-            "spin_2y": lambda inj: inj.spin2y,
-            "spin_2z": lambda inj: inj.spin2z,
-            "iota": lambda inj: inj.inclination,
-            "psi": lambda inj: inj.polarization,
-            "mass_ratio": lambda inj: con.q_from_m1_m2(
-                inj.mass1, inj.mass2),
-            "symmetric_mass_ratio": lambda inj: con.eta_from_m1_m2(
-                inj.mass1, inj.mass2),
-            "inversed_mass_ratio": lambda inj: con.invq_from_m1_m2(
-                inj.mass1, inj.mass2),
-            "total_mass": lambda inj: inj.mass1 + inj.mass2,
-            "chi_p": lambda inj: con._chi_p(
-                inj.mass1, inj.mass2, inj.spin1x, inj.spin1y, inj.spin2x,
-                inj.spin2y),
-            "chi_eff": lambda inj: con._chi_eff(
-                inj.mass1, inj.mass2, inj.spin1z, inj.spin2z)}
-
-        injection_values = []
-        for i in parameters:
-            try:
-                if func_map[i](table) is not None:
-                    injection_values.append(func_map[i](table))
-                else:
-                    injection_values.append(float("nan"))
-            except Exception:
-                injection_values.append(float("nan"))
-        return injection_values
-
-    def _grab_injection_data_from_hdf5_file(self):
-        """Grab the data from an hdf5 injection file
-        """
-        pass
-
-    @staticmethod
-    def check_for_calibration_data(function, path_to_results_file):
-        """Check to see if there is any calibration data in the results file
-
-        Parameters
-        ----------
-        function: func
-            callable function that will check to see if calibration data is in
-            the results file
-        path_to_results_file: str
-            path to the results file
-        """
-        return function(path_to_results_file)
-
-    @staticmethod
-    def grab_calibration_data(function, path_to_results_file):
-        """Grab the calibration data from the results file
-
-        Parameters
-        ----------
-        function: func
-            callable function that will grab the calibration data from the
-            results file
-        path_to_results_file: str
-            path to the results file
-        """
-        log_frequencies, amp_params, phase_params = function(path_to_results_file)
-        total = []
-        for key in log_frequencies.keys():
-            f = np.exp(log_frequencies[key])
-            fs = np.linspace(np.min(f), np.max(f), 100)
-            data = [interp1d(log_frequencies[key], samp, kind="cubic",
-                             fill_value=0, bounds_error=False)(np.log(fs)) for samp
-                    in np.column_stack(amp_params[key])]
-            amplitude_upper = 1. - np.mean(data, axis=0) + np.std(data, axis=0)
-            amplitude_lower = 1. - np.mean(data, axis=0) - np.std(data, axis=0)
-            amplitude_median = 1 - np.median(data, axis=0)
-
-            data = [interp1d(log_frequencies[key], samp, kind="cubic",
-                             fill_value=0, bounds_error=False)(np.log(fs)) for samp
-                    in np.column_stack(phase_params[key])]
-
-            phase_upper = np.mean(data, axis=0) + np.std(data, axis=0)
-            phase_lower = np.mean(data, axis=0) - np.std(data, axis=0)
-            phase_median = np.median(data, axis=0)
-            total.append(np.column_stack(
-                [fs, amplitude_median, phase_median, amplitude_lower,
-                 phase_lower, amplitude_upper, phase_upper]))
-        return total, log_frequencies.keys()
-
-    @staticmethod
-    def load_strain_data(strain_data):
-        """Load the strain data
-
-        Parameters
-        ----------
-        strain_data: dict
-            strain data with key equal to the channel and value the path to
-            the strain data
-        """
-        func_map = {"lcf": GWRead._timeseries_from_cache_file,
-                    "pickle": GWRead._timeseries_from_pickle_file}
-        timeseries = {}
-        if isinstance(strain_data, str):
-            ext = GWRead.extension_from_path(strain_data)
-            function = func_map[ext]
-            try:
-                timeseries = GWRead.load_from_function(function, strain_data)
-            except Exception as e:
-                logger.info("Failed to load in {} because {}".format(strain_data, e))
-                timeseries = None
-            return timeseries
-
-        for key in list(strain_data.keys()):
-            ext = GWRead.extension_from_path(strain_data[key])
-            function = func_map[ext]
-            reduced_dict = {key: strain_data[key]}
-            if "H1" in key:
-                ifo = "H1"
-            elif "L1" in key:
-                ifo = "L1"
-            elif "V1" in key:
-                ifo = "V1"
-            else:
-                ifo = key
-            try:
-                timeseries[ifo] = GWRead.load_from_function(function, reduced_dict)
-            except Exception as e:
-                logger.info("Failed to load {} because {}".format(strain_data[key], e))
-            if timeseries == {}:
-                timeseries = None
-        return timeseries
-
-    @staticmethod
-    def _timeseries_from_cache_file(strain_dictionary):
-        """Return a time series from a cache file
-
-        Parameters
-        ----------
-        strain_dictionary: dict
-            dictionary containing one key (channel) and one value (path to
-            cache file)
-        """
-        from gwpy.timeseries import TimeSeries
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    descriptions: dict
+        dictionary of descriptions with label as the key and new description as
+        the item
+    """
+    message = (
+        "Unable to find label '{}' in the metafile. Unable to modify "
+        "description"
+    )
+    for label, new_desc in descriptions.items():
+        check = _check_label(data, label, message.format(label))
+        if check:
+            if "description" not in data[label].keys():
+                data[label]["description"] = []
+            data[label]["description"] = [new_desc]
+    return data
 
+
+def _modify_kwargs(data, kwargs=None):
+    """Modify kwargs that are stored in the data
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    kwargs: dict
+        dictionary of kwargs showing the label as key and kwarg:value as the
+        item
+    """
+    def add_to_meta_data(data, label, string):
+        kwarg, value = string.split(":")
         try:
-            from glue.lal import Cache
-            GLUE = True
-        except ImportError:
-            GLUE = False
-
-        if not GLUE:
-            raise Exception("lscsoft-glue is required to read from a cached "
-                            "file. Please install this package")
-        channel = list(strain_dictionary.keys())[0]
-        cached_file = strain_dictionary[channel]
-        with open(cached_file, "r") as f:
-            data = Cache.fromfile(f)
-        try:
-            strain_data = TimeSeries.read(data, channel)
-        except Exception as e:
-            raise Exception("Failed to read in the cached file because %s" % (
-                            e))
-        return strain_data
-
-    @staticmethod
-    def _timeseries_from_pickle_file(pickle_file):
-        """Return a time series from a pickle file
-        """
-        try:
-            from pesummary.gw.file.formats.bilby import Bilby
+            _group, = paths_to_key(kwarg, data[label]["meta_data"])
+            group = _group[0]
+        except ValueError:
+            group = "other"
+        if group == "other" and group not in data[label]["meta_data"].keys():
+            data[label]["meta_data"]["other"] = {}
+        data[label]["meta_data"][group][kwarg] = value
+        return data
+
+    message = "Unable to find label '{}' in the metafile. Unable to modify kwargs"
+    for label, item in kwargs.items():
+        check = _check_label(data, label, message.format(label))
+        if check:
+            if isinstance(item, list):
+                for _item in item:
+                    data = add_to_meta_data(data, label, _item)
+            else:
+                data = add_to_meta_data(data, label, item)
+    return data
+
+
+def _modify_config(data, kwargs=None):
+    """Replace the config data that is stored in the data
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    kwargs: dict
+        dictionary of kwargs showing the label as key and config data as the
+        item
+    """
+    message = (
+        "Unable to find label '{}' in the metafile. Unable to modify config"
+    )
+    for label, config in kwargs.items():
+        check = _check_label(data, label, message.format(label))
+        if check:
+            data[label]["config_file"] = config
+    return data
+
+
+def _modify_posterior(data, kwargs=None):
+    """Replace a posterior distribution that is stored in the data
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    kwargs: dict
+        dictionary of kwargs showing the label as key and posterior:path as the
+        item
+    """
+    def _replace_posterior(data, string):
+        posterior, path = string.split(":")
+        _data = np.genfromtxt(path, usecols=0)
+        if math.isnan(_data[0]):
+            _data = np.genfromtxt(path, names=True, usecols=0)
+            _data = _data[_data.dtype.names[0]]
+        if posterior in data[label]["posterior_samples"].dtype.names:
+            data[label]["posterior_samples"][posterior] = _data
+        else:
+            from numpy.lib.recfunctions import append_fields
 
-            data = Bilby._timeseries_from_bilby_pickle(pickle_file)
-        except Exception as e:
-            raise Exception("Failed to read pickle file because %s" % (e))
+            data[label]["posterior_samples"] = append_fields(
+                data[label]["posterior_samples"], posterior, _data, usemask=False
+            )
         return data
 
-    @staticmethod
-    def translate_parameters(parameters, samples):
-        """Translate parameters to a standard names
-
-        Parameters
-        ----------
-        parameters: list
-            list of parameters used in the analysis
-        samples: list
-            list of samples for each parameters
-        """
-        path = ("https://git.ligo.org/lscsoft/pesummary/blob/master/pesummary/"
-                "gw/file/standard_names.py")
-        parameters_not_included = [
-            i for i in parameters if i not in standard_names.keys()
+    message = "Unable to find label '{}' in the metafile. Unable to modify posterior"
+    for label, item in kwargs.items():
+        check = _check_label(data, label, message.format(label))
+        if check:
+            if isinstance(item, list):
+                for _item in item:
+                    data = _replace_posterior(data, _item)
+            else:
+                data = _replace_posterior(data, item)
+    return data
+
+
+def _remove_label(data, kwargs=None):
+    """Remove an analysis that is stored in the data
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    kwargs: list
+        list of analysis you wish to remove
+    """
+    message = (
+        "Unable to find label '{}' in the metafile. Unable to remove"
+    )
+    for label in kwargs:
+        check = _check_label(data, label, message.format(label))
+        if check:
+            _ = data.pop(label)
+    return data
+
+
+def _remove_posterior(data, kwargs=None):
+    """Remove a posterior distribution that is stored in the data
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    kwargs: dict
+        dictionary of kwargs showing the label as key and posterior as the item
+    """
+    def _rmfield(array, *fieldnames_to_remove):
+        return array[
+            [name for name in array.dtype.names if name not in fieldnames_to_remove]
         ]
-        if len(parameters_not_included) > 0:
-            logger.debug(
-                "PESummary does not have a 'standard name' for the following "
-                "parameters: {}. This means that comparison plots between "
-                "different codes may not show these parameters. If you want to "
-                "assign a standard name for these parameters, please add an MR "
-                "which edits the following file: {}. These parameters will be "
-                "added to the result pages and meta file as is.".format(
-                    ", ".join(parameters_not_included), path
+
+    message = "Unable to find label '{}' in the metafile. Unable to remove posterior"
+    for label, item in kwargs.items():
+        check = _check_label(data, label, message.format(label))
+        if check:
+            group = "posterior_samples"
+            if isinstance(item, list):
+                for _item in item:
+                    data[label][group] = _rmfield(data[label][group], _item)
+            else:
+                data[label][group] = _rmfield(data[label][group], item)
+    return data
+
+
+def _store_skymap(data, kwargs=None, replace=False):
+    """Store a skymap in the metafile
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    kwargs: dict
+        dictionary of kwargs showing the label as key and posterior as the item
+    replace: dict
+        replace a skymap already stored in the result file
+    """
+    from pesummary.io import read
+
+    message = "Unable to find label '{}' in the metafile. Unable to store skymap"
+    for label, path in kwargs.items():
+        check = _check_label(data, label, message.format(label))
+        if check:
+            skymap = read(path, skymap=True)
+            if "skymap" not in data[label].keys():
+                data[label]["skymap"] = {}
+            if "meta_data" not in data[label]["skymap"].keys():
+                data[label]["skymap"]["meta_data"] = {}
+            if "data" in data[label]["skymap"].keys() and not replace:
+                raise ValueError(
+                    "Skymap already found in result file for {}. If you wish to replace "
+                    "the skymap, add the command line argument '--force_replace".format(
+                        label
+                    )
                 )
+            elif "data" in data[label]["skymap"].keys():
+                logger.warning("Replacing skymap data for {}".format(label))
+            data[label]["skymap"]["data"] = skymap
+            for key in skymap.meta_data:
+                data[label]["skymap"]["meta_data"][key] = skymap.meta_data[key]
+    return data
+
+
+def modify(data, function, **kwargs):
+    """Modify the data according to a given function
+
+    Parameters
+    ----------
+    data: dict
+        dictionary containing the data
+    function:
+        function you wish to use to modify the data
+    kwargs: dict
+        dictionary of kwargs for function
+    """
+    func_map = {
+        "labels": _modify_labels,
+        "descriptions": _modify_descriptions,
+        "kwargs": _modify_kwargs,
+        "add_posterior": _modify_posterior,
+        "rm_label": _remove_label,
+        "rm_posterior": _remove_posterior,
+        "skymap": _store_skymap,
+        "config": _modify_config
+    }
+    return func_map[function](data, **kwargs)
+
+
+def _main(opts):
+    """
+    """
+    args = Input(opts)
+    if not args.overwrite:
+        meta_file = os.path.join(
+            args.webdir, "modified_posterior_samples.{}".format(
+                "h5" if args.hdf5 else "json"
             )
-        standard_params = [i for i in parameters if i in standard_names.keys()]
-        converted_params = [
-            standard_names[i] if i in standard_params else i for i in
-            parameters
+        )
+        check_file_exists_and_rename(meta_file)
+    else:
+        meta_file = args.samples
+    if opts.preferred is not None and args.kwargs is None:
+        args.kwargs = {opts.preferred: "preferred:True"}
+    if opts.preferred is not None:
+        args.kwargs.update(
+            {
+                _label: "preferred:False" for _label in args.stored_labels
+                if _label != opts.preferred
+            }
+        )
+    if args.labels is not None:
+        modified_data = modify(args.data, "labels", labels=args.labels)
+    if args.descriptions is not None:
+        modified_data = modify(
+            args.data, "descriptions", descriptions=args.descriptions
+        )
+    if args.config is not None:
+        modified_data = modify(args.data, "config", kwargs=args.config)
+    if args.kwargs is not None:
+        modified_data = modify(args.data, "kwargs", kwargs=args.kwargs)
+    if args.replace_posterior is not None:
+        modified_data = modify(args.data, "add_posterior", kwargs=args.replace_posterior)
+    if args.remove_label is not None:
+        modified_data = modify(args.data, "rm_label", kwargs=args.remove_label)
+    if args.remove_posterior is not None:
+        modified_data = modify(args.data, "rm_posterior", kwargs=args.remove_posterior)
+    if args.store_skymap is not None:
+        modified_data = modify(
+            args.data, "skymap", kwargs=args.store_skymap, replace=args.force_replace
+        )
+    logger.info(
+        "Saving the modified data to '{}'".format(meta_file)
+    )
+    if args.hdf5:
+        _GWMetaFile.save_to_hdf5(
+            modified_data, list(modified_data.keys()), None, meta_file,
+            no_convert=True
+        )
+    else:
+        _GWMetaFile.save_to_json(modified_data, meta_file)
+
+
+def command_line():
+    parser = ArgumentParser(description=__doc__)
+    parser.add_known_options_to_parser(
+        [
+            "--descriptions", "--labels", "--config", "--delimiter",
+            "--samples", "--webdir", "--save_to_json", "--preferred",
+            "--kwargs", "--overwrite", "--replace_posterior",
+            "--remove_posterior", "--remove_label", "--store_skymap",
+            "--force_replace"
         ]
-        return converted_params, samples
+    )
+    return parser
 
-    @staticmethod
-    def _check_definition_of_inclination(parameters):
-        """Check the definition of inclination given the other parameters
-
-        Parameters
-        ----------
-        parameters: list
-            list of parameters used in the study
-        """
-        theta_jn = False
-        spin_angles = ["tilt_1", "tilt_2", "a_1", "a_2"]
-        names = [
-            standard_names[i] for i in parameters if i in standard_names.keys()]
-        if all(i in names for i in spin_angles):
-            theta_jn = True
-        if theta_jn:
-            if "theta_jn" not in names and "inclination" in parameters:
-                logger.warning("Because the spin angles are in your list of "
-                               "parameters, the angle 'inclination' probably "
-                               "refers to 'theta_jn'. If this is a mistake, "
-                               "please change the definition of 'inclination' to "
-                               "'iota' in your results file")
-                index = parameters.index("inclination")
-                parameters[index] = "theta_jn"
-        else:
-            if "inclination" in parameters:
-                index = parameters.index("inclination")
-                parameters[index] = "iota"
-        return parameters
-
-    def add_fixed_parameters_from_config_file(self, config_file):
-        """Search the conifiguration file and add fixed parameters to the
-        list of parameters and samples
-
-        Parameters
-        ----------
-        config_file: str
-            path to the configuration file
-        """
-        self._add_fixed_parameters_from_config_file(
-            config_file, self._add_fixed_parameters)
-
-    @staticmethod
-    @open_config(index=2)
-    def _add_fixed_parameters(parameters, samples, config_file):
-        """Open a LALInference configuration file and add the fixed parameters
-        to the list of parameters and samples
-
-        Parameters
-        ----------
-        parameters: list
-            list of existing parameters
-        samples: list
-            list of existing samples
-        config_file: str
-            path to the configuration file
-        """
-        from pesummary.gw.file.standard_names import standard_names
-
-        config = config_file
-        if not config.error:
-            fixed_data = {}
-            if "engine" in config.sections():
-                fixed_data = {
-                    key.split("fix-")[1]: item for key, item in
-                    config.items("engine") if "fix" in key}
-            for i in fixed_data.keys():
-                fixed_parameter = i
-                fixed_value = fixed_data[i]
-                try:
-                    param = standard_names[fixed_parameter]
-                    if param in parameters:
-                        pass
-                    else:
-                        parameters.append(param)
-                        for num in range(len(samples)):
-                            samples[num].append(float(fixed_value))
-                except Exception:
-                    if fixed_parameter == "logdistance":
-                        if "luminosity_distance" not in parameters:
-                            parameters.append(standard_names["distance"])
-                            for num in range(len(samples)):
-                                samples[num].append(float(fixed_value))
-                    if fixed_parameter == "costheta_jn":
-                        if "theta_jn" not in parameters:
-                            parameters.append(standard_names["theta_jn"])
-                            for num in range(len(samples)):
-                                samples[num].append(float(fixed_value))
-            return parameters, samples
-        return parameters, samples
-
-    def _specific_parameter_samples(self, param):
-        """Return the samples for a specific parameter
-
-        Parameters
-        ----------
-        param: str
-            the parameter that you would like to return the samples for
-        """
-        ind = self.parameters.index(param)
-        samples = np.array([i[ind] for i in self.samples])
-        return samples
-
-    def specific_parameter_samples(self, param):
-        """Return the samples for either a list or a single parameter
-
-        Parameters
-        ----------
-        param: list/str
-            the parameter/parameters that you would like to return the samples
-            for
-        """
-        if type(param) == list:
-            samples = [self._specific_parameter_samples(i) for i in param]
-        else:
-            samples = self._specific_parameter_samples(param)
-        return samples
 
-    def append_data(self, samples):
-        """Add a list of samples to the existing samples data object
+def main(args=None):
+    """
+    """
+    parser = command_line()
+    opts = parser.parse_args(args=args)
+    return _main(opts)
 
-        Parameters
-        ----------
-        samples: list
-            the list of samples that you would like to append
-        """
-        for num, i in enumerate(self.samples):
-            self.samples[num].append(samples[num])
-
-    def generate_all_posterior_samples(self, **kwargs):
-        if "no_conversion" in kwargs.keys():
-            no_conversion = kwargs.pop("no_conversion")
-        else:
-            no_conversion = False
-        if not no_conversion:
-            from pesummary.gw.file.conversions import _Conversion
-
-            data = _Conversion(
-                self.parameters, self.samples, extra_kwargs=self.extra_kwargs,
-                return_dict=False, **kwargs
-            )
-            self.parameters = data[0]
-            self.samples = data[1]
-            if kwargs.get("return_kwargs", False):
-                self.extra_kwargs = data[2]
-
-    def to_lalinference(self, **kwargs):
-        """Save the PESummary results file object to a lalinference hdf5 file
-
-        Parameters
-        ----------
-        kwargs: dict
-            all kwargs are passed to the pesummary.io.write.write function
-        """
-        return self.write(file_format="lalinference", package="gw", **kwargs)
+
+if __name__ == "__main__":
+    main()
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/formats/pesummary.py` & `pesummary-1.0.0/pesummary/core/file/formats/default.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,381 +1,300 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-
-from pesummary.gw.file.formats.base_read import GWRead
-from pesummary.core.file.formats.pesummary import (
-    PESummary as CorePESummary, PESummaryDeprecated as CorePESummaryDeprecated,
-    deprecation_warning
-)
-from pesummary.utils.utils import logger
+# Licensed under an MIT style license -- see LICENSE.md
+
 import numpy as np
-import warnings
+import os
+from pesummary.core.file.formats.base_read import (
+    Read, SingleAnalysisRead, MultiAnalysisRead
+)
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
-def write_pesummary(*args, **kwargs):
-    """Write a set of samples to a pesummary file
+class SingleAnalysisDefault(SingleAnalysisRead):
+    """Class to handle result files which only contain a single analysis
 
     Parameters
     ----------
-    args: tuple
-        either a 2d tuple containing the parameters as first argument and samples
-        as the second argument, or a SamplesDict object containing the samples
-    outdir: str, optional
-        directory to write the dat file
-    label: str, optional
-        The label of the analysis. This is used in the filename if a filename
-        if not specified
-    config: dict, optional
-        configuration file that you wish to save to file
-    injection_data: dict, optional
-        dictionary containing the injection values that you wish to save to file keyed
-        by parameter
-    file_kwargs: dict, optional
-        any kwargs that you wish to save to file
-    mcmc_samples: Bool, optional
-        if True, the set of samples provided are from multiple MCMC chains
-    hdf5: Bool, optional
-        if True, save the pesummary file in hdf5 format
-    kwargs: dict
-        all other kwargs are passed to the pesummary.gw.file.meta_file._GWMetaFile class
-    """
-    from pesummary.core.file.formats.pesummary import write_pesummary as core_write
-    from pesummary.gw.file.meta_file import _GWMetaFile
+    path_to_results_file: str
+        path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
 
-    return core_write(*args, cls=_GWMetaFile, **kwargs)
+    Attributes
+    ----------
+    parameters: list
+        list of parameters stored in the result file
+    samples: 2d list
+        list of samples stored in the result file
+    samples_dict: dict
+        dictionary of samples stored in the result file keyed by parameters
+    input_version: str
+        version of the result file passed.
+    extra_kwargs: dict
+        dictionary of kwargs that were extracted from the result file
+    injection_parameters: dict
+        dictionary of injection parameters extracted from the result file
+    pe_algorithm: str
+        name of the algorithm used to generate the posterior samples
 
+    Methods
+    -------
+    to_dat:
+        save the posterior samples to a .dat file
+    to_latex_table:
+        convert the posterior samples to a latex table
+    generate_latex_macros:
+        generate a set of latex macros for the stored posterior samples
+    """
+    def __init__(self, *args, _data=None, **kwargs):
+        super(SingleAnalysisDefault, self).__init__(*args, **kwargs)
+        if _data is not None:
+            self.load(None, _data=_data, **kwargs)
 
-class PESummary(GWRead, CorePESummary):
-    """This class handles the existing posterior_samples.h5 file
+
+class MultiAnalysisDefault(MultiAnalysisRead):
+    """Class to handle result files which contain multiple analyses
 
     Parameters
     ----------
     path_to_results_file: str
         path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
+
+    Attributes
+    ----------
+    parameters: 2d list
+        list of parameters stored in the result file for each analyses
+    samples: 2d list
+        list of samples stored in the result file for each analyses
+    samples_dict: dict
+        dictionary of samples stored in the result file keyed by analysis label
+    input_version: str
+        version of the result file passed.
+    extra_kwargs: dict
+        dictionary of kwargs that were extracted from the result file
+    injection_parameters: dict
+        dictionary of injection parameters extracted from the result file
+
+    Methods
+    -------
+    samples_dict_for_label: dict
+        dictionary of samples for a specific analysis
+    reduced_samples_dict: dict
+        dictionary of samples for one or more analyses
+    to_dat:
+        save the posterior samples to a .dat file
+    to_latex_table:
+        convert the posterior samples to a latex table
+    generate_latex_macros:
+        generate a set of latex macros for the stored posterior samples
+    """
+    def __init__(self, *args, _data=None, **kwargs):
+        super(MultiAnalysisDefault, self).__init__(*args, **kwargs)
+        if _data is not None:
+            self.load(None, _data=_data, **kwargs)
+
+
+class Default(object):
+    """Class to handle the default loading options.
+
+    Attributes
+    ----------
+    path_to_results_file: str
+        path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
 
     Attributes
     ----------
     parameters: list
         list of parameters stored in the result file
     samples: 2d list
         list of samples stored in the result file
     samples_dict: dict
         dictionary of samples stored in the result file keyed by parameters
     input_version: str
         version of the result file passed.
     extra_kwargs: dict
         dictionary of kwargs that were extracted from the result file
-    approximant: list
-        list of approximants stored in the result file
-    labels: list
-        list of analyses stored in the result file
-    config: list
-        list of dictonaries containing the configuration files for each
-        analysis
-    psd: dict
-        dictionary containing the psds stored in the result file keyed by
-        the analysis label
-    calibration: dict
-        dictionary containing the calibration posterior samples keyed by
-        the analysis label
-    skymap: dict
-        dictionary containing the skymap probabilities keyed by the analysis
-        label
-    prior: dict
-        dictionary containing the prior samples for each analysis
-    weights: dict
-        dictionary of weights for each samples for each analysis
-    detectors: list
-        list of IFOs used in each analysis
+    injection_parameters: dict
+        dictionary of injection parameters extracted from the result file
 
     Methods
     -------
     to_dat:
         save the posterior samples to a .dat file
     to_latex_table:
         convert the posterior samples to a latex table
     generate_latex_macros:
         generate a set of latex macros for the stored posterior samples
-    to_lalinference:
-        convert the posterior samples to a lalinference result file
-    to_bilby:
-        convert the posterior samples to a bilby result file
-    generate_all_posterior_samples:
-        generate all posterior distributions that may be derived from
-        sampled distributions
     """
-    def __init__(self, path_to_results_file, **kwargs):
-        super(PESummary, self).__init__(
-            path_to_results_file=path_to_results_file
-        )
+    def load_map(self):
+        return {
+            "json": self._grab_data_from_json_file,
+            "dat": self._grab_data_from_dat_file,
+            "txt": self._grab_data_from_dat_file,
+            "csv": self._grab_data_from_csv_file,
+            "hdf5": self._grab_data_from_hdf5_file,
+            "h5": self._grab_data_from_hdf5_file,
+            "hdf": self._grab_data_from_hdf5_file,
+            "db": self._grab_data_from_sql_database,
+            "sql": self._grab_data_from_sql_database,
+            "prior": self._grab_data_from_prior_file,
+            "npy": self._grab_data_from_numpy_file
+        }
+
+    def __new__(
+        self, path_to_results_file, _single_default=SingleAnalysisDefault,
+        _multi_default=MultiAnalysisDefault, **kwargs
+    ):
+        self.module = "core"
+        self.extension = Read.extension_from_path(path_to_results_file)
+        self.load_function = self.load_map(self)[self.extension]
+        try:
+            self._load_data = self.load_function(path_to_results_file, **kwargs)
+        except Exception as e:
+            raise Exception(
+                "Failed to read data for file %s because: %s" % (
+                    path_to_results_file, e
+                )
+            )
+        if np.array(self._load_data["parameters"]).ndim > 1:
+            return _multi_default(
+                path_to_results_file, _data=self._load_data, **kwargs
+            )
+        else:
+            return _single_default(
+                path_to_results_file, _data=self._load_data, **kwargs
+            )
 
-    @property
-    def load_kwargs(self):
-        return dict(grab_data_from_dictionary=self._grab_data_from_dictionary)
+    @classmethod
+    def load_file(cls, path, **kwargs):
+        if not os.path.isfile(path):
+            raise FileNotFoundError("%s does not exist" % (path))
+        return cls(path, **kwargs)
 
-    def load(self, function, **kwargs):
-        """Load a results file according to a given function
+    @staticmethod
+    def _default_injection(parameters):
+        """Return a dictionary of nan's for each parameter
 
         Parameters
         ----------
-        function: func
-            callable function that will load in your results file
+        parameters: tuple/list
+            list of parameters you wish to have null injections for
         """
-        data = self.load_from_function(
-            function, self.path_to_results_file, **kwargs)
-        self.data = data
-        self.parameters = self.data["parameters"]
-        self.samples = self.data["samples"]
-        if "mcmc_samples" in data.keys():
-            self.mcmc_samples = data["mcmc_samples"]
-        if "version" in data.keys() and data["version"] is not None:
-            self.input_version = data["version"]
-        else:
-            self.input_version = "No version information found"
-        if "kwargs" in data.keys():
-            self.extra_kwargs = data["kwargs"]
-        else:
-            self.extra_kwargs = {"sampler": {}, "meta_data": {}}
-            self.extra_kwargs["sampler"]["nsamples"] = len(self.data["samples"])
-        if data["injection"] is not None:
-            self.injection_parameters = data["injection"]
-        if "prior" in data.keys() and data["prior"] != {}:
-            self.priors = data["prior"]
-        if "weights" in self.data.keys():
-            self.weights = self.data["weights"]
-        if "approximant" in self.data.keys():
-            self.approximant = self.data["approximant"]
-        if "labels" in self.data.keys():
-            self.labels = self.data["labels"]
-        if "config" in self.data.keys():
-            self.config = self.data["config"]
-        if "psd" in self.data.keys():
-            from pesummary.gw.file.psd import PSDDict
-
-            try:
-                self.psd = {
-                    label: PSDDict(
-                        {ifo: value for ifo, value in psd_data.items()}
-                    ) for label, psd_data in self.data["psd"].items()
-                }
-            except (KeyError, AttributeError):
-                self.psd = self.data["psd"]
-        if "calibration" in self.data.keys():
-            from pesummary.gw.file.calibration import Calibration
-
-            try:
-                self.calibration = {
-                    label: {
-                        ifo: Calibration(value) for ifo, value in
-                        calibration_data.items()
-                    } for label, calibration_data in
-                    self.data["calibration"].items()
-                }
-            except (KeyError, AttributeError):
-                self.calibration = self.data["calibration"]
-        if "calibration" in self.data["prior"].keys():
-            from pesummary.gw.file.calibration import Calibration
-
-            try:
-                self.priors["calibration"] = {
-                    label: {
-                        ifo: Calibration(value) for ifo, value in
-                        calibration_data.items()
-                    } for label, calibration_data in
-                    self.data["prior"]["calibration"].items()
-                }
-            except (KeyError, AttributeError):
-                pass
-        if "skymap" in self.data.keys():
-            from pesummary.gw.file.skymap import SkyMapDict, SkyMap
-
-            try:
-                self.skymap = SkyMapDict({
-                    label: SkyMap(skymap["data"], skymap["meta_data"])
-                    for label, skymap in self.data["skymap"].items()
-                })
-            except (KeyError, AttributeError):
-                self.skymap = self.data["skymap"]
+        return {i: float("nan") for i in parameters}
 
     @staticmethod
-    def _grab_data_from_dictionary(dictionary):
-        """
+    def _grab_data_from_dat_file(path, **kwargs):
+        """Grab the data stored in a .dat file
         """
-        stored_data = CorePESummary._grab_data_from_dictionary(
-            dictionary=dictionary
-        )
+        from pesummary.core.file.formats.dat import read_dat
 
-        approx_list = list()
-        psd_dict, cal_dict, skymap_dict = {}, {}, {}
-        psd, cal = None, None
-        for num, label in enumerate(stored_data["labels"]):
-            data, = GWRead.load_recursively(label, dictionary)
-            if "psds" in data.keys():
-                psd_dict[label] = data["psds"]
-            if "calibration_envelope" in data.keys():
-                cal_dict[label] = data["calibration_envelope"]
-            if "skymap" in data.keys():
-                skymap_dict[label] = data["skymap"]
-            if "approximant" in data.keys():
-                approx_list.append(data["approximant"])
-            else:
-                approx_list.append(None)
-        stored_data["approximant"] = approx_list
-        stored_data["calibration"] = cal_dict
-        stored_data["psd"] = psd_dict
-        stored_data["skymap"] = skymap_dict
-        return stored_data
-
-    @property
-    def calibration_data_in_results_file(self):
-        if self.calibration:
-            keys = [list(self.calibration[i].keys()) for i in self.labels]
-            total = [[self.calibration[key][ifo] for ifo in keys[num]] for
-                     num, key in enumerate(self.labels)]
-            return total, keys
-        return None
-
-    @property
-    def detectors(self):
-        det_list = list()
-        for parameters in self.parameters:
-            detectors = list()
-            for param in parameters:
-                if "_optimal_snr" in param and param != "network_optimal_snr":
-                    detectors.append(param.split("_optimal_snr")[0])
-            if not detectors:
-                detectors.append(None)
-            det_list.append(detectors)
-        return det_list
-
-    def write(self, labels="all", **kwargs):
-        """Save the data to file
+        parameters, samples = read_dat(path)
+        return {
+            "parameters": parameters, "samples": samples,
+            "injection": Default._default_injection(parameters)
+        }
 
-        Parameters
-        ----------
-        package: str, optional
-            package you wish to use when writing the data
-        kwargs: dict, optional
-            all additional kwargs are passed to the pesummary.io.write function
+    @staticmethod
+    def _grab_data_from_numpy_file(path, **kwargs):
+        """Grab the data stored in a .npy file
         """
-        approximant = {
-            label: self.approximant[num] if self.approximant[num] != {} else
-            None for num, label in enumerate(self.labels)
+        from pesummary.core.file.formats.numpy import read_numpy
+
+        parameters, samples = read_numpy(path)
+        return {
+            "parameters": parameters, "samples": samples,
+            "injection": Default._default_injection(parameters)
         }
-        properties = dict(
-            calibration=self.calibration, psd=self.psd, approximant=approximant,
-            skymap=self.skymap
-        )
-        CorePESummary.write(
-            self, package="gw", labels=labels, cls_properties=properties, **kwargs
-        )
 
-    def generate_all_posterior_samples(self, **conversion_kwargs):
-        if "no_conversion" in conversion_kwargs.keys():
-            no_conversion = conversion_kwargs.pop("no_conversion")
-        else:
-            no_conversion = False
-        if no_conversion:
-            return
-        from pesummary.gw.file.conversions import _Conversion
-
-        converted_params, converted_samples, converted_kwargs = [], [], []
-        for param, samples, kwargs in zip(
-                self.parameters, self.samples, self.extra_kwargs
-        ):
-            if conversion_kwargs.get("evolve_spins", False):
-                if not conversion_kwargs.get("return_kwargs", False):
-                    conversion_kwargs["return_kwargs"] = True
-            data = _Conversion(
-                param, samples, extra_kwargs=kwargs, return_dict=False,
-                **conversion_kwargs
-            )
-            converted_params.append(data[0])
-            converted_samples.append(data[1])
-            if kwargs.get("return_kwargs", False):
-                converted_kwargs.append(data[2])
-        self.parameters = converted_params
-        self.samples = converted_samples
-        if converted_kwargs != []:
-            self.extra_kwargs = {
-                label: converted_kwargs[num] for num, label in enumerate(
-                    self.labels
-                )
-            }
+    @staticmethod
+    def _grab_data_from_csv_file(path, **kwargs):
+        """Grab the data stored in a .csv file
+        """
+        from pesummary.core.file.formats.csv import read_csv
 
-    def to_bilby(self, labels="all", **kwargs):
-        """Convert a PESummary metafile to a bilby results object
+        parameters, samples = read_csv(path)
+        return {
+            "parameters": parameters, "samples": samples,
+            "injection": Default._default_injection(parameters)
+        }
+
+    @staticmethod
+    def _grab_data_from_prior_file(path, module="core", **kwargs):
+        """Grab the data stored in a .prior file
         """
-        from bilby.gw.result import CompactBinaryCoalescenceResult
+        import importlib
 
-        return CorePESummary.write(
-            self, labels=labels, package="core", file_format="bilby",
-            _return=True, cls=CompactBinaryCoalescenceResult, **kwargs
+        module = importlib.import_module(
+            "pesummary.{}.file.formats.bilby".format(module)
         )
+        func = getattr(module, "prior_samples_from_file")
 
-    def to_lalinference(self, labels="all", **kwargs):
-        """Convert the samples stored in a PESummary metafile to a .dat file
+        samples = func(path, **kwargs)
+        parameters = samples.parameters
+        analytic = samples.analytic
+        return {
+            "parameters": parameters, "samples": samples.samples.T.tolist(),
+            "injection": Default._default_injection(parameters),
+            "analytic": analytic
+        }
 
-        Parameters
-        ----------
-        labels: list, optional
-            optional list of analyses to save to file
-        kwargs: dict, optional
-            all additional kwargs are passed to the pesummary.io.write function
+    @staticmethod
+    def _grab_data_from_sql_database(path, **kwargs):
+        """Grab the data stored in a sql database
         """
-        return self.write(
-            labels=labels, file_format="lalinference", **kwargs
-        )
+        from pesummary.core.file.formats.sql import read_sql
 
+        parameters, samples, labels = read_sql(path, **kwargs)
+        if len(labels) > 1:
+            injection = {
+                label: Default._default_injection(parameters[num]) for num, label
+                in enumerate(labels)
+            }
+        else:
+            injection = Default._default_injection(parameters)
+        return {
+            "parameters": parameters, "samples": samples, "injection": injection,
+            "labels": labels
+        }
 
-class PESummaryDeprecated(PESummary):
-    """
-    """
-    def __init__(self, path_to_results_file, **kwargs):
-        warnings.warn(deprecation_warning)
-        super(PESummaryDeprecated, self).__init__(path_to_results_file, **kwargs)
+    @staticmethod
+    def _grab_data_from_json_file(path, path_to_samples=None, **kwargs):
+        """Grab the data stored in a .json file
+        """
+        from pesummary.core.file.formats.json import read_json
 
-    @property
-    def load_kwargs(self):
+        parameters, samples = read_json(path, path_to_samples=path_to_samples)
         return {
-            "grab_data_from_dictionary": PESummaryDeprecated._grab_data_from_dictionary
+            "parameters": parameters, "samples": samples,
+            "injection": Default._default_injection(parameters)
         }
 
     @staticmethod
-    def _grab_data_from_dictionary(dictionary):
-        """
+    def _grab_data_from_hdf5_file(
+        path, remove_params=[], path_to_samples=None, **kwargs
+    ):
+        """Grab the data stored in an hdf5 file
         """
-        data = CorePESummaryDeprecated._grab_data_from_dictionary(
-            dictionary=dictionary
+        from pesummary.core.file.formats.hdf5 import read_hdf5
+
+        parameters, samples = read_hdf5(
+            path, remove_params=remove_params, path_to_samples=path_to_samples
         )
+        return {
+            "parameters": parameters, "samples": samples,
+            "injection": Default._default_injection(parameters)
+        }
 
-        approx_list = list()
-        psd, cal = None, None
-        for num, key in enumerate(data["labels"]):
-            if "psds" in dictionary.keys():
-                psd, = GWRead.load_recursively("psds", dictionary)
-            if "calibration_envelope" in dictionary.keys():
-                cal, = GWRead.load_recursively("calibration_envelope", dictionary)
-            if "approximant" in dictionary.keys():
-                if key in dictionary["approximant"].keys():
-                    approx_list.append(dictionary["approximant"][key])
-                else:
-                    approx_list.append(None)
-            else:
-                approx_list.append(None)
-        data["approximant"] = approx_list
-        data["calibration"] = cal
-        data["psd"] = psd
+    def add_marginalized_parameters_from_config_file(self, config_file):
+        """Search the configuration file and add the marginalized parameters
+        to the list of parameters and samples
 
-        return data
+        Parameters
+        ----------
+        config_file: str
+            path to the configuration file
+        """
+        pass
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/conversions.py` & `pesummary-1.0.0/pesummary/gw/conversions/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,1420 +1,82 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
+error_msg = (
+    "Unable to install '{}'. You will not be able to use some of the inbuilt "
+    "functions."
+)
+import copy
 import numpy as np
+from pathlib import Path
 
-from pesummary.utils.utils import logger, iterator
-from pesummary.utils.decorators import array_input
-from pesummary.utils.exceptions import EvolveSpinError
 from pesummary import conf
+from pesummary.utils.decorators import set_docstring
+from pesummary.utils.exceptions import EvolveSpinError
+from pesummary.utils.utils import logger
 
 try:
     import lalsimulation
-    from lalsimulation import (
-        SimInspiralTransformPrecessingNewInitialConditions,
-        SimInspiralTransformPrecessingWvf2PE, DetectorPrefixToLALDetector,
-        FLAG_SEOBNRv4P_HAMILTONIAN_DERIVATIVE_NUMERICAL,
-        FLAG_SEOBNRv4P_EULEREXT_QNM_SIMPLE_PRECESSION,
-        FLAG_SEOBNRv4P_ZFRAME_L, SimNeutronStarEOS4ParameterPiecewisePolytrope,
-        SimNeutronStarRadius, CreateSimNeutronStarFamily,
-        SimNeutronStarLoveNumberK2, SimNeutronStarEOS4ParameterSpectralDecomposition
-    )
-    from lal import MSUN_SI, C_SI, MRSUN_SI
-    LALINFERENCE_INSTALL = True
 except ImportError:
-    LALINFERENCE_INSTALL = False
-
+    logger.warning(error_msg.format("lalsimulation"))
 try:
-    from astropy.cosmology import z_at_value, Planck15
-    import astropy.units as u
-    from astropy.time import Time
-    ASTROPY = True
+    import astropy
 except ImportError:
-    ASTROPY = False
-    logger.warning("You do not have astropy installed currently. You will"
-                   " not be able to use some of the prebuilt functions.")
-
-DEFAULT_SEOBFLAGS = {
-    "SEOBNRv4P_SpinAlignedEOBversion": 4,
-    "SEOBNRv4P_SymmetrizehPlminusm": 1,
-    "SEOBNRv4P_HamiltonianDerivative": FLAG_SEOBNRv4P_HAMILTONIAN_DERIVATIVE_NUMERICAL,
-    "SEOBNRv4P_euler_extension": FLAG_SEOBNRv4P_EULEREXT_QNM_SIMPLE_PRECESSION,
-    "SEOBNRv4P_Zframe": FLAG_SEOBNRv4P_ZFRAME_L,
-    "SEOBNRv4P_debug": 0
-}
-
-
-@array_input
-def _z_from_dL_exact(luminosity_distance, cosmology):
-    """Return the redshift given samples for the luminosity distance
-    """
-    return z_at_value(
-        cosmology.luminosity_distance, luminosity_distance * u.Mpc
-    )
-
-
-def z_from_dL_exact(luminosity_distance, cosmology="Planck15", multi_process=1):
-    """Return the redshift given samples for the luminosity distance
-    """
-    from pesummary.gw.cosmology import get_cosmology
-    import multiprocessing
-
-    logger.warning("Estimating the exact redshift for every luminosity "
-                   "distance. This may take a few minutes.")
-    cosmo = get_cosmology(cosmology)
-    args = np.array(
-        [luminosity_distance, [cosmo] * len(luminosity_distance)],
-        dtype=object
-    ).T
-    with multiprocessing.Pool(multi_process) as pool:
-        z = pool.starmap(_z_from_dL_exact, args)
-    return z
-
-
-@array_input
-def z_from_dL_approx(
-    luminosity_distance, N=100, cosmology="Planck15", **kwargs
-):
-    """Return the approximate redshift given samples for the luminosity
-    distance. This technique uses interpolation to estimate the redshift
-    """
-    from pesummary.gw.cosmology import get_cosmology
-
-    logger.warning("The redshift is being approximated using interpolation. "
-                   "Bear in mind that this does introduce a small error.")
-    cosmo = get_cosmology(cosmology)
-    d_min = np.min(luminosity_distance)
-    d_max = np.max(luminosity_distance)
-    zmin = z_at_value(cosmo.luminosity_distance, d_min * u.Mpc)
-    zmax = z_at_value(cosmo.luminosity_distance, d_max * u.Mpc)
-    zgrid = np.logspace(np.log10(zmin), np.log10(zmax), N)
-    Dgrid = [cosmo.luminosity_distance(i).value for i in zgrid]
-    zvals = np.interp(luminosity_distance, Dgrid, zgrid)
-    return zvals
-
-
-@array_input
-def dL_from_z(redshift, cosmology="Planck15"):
-    """Return the luminosity distance given samples for the redshift
-    """
-    from pesummary.gw.cosmology import get_cosmology
-
-    cosmo = get_cosmology(cosmology)
-    return cosmo.luminosity_distance(redshift).value
-
-
-@array_input
-def comoving_distance_from_z(redshift, cosmology="Planck15"):
-    """Return the comoving distance given samples for the redshift
-    """
-    from pesummary.gw.cosmology import get_cosmology
-
-    cosmo = get_cosmology(cosmology)
-    return cosmo.comoving_distance(redshift).value
-
-
-def _source_from_detector(parameter, z):
-    """Return the source parameter given samples for the detector parameter and
-    the redshift
-    """
-    return parameter / (1. + z)
-
-
-def _detector_from_source(parameter, z):
-    """Return the detector parameter given samples for the source parameter and
-    the redshift
-    """
-    return parameter * (1. + z)
-
-
-@array_input
-def m1_source_from_m1_z(mass_1, z):
-    """Return the source mass of the bigger black hole given samples for the
-    detector mass of the bigger black hole and the redshift
-    """
-    return _source_from_detector(mass_1, z)
-
-
-@array_input
-def m2_source_from_m2_z(mass_2, z):
-    """Return the source mass of the smaller black hole given samples for the
-    detector mass of the smaller black hole and the redshift
-    """
-    return _source_from_detector(mass_2, z)
-
-
-@array_input
-def m_total_source_from_mtotal_z(total_mass, z):
-    """Return the source total mass of the binary given samples for detector
-    total mass and redshift
-    """
-    return _source_from_detector(total_mass, z)
-
-
-@array_input
-def mtotal_from_mtotal_source_z(total_mass_source, z):
-    """Return the total mass of the binary given samples for the source total
-    mass and redshift
-    """
-    return _detector_from_source(total_mass_source, z)
-
-
-@array_input
-def mchirp_source_from_mchirp_z(mchirp, z):
-    """Return the source chirp mass of the binary given samples for detector
-    chirp mass and redshift
-    """
-    return _source_from_detector(mchirp, z)
-
-
-@array_input
-def mchirp_from_mchirp_source_z(mchirp_source, z):
-    """Return the chirp mass of the binary given samples for the source chirp
-    mass and redshift
-    """
-    return _detector_from_source(mchirp_source, z)
-
-
-@array_input
-def mchirp_from_m1_m2(mass1, mass2):
-    """Return the chirp mass given the samples for mass1 and mass2
-
-    Parameters
-    ----------
-    """
-    return (mass1 * mass2)**0.6 / (mass1 + mass2)**0.2
-
-
-@array_input
-def m_total_from_m1_m2(mass1, mass2):
-    """Return the total mass given the samples for mass1 and mass2
-    """
-    return mass1 + mass2
-
-
-@array_input
-def m1_from_mchirp_q(mchirp, q):
-    """Return the mass of the larger black hole given the chirp mass and
-    mass ratio
-    """
-    return ((1. / q)**(2. / 5.)) * ((1.0 + (1. / q))**(1. / 5.)) * mchirp
-
-
-@array_input
-def m2_from_mchirp_q(mchirp, q):
-    """Return the mass of the smaller black hole given the chirp mass and
-    mass ratio
-    """
-    return ((1. / q)**(-3. / 5.)) * ((1.0 + (1. / q))**(1. / 5.)) * mchirp
-
-
-@array_input
-def eta_from_m1_m2(mass1, mass2):
-    """Return the symmetric mass ratio given the samples for mass1 and mass2
-    """
-    return (mass1 * mass2) / (mass1 + mass2)**2
-
-
-@array_input
-def q_from_m1_m2(mass1, mass2):
-    """Return the mass ratio given the samples for mass1 and mass2
-    """
-    return mass2 / mass1
-
-
-@array_input
-def invq_from_m1_m2(mass1, mass2):
-    """Return the inverted mass ratio (mass1/mass2 for mass1 > mass2)
-    given the samples for mass1 and mass2
-    """
-    return 1. / q_from_m1_m2(mass1, mass2)
-
-
-@array_input
-def invq_from_q(mass_ratio):
-    """Return the inverted mass ratio (mass1/mass2 for mass1 > mass2)
-    given the samples for mass ratio (mass2/mass1)
-    """
-    return 1. / mass_ratio
-
-
-@array_input
-def q_from_eta(symmetric_mass_ratio):
-    """Return the mass ratio given samples for symmetric mass ratio
-    """
-    temp = (1 / symmetric_mass_ratio / 2 - 1)
-    return (temp - (temp ** 2 - 1) ** 0.5)
-
-
-@array_input
-def mchirp_from_mtotal_q(total_mass, mass_ratio):
-    """Return the chirp mass given samples for total mass and mass ratio
-    """
-    mass1 = (1. / mass_ratio) * total_mass / (1. + (1. / mass_ratio))
-    mass2 = total_mass / (1. + (1. / mass_ratio))
-    return eta_from_m1_m2(mass1, mass2)**(3. / 5) * (mass1 + mass2)
-
-
-@array_input
-def chi_p(mass1, mass2, spin1x, spin1y, spin2x, spin2y):
-    """Return chi_p given samples for mass1, mass2, spin1x, spin1y, spin2x,
-    spin2y
-    """
-    mass_ratio = mass2 / mass1
-    S1_perp = ((spin1x)**2 + (spin1y)**2)**0.5
-    S2_perp = ((spin2x)**2 + (spin2y)**2)**0.5
-    chi_p = np.maximum(
-        S1_perp, (4 * mass_ratio + 3) / (3 * mass_ratio + 4) * mass_ratio
-        * S2_perp
-    )
-    return chi_p
-
-
-@array_input
-def chi_eff(mass1, mass2, spin1z, spin2z):
-    """Return chi_eff given samples for mass1, mass2, spin1z, spin2z
-    """
-    return (spin1z * mass1 + spin2z * mass2) / (mass1 + mass2)
-
-
-@array_input
-def phi_12_from_phi1_phi2(phi1, phi2):
-    """Return the difference in azimuthal angle between S1 and S2 given samples
-    for phi1 and phi2
-    """
-    phi12 = phi2 - phi1
-    if isinstance(phi12, float) and phi12 < 0.:
-        phi12 += 2 * np.pi
-    elif isinstance(phi12, np.ndarray):
-        ind = np.where(phi12 < 0.)
-        phi12[ind] += 2 * np.pi
-    return phi12
-
-
-@array_input
-def phi1_from_spins(spin_1x, spin_1y):
-    """Return phi_1 given samples for spin_1x and spin_1y
-    """
-    phi_1 = np.fmod(2 * np.pi + np.arctan2(spin_1y, spin_1x), 2 * np.pi)
-    return phi_1
-
-
-@array_input
-def phi2_from_spins(spin_2x, spin_2y):
-    """Return phi_2 given samples for spin_2x and spin_2y
-    """
-    phi_2 = np.fmod(2 * np.pi + np.arctan2(spin_2y, spin_2x), 2 * np.pi)
-    return phi_2
-
-
-@array_input
-def spin_angles(mass_1, mass_2, inc, spin1x, spin1y, spin1z, spin2x, spin2y,
-                spin2z, f_ref, phase):
-    """Return the spin angles given samples for mass_1, mass_2, inc, spin1x,
-    spin1y, spin1z, spin2x, spin2y, spin2z, f_ref, phase
-    """
-    return_float = False
-    if isinstance(mass_1, (int, float)):
-        return_float = True
-        mass_1 = [mass_1]
-        mass_2 = [mass_2]
-        inc = [inc]
-        spin1x = [spin1x]
-        spin1y = [spin1y]
-        spin1z = [spin1z]
-        spin2x = [spin2x]
-        spin2y = [spin2y]
-        spin2z = [spin2z]
-        f_ref = [f_ref]
-        phase = [phase]
-
-    if LALINFERENCE_INSTALL:
-        data = []
-        for i in range(len(mass_1)):
-            theta_jn, phi_jl, tilt_1, tilt_2, phi_12, a_1, a_2 = \
-                SimInspiralTransformPrecessingWvf2PE(
-                    incl=inc[i], m1=mass_1[i], m2=mass_2[i], S1x=spin1x[i],
-                    S1y=spin1y[i], S1z=spin1z[i], S2x=spin2x[i], S2y=spin2y[i],
-                    S2z=spin2z[i], fRef=float(f_ref[i]), phiRef=float(phase[i]))
-            data.append([theta_jn, phi_jl, tilt_1, tilt_2, phi_12, a_1, a_2])
-        if return_float:
-            return data[0]
-        return data
-
-
-@array_input
-def component_spins(theta_jn, phi_jl, tilt_1, tilt_2, phi_12, a_1, a_2, mass_1,
-                    mass_2, f_ref, phase):
-    """Return the component spins given samples for theta_jn, phi_jl, tilt_1,
-    tilt_2, phi_12, a_1, a_2, mass_1, mass_2, f_ref, phase
-    """
-    if LALINFERENCE_INSTALL:
-        data = []
-        for i in range(len(theta_jn)):
-            iota, S1x, S1y, S1z, S2x, S2y, S2z = \
-                SimInspiralTransformPrecessingNewInitialConditions(
-                    theta_jn[i], phi_jl[i], tilt_1[i], tilt_2[i], phi_12[i],
-                    a_1[i], a_2[i], mass_1[i] * MSUN_SI, mass_2[i] * MSUN_SI,
-                    float(f_ref[i]), float(phase[i]))
-            data.append([iota, S1x, S1y, S1z, S2x, S2y, S2z])
-        return data
-    else:
-        raise Exception("Please install LALSuite for full conversions")
-
-
-@array_input
-def spin_angles_from_azimuthal_and_polar_angles(
-        a_1, a_2, a_1_azimuthal, a_1_polar, a_2_azimuthal, a_2_polar):
-    """Return the spin angles given samples for a_1, a_2, a_1_azimuthal,
-    a_1_polar, a_2_azimuthal, a_2_polar
-    """
-    spin1x = a_1 * np.sin(a_1_polar) * np.cos(a_1_azimuthal)
-    spin1y = a_1 * np.sin(a_1_polar) * np.sin(a_1_azimuthal)
-    spin1z = a_1 * np.cos(a_1_polar)
-
-    spin2x = a_2 * np.sin(a_2_polar) * np.cos(a_2_azimuthal)
-    spin2y = a_2 * np.sin(a_2_polar) * np.sin(a_2_azimuthal)
-    spin2z = a_2 * np.cos(a_2_polar)
-
-    data = [[s1x, s1y, s1z, s2x, s2y, s2z] for s1x, s1y, s1z, s2x, s2y, s2z in
-            zip(spin1x, spin1y, spin1z, spin2x, spin2y, spin2z)]
-    return data
-
-
-@array_input
-def time_in_each_ifo(detector, ra, dec, time_gps):
-    """Return the event time in a given detector, given samples for ra, dec,
-    time
-    """
-    if LALINFERENCE_INSTALL and ASTROPY:
-        gmst = Time(time_gps, format='gps', location=(0, 0))
-        corrected_ra = gmst.sidereal_time('mean').rad - ra
-
-        i = np.cos(dec) * np.cos(corrected_ra)
-        j = np.cos(dec) * -1 * np.sin(corrected_ra)
-        k = np.sin(dec)
-        n = np.array([i, j, k])
-
-        dx = [0, 0, 0] - DetectorPrefixToLALDetector(detector).location
-        dt = dx.dot(n) / C_SI
-        return time_gps + dt
-    else:
-        raise Exception("Please install LALSuite and astropy for full "
-                        "conversions")
-
-
-@array_input
-def lambda_tilde_from_lambda1_lambda2(lambda1, lambda2, mass1, mass2):
-    """Return the dominant tidal term given samples for lambda1 and lambda2
-    """
-    eta = eta_from_m1_m2(mass1, mass2)
-    plus = lambda1 + lambda2
-    minus = lambda1 - lambda2
-    lambda_tilde = 8 / 13 * (
-        (1 + 7 * eta - 31 * eta**2) * plus
-        + (1 - 4 * eta)**0.5 * (1 + 9 * eta - 11 * eta**2) * minus)
-    return lambda_tilde
-
-
-@array_input
-def delta_lambda_from_lambda1_lambda2(lambda1, lambda2, mass1, mass2):
-    """Return the second dominant tidal term given samples for lambda1 and
-    lambda 2
-    """
-    eta = eta_from_m1_m2(mass1, mass2)
-    plus = lambda1 + lambda2
-    minus = lambda1 - lambda2
-    delta_lambda = 1 / 2 * (
-        (1 - 4 * eta) ** 0.5 * (1 - 13272 / 1319 * eta + 8944 / 1319 * eta**2)
-        * plus + (1 - 15910 / 1319 * eta + 32850 / 1319 * eta**2
-                  + 3380 / 1319 * eta**3) * minus)
-    return delta_lambda
-
-
-@array_input
-def lambda1_from_lambda_tilde(lambda_tilde, mass1, mass2):
-    """Return the individual tidal parameter given samples for lambda_tilde
-    """
-    eta = eta_from_m1_m2(mass1, mass2)
-    q = q_from_m1_m2(mass1, mass2)
-    lambda1 = 13 / 8 * lambda_tilde / (
-        (1 + 7 * eta - 31 * eta**2) * (1 + q**-5)
-        + (1 - 4 * eta)**0.5 * (1 + 9 * eta - 11 * eta**2) * (1 - q**-5))
-    return lambda1
-
-
-@array_input
-def lambda2_from_lambda1(lambda1, mass1, mass2):
-    """Return the individual tidal parameter given samples for lambda1
-    """
-    q = q_from_m1_m2(mass1, mass2)
-    lambda2 = lambda1 / q**5
-    return lambda2
-
-
-def _lambda1_lambda2_from_eos(eos, mass_1, mass_2):
-    """Return lambda_1 and lambda_2 assuming a given equation of state
-    """
-    fam = CreateSimNeutronStarFamily(eos)
-    _lambda = []
-    for mass in [mass_1, mass_2]:
-        r = SimNeutronStarRadius(mass * MSUN_SI, fam)
-        k = SimNeutronStarLoveNumberK2(mass * MSUN_SI, fam)
-        c = mass * MRSUN_SI / r
-        _lambda.append((2. / 3.) * k / c**5.0)
-    return _lambda
-
-
-def wrapper_for_lambda1_lambda2_polytrope_EOS(args):
-    """Wrapper function to calculate the tidal deformability parameters from the
-    4_parameter_piecewise_polytrope_equation_of_state parameters for a pool
-    of workers
-    """
-    return _lambda1_lambda2_from_4_parameter_piecewise_polytrope_equation_of_state(*args)
-
-
-def _lambda1_lambda2_from_4_parameter_piecewise_polytrope_equation_of_state(
-    log_pressure_si, gamma_1, gamma_2, gamma_3, mass_1, mass_2
-):
-    """Wrapper function to calculate the tidal deformability parameters from the
-    4_parameter_piecewise_polytrope_equation_of_state parameters for a pool
-    of workers
-    """
-    eos = SimNeutronStarEOS4ParameterPiecewisePolytrope(
-        log_pressure_si, gamma_1, gamma_2, gamma_3
-    )
-    return _lambda1_lambda2_from_eos(eos, mass_1, mass_2)
-
-
-def wrapper_for_lambda1_lambda2_from_spectral_decomposition(args):
-    """Wrapper function to calculate the tidal deformability parameters from
-    the spectral decomposition parameters for a pool of workers
-    """
-    return _lambda1_lambda2_from_spectral_decomposition(*args)
-
-
-def _lambda1_lambda2_from_spectral_decomposition(
-    spectral_decomposition_gamma_0, spectral_decomposition_gamma_1,
-    spectral_decomposition_gamma_2, spectral_decomposition_gamma_3,
-    mass_1, mass_2
-):
-    """Wrapper function to calculate the tidal deformability parameters from
-    the spectral decomposition parameters for a pool of workers
-    """
-    gammas = [
-        spectral_decomposition_gamma_0, spectral_decomposition_gamma_1,
-        spectral_decomposition_gamma_2, spectral_decomposition_gamma_3
-    ]
-    eos = SimNeutronStarEOS4ParameterSpectralDecomposition(*gammas)
-    return _lambda1_lambda2_from_eos(eos, mass_1, mass_2)
-
-
-@array_input
-def lambda1_lambda2_from_4_parameter_piecewise_polytrope_equation_of_state(
-    log_pressure, gamma_1, gamma_2, gamma_3, mass_1, mass_2, multi_process=1
-):
-    """Convert 4 parameter piecewise polytrope EOS parameters to the tidal
-    deformability parameters lambda_1, lambda_2
-    """
-    import multiprocessing
-
-    logger.warn(
-        "Calculating the tidal deformability parameters based on the 4 "
-        "parameter piecewise polytrope equation of state parameters. This may "
-        "take some time"
-    )
-    log_pressure_si = log_pressure - 1.
-    args = np.array(
-        [log_pressure_si, gamma_1, gamma_2, gamma_3, mass_1, mass_2]
-    ).T
-    with multiprocessing.Pool(multi_process[0]) as pool:
-        lambdas = np.array(
-            list(
-                iterator(
-                    pool.imap(wrapper_for_lambda1_lambda2_polytrope_EOS, args),
-                    tqdm=True, logger=logger, total=len(mass_1),
-                    desc="Calculating tidal parameters"
-                )
-            )
-        )
-
-    lambdas = np.array(lambdas).T
-    return lambdas[0], lambdas[1]
-
-
-@array_input
-def lambda1_lambda2_from_spectral_decomposition(
-    spectral_decomposition_gamma_0, spectral_decomposition_gamma_1,
-    spectral_decomposition_gamma_2, spectral_decomposition_gamma_3,
-    mass_1, mass_2, multi_process=1
-):
-    """Convert spectral decomposition parameters to the tidal deformability
-    parameters lambda_1, lambda_2
-    """
-    import multiprocessing
-
-    logger.warn(
-        "Calculating the tidal deformability parameters from the spectral "
-        "decomposition equation of state parameters. This may take some time"
-    )
-    args = np.array(
-        [
-            spectral_decomposition_gamma_0, spectral_decomposition_gamma_1,
-            spectral_decomposition_gamma_2, spectral_decomposition_gamma_3,
-            mass_1, mass_2
-        ]
-    ).T
-    with multiprocessing.Pool(multi_process[0]) as pool:
-        lambdas = np.array(
-            list(
-                iterator(
-                    pool.imap(wrapper_for_lambda1_lambda2_from_spectral_decomposition, args),
-                    tqdm=True, logger=logger, total=len(mass_1),
-                    desc="Calculating tidal parameters"
-                )
-            )
-        )
-
-    lambdas = np.array(lambdas).T
-    return lambdas[0], lambdas[1]
-
-
-@array_input
-def lambda1_from_4_parameter_piecewise_polytrope_equation_of_state(
-    log_pressure, gamma_1, gamma_2, gamma_3, mass_1, mass_2
-):
-    """Convert 4 parameter piecewise polytrope EOS parameters to the tidal
-    deformability parameters lambda_1
-    """
-    return lambda1_lambda2_from_4_parameter_piecewise_polytrope_equation_of_state(
-        log_pressure, gamma_1, gamma_2, gamma_3, mass_1, mass_2
-    )[0]
-
-
-@array_input
-def lambda2_from_4_parameter_piecewise_polytrope_equation_of_state(
-    log_pressure, gamma_1, gamma_2, gamma_3, mass_1, mass_2
-):
-    """Convert 4 parameter piecewise polytrope EOS parameters to the tidal
-    deformability parameters lambda_2
-    """
-    return lambda1_lambda2_from_4_parameter_piecewise_polytrope_equation_of_state(
-        log_pressure, gamma_1, gamma_2, gamma_3, mass_1, mass_2
-    )[1]
-
-
-@array_input
-def _ifo_snr(IFO_abs_snr, IFO_snr_angle):
-    """Return the matched filter SNR for a given IFO given samples for the
-    absolute SNR and the angle
-    """
-    return IFO_abs_snr * np.cos(IFO_snr_angle)
-
-
-@array_input
-def _ifo_snr_from_real_and_imaginary(IFO_real_snr, IFO_imag_snr):
-    """Return the matched filter SNR for a given IFO given samples for the
-    real and imaginary SNR
-    """
-    _complex = IFO_real_snr + IFO_imag_snr * 1j
-    _abs = np.abs(_complex)
-    return _ifo_snr(_abs, np.angle(_complex))
-
-
-@array_input
-def network_snr(snrs):
-    """Return the network SNR for N IFOs
-
-    Parameters
-    ----------
-    snrs: list
-        list of numpy.array objects containing the snrs samples for a particular
-        IFO
-    """
-    squares = np.square(snrs)
-    network_snr = np.sqrt(np.sum(squares, axis=0))
-    return network_snr
-
-
-@array_input
-def network_matched_filter_snr(IFO_matched_filter_snrs, IFO_optimal_snrs):
-    """Return the network matched filter SNR for a given detector network. Code
-    adapted from Christopher Berry's python notebook
-
-    Parameters
-    ----------
-    IFO_matched_filter_snrs: list
-        list of matched filter SNRs for each IFO in the network
-    IFO_optimal_snrs: list
-        list of optimal SNRs
-    """
-    for num, det_snr in enumerate(IFO_matched_filter_snrs):
-        complex_snr = np.iscomplex(det_snr)
-        convert_mf_snr = False
-        try:
-            if complex_snr:
-                convert_mf_snr = True
-        except ValueError:
-            if any(_complex for _complex in complex_snr):
-                convert_mf_snr = True
-        if convert_mf_snr:
-            IFO_matched_filter_snrs[num] = np.real(det_snr)
-    network_optimal_snr = network_snr(IFO_optimal_snrs)
-    network_matched_filter_snr = np.sum(
-        [
-            mf_snr * opt_snr / network_optimal_snr for mf_snr, opt_snr in zip(
-                IFO_matched_filter_snrs, IFO_optimal_snrs
-            )
-        ], axis=0
-    )
-    return network_matched_filter_snr
-
-
-@array_input
-def tilt_angles_and_phi_12_from_spin_vectors_and_L(a_1, a_2, Ln):
-    """Return the tilt angles and phi_12 given samples for the spin vectors
-    and the orbital angular momentum
-
-    Parameters
-    ----------
-    a_1: np.ndarray
-        Spin vector for the larger object
-    a_2: np.ndarray
-        Spin vector for the smaller object
-    Ln: np.ndarray
-        Orbital angular momentum of the binary
-    """
-    a_1_norm = np.linalg.norm(a_1)
-    a_2_norm = np.linalg.norm(a_2)
-    Ln /= np.linalg.norm(Ln)
-    a_1_dot = np.dot(a_1, Ln)
-    a_2_dot = np.dot(a_2, Ln)
-    a_1_perp = a_1 - a_1_dot * Ln
-    a_2_perp = a_2 - a_2_dot * Ln
-    cos_tilt_1 = a_1_dot / a_1_norm
-    cos_tilt_2 = a_2_dot / a_2_norm
-    cos_phi_12 = np.dot(a_1_perp, a_2_perp) / (
-        np.linalg.norm(a_1_perp) * np.linalg.norm(a_2_perp)
-    )
-    # set quadrant of phi12
-    phi_12 = np.arccos(cos_phi_12)
-    if np.sign(np.dot(Ln, np.cross(a_1, a_2))) < 0.:
-        phi_12 = 2. * np.pi - phi_12
-
-    return np.arccos(cos_tilt_1), np.arccos(cos_tilt_2), phi_12
-
-
-def _wrapper_return_final_mass_and_final_spin_from_waveform(args):
-    """Wrapper function to calculate the remnant properties for a given waveform
-    for a pool of workers
-
-    Parameters
-    ----------
-    args: np.ndarray
-        2 dimensional array giving arguments to pass to
-        _return_final_mass_and_final_spin_from_waveform. The first argument
-        in each sublist is the keyword and the second argument in each sublist
-        is the item you wish to pass
-    """
-    kwargs = {arg[0]: arg[1] for arg in args}
-    return _return_final_mass_and_final_spin_from_waveform(**kwargs)
-
-
-def _return_final_mass_and_final_spin_from_waveform(
-    mass_function=None, spin_function=None, mass_function_args=[],
-    spin_function_args=[], mass_function_return_function=None,
-    mass_function_return_index=None, spin_function_return_function=None,
-    spin_function_return_index=None, mass_1_index=0, mass_2_index=1,
-    nsamples=0, approximant=None, default_SEOBNRv4P_kwargs=False
-):
-    """Return the final mass and final spin given functions to use
-
-    Parameters
-    ----------
-    mass_function: func
-        function you wish to use to calculate the final mass
-    spin_function: func
-        function you wish to use to calculate the final spin
-    mass_function_args: list
-        list of arguments you wish to pass to mass_function
-    spin_function_args: list
-        list of arguments you wish to pass to spin_function
-    mass_function_return_function: str, optional
-        function used to extract the final mass from the quantity returned from
-        mass_function. For example, if mass_function returns a list and the
-        final_mass is a property of the 3 arg of this list,
-        mass_function_return_function='[3].final_mass'
-    mass_function_return_index: str, optional
-        if mass_function returns a list of parameters,
-        mass_function_return_index indicates the index of `final_mass` in the
-        list
-    spin_function_return_function: str, optional
-        function used to extract the final spin from the quantity returned from
-        spin_function. For example, if spin_function returns a list and the
-        final_spin is a property of the 3 arg of this list,
-        spin_function_return_function='[3].final_spin'
-    spin_function_return_index: str, optional
-        if spin_function returns a list of parameters,
-        spin_function_return_index indicates the index of `final_spin` in the
-        list
-    mass_1_index: int, optional
-        the index of mass_1 in mass_function_args. Default is 0
-    mass_2_index: int, optional
-        the index of mass_2 in mass_function_args. Default is 1
-    nsamples: int, optional
-        the total number of samples
-    approximant: str, optional
-        the approximant used
-    default_SEOBNRv4P_kwargs: Bool, optional
-        if True, use the default SEOBNRv4P flags
-    """
-    if default_SEOBNRv4P_kwargs:
-        mode_array, seob_flags = _setup_SEOBNRv4P_args()
-        mass_function_args += [mode_array, seob_flags]
-        spin_function_args += [mode_array, seob_flags]
-    fm = mass_function(*mass_function_args)
-    if mass_function_return_function is not None:
-        fm = eval("fm{}".format(mass_function_return_function))
-    elif mass_function_return_index is not None:
-        fm = fm[mass_function_return_index]
-    fs = spin_function(*spin_function_args)
-    if spin_function_return_function is not None:
-        fs = eval("fs{}".format(spin_function_return_function))
-    elif spin_function_return_index is not None:
-        fs = fs[spin_function_return_index]
-    final_mass = fm * (
-        mass_function_args[mass_1_index] + mass_function_args[mass_2_index]
-    ) / MSUN_SI
-    final_spin = fs
-    return final_mass, final_spin
-
-
-def _setup_SEOBNRv4P_args(mode=[2, 2], seob_flags=DEFAULT_SEOBFLAGS):
-    """Setup the SEOBNRv4P[HM] kwargs
-    """
-    from lalsimulation import (
-        SimInspiralCreateModeArray, SimInspiralModeArrayActivateMode
-    )
-    from lal import DictInsertINT4Value, CreateDict
-
-    mode_array = SimInspiralCreateModeArray()
-    SimInspiralModeArrayActivateMode(mode_array, mode[0], mode[1])
-    _seob_flags = CreateDict()
-    for key, item in seob_flags.items():
-        DictInsertINT4Value(_seob_flags, key, item)
-    return mode_array, _seob_flags
-
-
-@array_input
-def _final_from_initial(
-    mass_1, mass_2, spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z,
-    approximant="SEOBNRv4", iota=None, luminosity_distance=None, f_ref=None,
-    phi_ref=None, mode=[2, 2], delta_t=1. / 4096, seob_flags=DEFAULT_SEOBFLAGS,
-    return_fits_used=False, multi_process=None
-):
-    """Calculate the final mass and final spin given the initial parameters
-    of the binary using the approximant directly
-
-    Parameters
-    ----------
-    mass_1: float/np.ndarray
-        primary mass of the binary
-    mass_2: float/np.ndarray
-        secondary mass of the binary
-    spin_1x: float/np.ndarray
-        x component of the primary spin
-    spin_1y: float/np.ndarray
-        y component of the primary spin
-    spin_1z: float/np.ndarray
-        z component of the primary spin
-    spin_2x: float/np.ndarray
-        x component of the secondary spin
-    spin_2y: float/np.ndarray
-        y component of the secondary spin
-    spin_2z: float/np.ndarray
-        z component of the seconday spin
-    approximant: str
-        name of the approximant you wish to use for the remnant fits
-    iota: float/np.ndarray, optional
-        the angle between the total orbital angular momentum and the line of
-        sight of the source. Used when calculating the remnant fits for
-        SEOBNRv4PHM. Since we only need the EOB dynamics here it does not matter
-        what we pass
-    luminosity_distance: float/np.ndarray, optional
-        the luminosity distance of the source. Used when calculating the
-        remnant fits for SEOBNRv4PHM. Since we only need the EOB dynamics here
-        it does not matter what we pass.
-    f_ref: float/np.ndarray, optional
-        the reference frequency at which the spins are defined
-    phi_ref: float/np.ndarray, optional
-        the coalescence phase of the binary
-    mode: list, optional
-        specific mode to use when calculating the remnant fits for SEOBNRv4PHM.
-        Since we only need the EOB dynamics here it does not matter what we
-        pass.
-    delta_t: float, optional
-        the sampling rate used in the analysis, Used when calculating the
-        remnant fits for SEOBNRv4PHM
-    seob_flags: dict, optional
-        dictionary containing the SEOB flags. Used when calculating the remnant
-        fits for SEOBNRv4PHM
-    return_fits_used: Bool, optional
-        if True, return the approximant that was used.
-    multi_process: int, optional
-        the number of cores to use when calculating the remnant fits
-    """
-    from lalsimulation import (
-        SimIMREOBFinalMassSpin, SimIMREOBFinalMassSpinPrec,
-        SimInspiralGetSpinSupportFromApproximant,
-        SimIMRSpinPrecEOBWaveformAll, SimPhenomUtilsIMRPhenomDFinalMass,
-        SimPhenomUtilsPhenomPv2FinalSpin
-    )
-    from pesummary.utils.utils import iterator
-    import multiprocessing
-
-    def convert_args_for_multi_processing(kwargs):
-        args = []
-        for n in range(kwargs["nsamples"]):
-            _args = []
-            for key, item in kwargs.items():
-                if key == "mass_function_args" or key == "spin_function_args":
-                    _args.append([key, [arg[n] for arg in item]])
-                else:
-                    _args.append([key, item])
-            args.append(_args)
-        return args
-
-    try:
-        approx = getattr(lalsimulation, approximant)
-    except AttributeError:
-        raise ValueError(
-            "The waveform '{}' is not supported by lalsimulation"
-        )
-
-    m1 = mass_1 * MSUN_SI
-    m2 = mass_2 * MSUN_SI
-    kwargs = {"nsamples": len(mass_1), "approximant": approximant}
-    if approximant.lower() in ["seobnrv4p", "seobnrv4phm"]:
-        if any(i is None for i in [iota, luminosity_distance, f_ref, phi_ref]):
-            raise ValueError(
-                "The approximant '{}' requires samples for iota, f_ref, "
-                "phi_ref and luminosity_distance. Please pass these "
-                "samples.".format(approximant)
-            )
-        if len(delta_t) == 1:
-            delta_t = [delta_t[0]] * len(mass_1)
-        elif len(delta_t) != len(mass_1):
-            raise ValueError(
-                "Please provide either a single 'delta_t' that is is used for "
-                "all samples, or a single 'delta_t' for each sample"
-            )
-        mode_array, _seob_flags = _setup_SEOBNRv4P_args(
-            mode=mode, seob_flags=seob_flags
-        )
-        args = np.array([
-            phi_ref, delta_t, m1, m2, f_ref, luminosity_distance, iota,
-            spin_1x, spin_1y, spin_1z, spin_2x, spin_2y, spin_2z,
-            [mode_array] * len(mass_1), [_seob_flags] * len(mass_1)
-        ])
-        kwargs.update(
-            {
-                "mass_function": SimIMRSpinPrecEOBWaveformAll,
-                "spin_function": SimIMRSpinPrecEOBWaveformAll,
-                "mass_function_args": args,
-                "spin_function_args": args,
-                "mass_function_return_function": "[21].data[6]",
-                "spin_function_return_function": "[21].data[7]",
-                "mass_1_index": 2,
-                "mass_2_index": 3,
-            }
-        )
-    elif approximant.lower() in ["seobnrv4"]:
-        spin1 = np.array([spin_1x, spin_1y, spin_1z]).T
-        spin2 = np.array([spin_2x, spin_2y, spin_2z]).T
-        app = np.array([approx] * len(mass_1))
-        kwargs.update(
-            {
-                "mass_function": SimIMREOBFinalMassSpin,
-                "spin_function": SimIMREOBFinalMassSpin,
-                "mass_function_args": [m1, m2, spin1, spin2, app],
-                "spin_function_args": [m1, m2, spin1, spin2, app],
-                "mass_function_return_index": 1,
-                "spin_function_return_index": 2
-            }
-        )
-    elif "phenompv3" in approximant.lower():
-        kwargs.update(
-            {
-                "mass_function": SimPhenomUtilsIMRPhenomDFinalMass,
-                "spin_function": SimPhenomUtilsPhenomPv2FinalSpin,
-                "mass_function_args": [m1, m2, spin_1z, spin_2z],
-                "spin_function_args": [m1, m2, spin_1z, spin_2z]
-            }
-        )
-        if SimInspiralGetSpinSupportFromApproximant(approx) > 2:
-            # matches the waveform's internal usage as corrected in
-            # https://git.ligo.org/lscsoft/lalsuite/-/merge_requests/1270
-            _chi_p = chi_p(mass_1, mass_2, spin_1x, spin_1y, spin_2x, spin_2y)
-            kwargs["spin_function_args"].append(_chi_p)
-        else:
-            kwargs["spin_function_args"].append(np.zeros_like(mass_1))
-    else:
-        raise ValueError(
-            "The waveform '{}' is not support by this function.".format(
-                approximant
-            )
-        )
-
-    args = convert_args_for_multi_processing(kwargs)
-    if multi_process is not None and multi_process[0] != 1:
-        _multi_process = multi_process[0]
-        if approximant.lower() in ["seobnrv4p", "seobnrv4phm"]:
-            logger.warning(
-                "Ignoring passed 'mode' and 'seob_flags' options. Defaults "
-                "must be used with multiprocessing. If you wish to use custom "
-                "options, please set `multi_process=None`"
-            )
-            _kwargs = kwargs.copy()
-            _kwargs["mass_function_args"] = kwargs["mass_function_args"][:-2]
-            _kwargs["spin_function_args"] = kwargs["spin_function_args"][:-2]
-            _kwargs["default_SEOBNRv4P_kwargs"] = True
-            args = convert_args_for_multi_processing(_kwargs)
-        with multiprocessing.Pool(_multi_process) as pool:
-            data = np.array(list(
-                iterator(
-                    pool.imap(
-                        _wrapper_return_final_mass_and_final_spin_from_waveform,
-                        args
-                    ), tqdm=True, desc="Evaluating {} fit".format(approximant),
-                    logger=logger, total=len(mass_1)
-                )
-            )).T
-    else:
-        final_mass, final_spin = [], []
-        _iterator = iterator(
-            range(kwargs["nsamples"]), tqdm=True, total=len(mass_1),
-            desc="Evaluating {} fit".format(approximant), logger=logger
-        )
-        for i in _iterator:
-            data = _wrapper_return_final_mass_and_final_spin_from_waveform(
-                args[i]
-            )
-            final_mass.append(data[0])
-            final_spin.append(data[1])
-        data = [final_mass, final_spin]
-    if return_fits_used:
-        return data, [approximant]
-    return data
-
-
-def final_remnant_properties_from_NRSurrogate(
-    *args, f_low=20., f_ref=20., model="NRSur7dq4Remnant", return_fits_used=False,
-    properties=["final_mass", "final_spin", "final_kick"], approximant="SEOBNRv4PHM"
-):
-    """Return the properties of the final remnant resulting from a BBH merger using
-    NRSurrogate fits
-
-    Parameters
-    ---------
-    f_low: float/np.ndarray
-        The low frequency cut-off used in the analysis. Default is 20Hz
-    f_ref: float/np.ndarray
-        The reference frequency used in the analysis. Default is 20Hz
-    model: str, optional
-        The name of the NRSurrogate model you wish to use
-    return_fits_used: Bool, optional
-        if True, return the approximant that was used.
-    properties: list, optional
-        The list of properties you wish to calculate
-    approximant: str, optional
-        The approximant that was used to generate the posterior samples
-    """
-    from pesummary.gw.file.nrutils import NRSur_fit
-
-    fit = NRSur_fit(
-        *args, f_low=f_low, f_ref=f_ref, model=model, fits=properties,
-        approximant=approximant
-    )
-    if return_fits_used:
-        return fit, [model]
-    return fit
-
-
-def final_mass_of_merger_from_NR(
-    *args, NRfit="average", final_spin=None, return_fits_used=False
-):
-    """Return the final mass resulting from a BBH merger using NR fits
-
-    Parameters
-    ----------
-    NRfit: str
-        Name of the fit you wish to use. If you wish to use a precessing fit
-        please use the syntax 'precessing_{}'.format(fit_name). If you wish
-        to have an average NR fit, then pass 'average'
-    final_spin: float/np.ndarray, optional
-        precomputed final spin of the remnant.
-    return_fits_used: Bool, optional
-        if True, return the fits that were used. Only used when NRfit='average'
-    """
-    from pesummary.gw.file import nrutils
-
-    if NRfit.lower() == "average":
-        func = getattr(nrutils, "bbh_final_mass_average")
-    elif "panetal" in NRfit.lower():
-        func = getattr(
-            nrutils, "bbh_final_mass_non_spinning_Panetal"
-        )
-    else:
-        func = getattr(
-            nrutils, "bbh_final_mass_non_precessing_{}".format(NRfit)
-        )
-    if "healy" in NRfit.lower():
-        return func(*args, final_spin=final_spin)
-    if NRfit.lower() == "average":
-        return func(*args, return_fits_used=return_fits_used)
-    return func(*args)
-
-
-def final_mass_of_merger_from_NRSurrogate(
-    *args, model="NRSur7dq4Remnant", return_fits_used=False, approximant="SEOBNRv4PHM"
-):
-    """Return the final mass resulting from a BBH merger using NRSurrogate
-    fits
-    """
-    data = final_remnant_properties_from_NRSurrogate(
-        *args, model=model, properties=["final_mass"],
-        return_fits_used=return_fits_used,
-        approximant=approximant
-    )
-    if return_fits_used:
-        return data[0]["final_mass"], data[1]
-    return data["final_mass"]
-
-
-def final_mass_of_merger_from_waveform(*args, **kwargs):
-    """Return the final mass resulting from a BBH merger using a given
-    approximant
-    """
-    return _final_from_initial(*args, **kwargs)[0]
-
-
-def final_spin_of_merger_from_NR(
-    *args, NRfit="average", return_fits_used=False
-):
-    """Return the final spin resulting from a BBH merger using NR fits
-
-    Parameters
-    ----------
-    NRfit: str
-        Name of the fit you wish to use. If you wish to use a precessing fit
-        please use the syntax 'precessing_{}'.format(fit_name). If you wish
-        to have an average NR fit, then pass 'average'
-    return_fits_used: Bool, optional
-        if True, return the fits that were used. Only used when NRfit='average'
-    """
-    from pesummary.gw.file import nrutils
-
-    if NRfit.lower() == "average":
-        func = getattr(nrutils, "bbh_final_spin_average_precessing")
-    elif "pan" in NRfit.lower():
-        func = getattr(
-            nrutils, "bbh_final_spin_non_spinning_Panetal"
-        )
-    elif "precessing" in NRfit.lower():
-        func = getattr(
-            nrutils, "bbh_final_spin_precessing_{}".format(
-                NRfit.split("precessing_")[1]
-            )
-        )
-    else:
-        func = getattr(
-            nrutils, "bbh_final_spin_non_precessing_{}".format(NRfit)
-        )
-    if NRfit.lower() == "average":
-        return func(*args, return_fits_used=return_fits_used)
-    return func(*args)
-
-
-def final_spin_of_merger_from_NRSurrogate(
-    *args, model="NRSur7dq4Remnant", return_fits_used=False, approximant="SEOBNRv4PHM"
-):
-    """Return the final spin resulting from a BBH merger using NRSurrogate
-    fits
-    """
-    data = final_remnant_properties_from_NRSurrogate(
-        *args, model=model, properties=["final_spin"],
-        return_fits_used=return_fits_used, approximant=approximant
-    )
-    if return_fits_used:
-        return data[0]["final_spin"], data[1]
-    return data["final_spin"]
-
-
-def final_spin_of_merger_from_waveform(*args, **kwargs):
-    """Return the final spin resulting from a BBH merger using a given
-    approximant
-    """
-    return _final_from_initial(*args, **kwargs)[1]
-
-
-def final_kick_of_merger_from_NRSurrogate(
-    *args, model="NRSur7dq4Remnant", return_fits_used=False, approximant="SEOBNRv4PHM"
-):
-    """Return the final kick of the remnant resulting from a BBH merger
-    using NRSurrogate fits
-    """
-    data = final_remnant_properties_from_NRSurrogate(
-        *args, model=model, properties=["final_kick"],
-        return_fits_used=return_fits_used, approximant=approximant
-    )
-    if return_fits_used:
-        return data[0]["final_kick"], data[1]
-    return data["final_kick"]
-
-
-def final_mass_of_merger(
-    *args, method="NR", approximant="SEOBNRv4", NRfit="average",
-    final_spin=None, return_fits_used=False, model="NRSur7dq4Remnant"
-):
-    """Return the final mass resulting from a BBH merger
-
-    Parameters
-    ----------
-    mass_1: float/np.ndarray
-        float/array of masses for the primary object
-    mass_2: float/np.ndarray
-        float/array of masses for the secondary object
-    spin_1z: float/np.ndarray
-        float/array of primary spin aligned with the orbital angular momentum
-    spin_2z: float/np.ndarray
-        float/array of secondary spin aligned with the orbital angular momentum
-    method: str
-        The method you wish to use to calculate the final mass of merger. Either
-        NR, NRSurrogate or waveform
-    approximant: str
-        Name of the approximant you wish to use if the chosen method is waveform
-        or NRSurrogate
-    NRFit: str
-        Name of the NR fit you wish to use if chosen method is NR
-    return_fits_used: Bool, optional
-        if True, return the NR fits that were used. Only used when
-        NRFit='average' or when method='NRSurrogate'
-    model: str, optional
-        The NRSurrogate model to use when evaluating the fits
-    """
-    if method.lower() == "nr":
-        mass_func = final_mass_of_merger_from_NR
-        kwargs = {
-            "NRfit": NRfit, "final_spin": final_spin,
-            "return_fits_used": return_fits_used
-        }
-    elif "nrsur" in method.lower():
-        mass_func = final_mass_of_merger_from_NRSurrogate
-        kwargs = {
-            "approximant": approximant, "return_fits_used": return_fits_used,
-            "model": model
-        }
-    else:
-        mass_func = final_mass_of_merger_from_waveform
-        kwargs = {"approximant": approximant}
-
-    return mass_func(*args, **kwargs)
-
-
-def final_spin_of_merger(
-    *args, method="NR", approximant="SEOBNRv4", NRfit="average",
-    return_fits_used: False, model="NRSur7dq4Remnant"
-):
-    """Return the final mass resulting from a BBH merger
-
-    Parameters
-    ----------
-    mass_1: float/np.ndarray
-        float/array of masses for the primary object
-    mass_2: float/np.ndarray
-        float/array of masses for the secondary object
-    a_1: float/np.ndarray
-        float/array of primary spin magnitudes
-    a_2: float/np.ndarray
-        float/array of secondary spin magnitudes
-    tilt_1: float/np.ndarray
-        float/array of primary spin tilt angle from the orbital angular momentum
-    tilt_2: float/np.ndarray
-        float/array of secondary spin tilt angle from the orbital angular
-        momentum
-    phi_12: float/np.ndarray
-        float/array of samples for the angle between the in-plane spin
-        components
-    method: str
-        The method you wish to use to calculate the final mass of merger. Either
-        NR, NRSurrogate or waveform
-    approximant: str
-        Name of the approximant you wish to use if the chosen method is waveform
-        or NRSurrogate
-    NRFit: str
-        Name of the NR fit you wish to use if chosen method is NR
-    return_fits_used: Bool, optional
-        if True, return the NR fits that were used. Only used when
-        NRFit='average' or when method='NRSurrogate'
-    model: str, optional
-        The NRSurrogate model to use when evaluating the fits
-    """
-    if method.lower() == "nr":
-        spin_func = final_spin_of_merger_from_NR
-        kwargs = {"NRfit": NRfit, "return_fits_used": return_fits_used}
-    elif "nrsur" in method.lower():
-        spin_func = final_spin_of_merger_from_NRSurrogate
-        kwargs = {
-            "approximant": approximant, "return_fits_used": return_fits_used,
-            "model": model
-        }
-    else:
-        spin_func = final_spin_of_merger_from_waveform
-        kwargs = {"approximant": approximant}
-
-    return spin_func(*args, **kwargs)
-
-
-def final_kick_of_merger(
-    *args, method="NR", approximant="SEOBNRv4", NRfit="average",
-    return_fits_used: False, model="NRSur7dq4Remnant"
-):
-    """Return the final kick velocity of the remnant resulting from a BBH merger
-
-    Parameters
-    ----------
-    mass_1: float/np.ndarray
-        float/array of masses for the primary object
-    mass_2: float/np.ndarray
-        float/array of masses for the secondary object
-    a_1: float/np.ndarray
-        float/array of primary spin magnitudes
-    a_2: float/np.ndarray
-        float/array of secondary spin magnitudes
-    tilt_1: float/np.ndarray
-        float/array of primary spin tilt angle from the orbital angular momentum
-    tilt_2: float/np.ndarray
-        float/array of secondary spin tilt angle from the orbital angular
-        momentum
-    phi_12: float/np.ndarray
-        float/array of samples for the angle between the in-plane spin
-        components
-    method: str
-        The method you wish to use to calculate the final kick of merger. Either
-        NR, NRSurrogate or waveform
-    approximant: str
-        Name of the approximant you wish to use if the chosen method is waveform
-        or NRSurrogate
-    NRFit: str
-        Name of the NR fit you wish to use if chosen method is NR
-    return_fits_used: Bool, optional
-        if True, return the NR fits that were used. Only used when
-        NRFit='average' or when method='NRSurrogate'
-    model: str, optional
-        The NRSurrogate model to use when evaluating the fits
-    """
-    if "nrsur" not in method.lower():
-        raise NotImplementedError(
-            "Currently you can only work out the final kick velocity using "
-            "NRSurrogate fits."
-        )
-    velocity_func = final_kick_of_merger_from_NRSurrogate
-    kwargs = {
-        "approximant": approximant, "return_fits_used": return_fits_used,
-        "model": model
-    }
-    return velocity_func(*args, **kwargs)
-
-
-def peak_luminosity_of_merger(*args, NRfit="average", return_fits_used=False):
-    """Return the peak luminosity of an aligned-spin BBH using NR fits
-
-    Parameters
-    ----------
-    mass_1: float/np.ndarray
-        float/array of masses for the primary object
-    mass_2: float/np.ndarray
-        float/array of masses for the secondary object
-    spin_1z: float/np.ndarray
-        float/array of primary spin aligned with the orbital angular momentum
-    spin_2z: float/np.ndarray
-        float/array of secondary spin aligned with the orbital angular momentum
-    NRFit: str
-        Name of the NR fit you wish to use if chosen method is NR
-    return_fits_used: Bool, optional
-        if True, return the NR fits that were used. Only used when
-        NRFit='average'
-    """
-    from pesummary.gw.file import nrutils
-
-    if NRfit.lower() == "average":
-        func = getattr(nrutils, "bbh_peak_luminosity_average")
-    else:
-        func = getattr(
-            nrutils, "bbh_peak_luminosity_non_precessing_{}".format(NRfit)
-        )
-    if NRfit.lower() == "average":
-        return func(*args, return_fits_used=return_fits_used)
-    return func(*args)
+    logger.warning(error_msg.format("astropy"))
 
-
-def magnitude_from_vector(vector):
-    """Return the magnitude of a vector
-
-    Parameters
-    ----------
-    vector: list, np.ndarray
-        The vector you wish to return the magnitude for.
-    """
-    vector = np.atleast_2d(vector)
-    return np.linalg.norm(vector, axis=1)
-
-
-class _Redshift(object):
-    exact = z_from_dL_exact
-    approx = z_from_dL_approx
-
-
-class _Conversion(object):
-    """Class to calculate all possible derived quantities
+from .angles import *
+from .cosmology import *
+from .cosmology import _source_from_detector
+from .mass import *
+from .remnant import *
+from .remnant import _final_from_initial_BBH
+from .snr import *
+from .snr import _ifo_snr
+from .spins import *
+from .tidal import *
+from .tidal import _check_NSBH_approximant
+from .time import *
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+_conversion_doc = """
+    Class to calculate all possible derived quantities
 
     Parameters
     ----------
     data: dict, list
         either a dictionary or samples or a list of parameters and a list of
         samples. See the examples below for details
     extra_kwargs: dict, optional
         dictionary of kwargs associated with this set of posterior samples.
     f_low: float, optional
         the low frequency cut-off to use when evolving the spins
     f_ref: float, optional
         the reference frequency when spins are defined
+    f_final: float, optional
+        the final frequency to use when integrating over frequencies
     approximant: str, optional
         the approximant to use when evolving the spins
-    evolve_spins: float/str, optional
+    evolve_spins_forwards: float/str, optional
         the final velocity to evolve the spins up to.
+    evolve_spins_backwards: str, optional
+        method to use when evolving the spins backwards to an infinite separation
     return_kwargs: Bool, optional
         if True, return a modified dictionary of kwargs containing information
         about the conversion
     NRSur_fits: float/str, optional
         the NRSurrogate model to use to calculate the remnant fits. If nothing
         passed, the average NR fits are used instead
+    multipole_snr: Bool, optional
+        if True, the SNR in the (l, m) = [(2, 1), (3, 3), (4, 4)] multipoles are
+        calculated from the posterior samples.
+        samples.
+    precessing_snr: Bool, optional
+        if True, the precessing SNR is calculated from the posterior samples.
+    psd: dict, optional
+        dictionary containing a psd frequency series for each detector you wish
+        to include in calculations
     waveform_fits: Bool, optional
         if True, the approximant is used to calculate the remnant fits. Default
         is False which means that the average NR fits are used
     multi_process: int, optional
         number of cores to use to parallelize the computationally expensive
         conversions
     redshift_method: str, optional
@@ -1424,135 +86,282 @@
         the redshift given N luminosity distance points.
     cosmology: str, optional
         cosmology you wish to use when calculating the redshift given luminosity
         distance samples.
     force_non_evolved: Bool, optional
         force non evolved remnant quantities to be calculated when evolved quantities
         already exist in the input. Default False
-    force_remnant_computation: Bool, optional
-        force remnant quantities to be calculated for systems that include
+    force_BBH_remnant_computation: Bool, optional
+        force BBH remnant quantities to be calculated for systems that include
         tidal deformability parameters where BBH fits may not be applicable.
         Default False.
+    force_BH_spin_evolution: Bool, optional
+        force BH spin evolution methods to be applied for systems that include
+        tidal deformability parameters where these methods may not be applicable.
+        Default False.
+    disable_remnant: Bool, optional
+        disable all remnant quantities from being calculated. Default False.
     add_zero_spin: Bool, optional
         if no spins are present in the posterior table, add spins with 0 value.
         Default False.
+    psd_default: str/pycbc.psd obj, optional
+        Default PSD to use for conversions when no other PSD is provided.
     regenerate: list, optional
         list of posterior distributions that you wish to regenerate
     return_dict: Bool, optional
         if True, return a pesummary.utils.utils.SamplesDict object
+    resume_file: str, optional
+        path to file to use for checkpointing. If not provided, checkpointing
+        is not used. Default None
 
     Examples
     --------
     There are two ways of passing arguments to this conversion class, either
     a dictionary of samples or a list of parameters and a list of samples. See
     the examples below:
 
     >>> samples = {"mass_1": 10, "mass_2": 5}
-    >>> converted_samples = _Conversion(samples)
+    >>> converted_samples = %(function)s(samples)
 
     >>> parameters = ["mass_1", "mass_2"]
     >>> samples = [10, 5]
-    >>> converted_samples = _Conversion(parameters, samples)
+    >>> converted_samples = %(function)s(parameters, samples)
 
     >>> samples = {"mass_1": [10, 20], "mass_2": [5, 8]}
-    >>> converted_samples = _Conversion(samples)
+    >>> converted_samples = %(function)s(samples)
 
     >>> parameters = ["mass_1", "mass_2"]
     >>> samples = [[10, 5], [20, 8]]
     """
+
+
+@set_docstring(_conversion_doc % {"function": "convert"})
+def convert(*args, restart_from_checkpoint=False, resume_file=None, **kwargs):
+    import os
+    if resume_file is not None:
+        if os.path.isfile(resume_file) and restart_from_checkpoint:
+            return _Conversion.load_current_state(resume_file)
+        logger.info(
+            "Unable to find resume file for conversion. Not restarting from "
+            "checkpoint"
+        )
+    return _Conversion(*args, resume_file=resume_file, **kwargs)
+
+
+class _PickledConversion(object):
+    pass
+
+
+@set_docstring(_conversion_doc % {"function": "_Conversion"})
+class _Conversion(object):
+    @classmethod
+    def load_current_state(cls, resume_file):
+        """Load current state from a resume file
+
+        Parameters
+        ----------
+        resume_file: str
+            path to a resume file to restart conversion
+        """
+        from pesummary.io import read
+        logger.info(
+            "Reading checkpoint file: {}".format(resume_file)
+        )
+        state = read(resume_file, checkpoint=True)
+        return cls(
+            state.parameters, state.samples, extra_kwargs=state.extra_kwargs,
+            evolve_spins_forwards=state.evolve_spins_forwards,
+            evolve_spins_backwards=state.evolve_spins_backwards,
+            NRSur_fits=state.NRSurrogate,
+            waveform_fits=state.waveform_fit, multi_process=state.multi_process,
+            redshift_method=state.redshift_method, cosmology=state.cosmology,
+            force_non_evolved=state.force_non_evolved,
+            force_BBH_remnant_computation=state.force_remnant,
+            disable_remnant=state.disable_remnant,
+            add_zero_spin=state.add_zero_spin, regenerate=state.regenerate,
+            return_kwargs=state.return_kwargs, return_dict=state.return_dict,
+            resume_file=state.resume_file
+        )
+
+    def write_current_state(self):
+        """Write the current state of the conversion class to file
+        """
+        from pesummary.io import write
+        state = _PickledConversion()
+        for key, value in vars(self).items():
+            setattr(state, key, value)
+
+        _path = Path(self.resume_file)
+        write(
+            state, outdir=_path.parent, file_format="pickle",
+            filename=_path.name, overwrite=True
+        )
+        logger.debug(
+            "Written checkpoint file: {}".format(self.resume_file)
+        )
+
     def __new__(cls, *args, **kwargs):
         from pesummary.utils.samples_dict import SamplesDict
+        from pesummary.utils.parameters import Parameters
 
         obj = super(_Conversion, cls).__new__(cls)
         base_replace = (
             "'{}': {} already found in the result file. Overwriting with "
             "the passed {}"
         )
         if len(args) > 2:
             raise ValueError(
                 "The _Conversion module only takes as arguments a dictionary "
                 "of samples or a list of parameters and a list of samples"
             )
         elif isinstance(args[0], dict):
-            parameters = list(args[0].keys())
+            parameters = Parameters(args[0].keys())
             samples = np.atleast_2d(
                 np.array([args[0][i] for i in parameters]).T
             ).tolist()
         else:
-            parameters, samples = args
+            if not isinstance(args[0], Parameters):
+                parameters = Parameters(args[0])
+            else:
+                parameters = args[0]
+            samples = args[1]
             samples = np.atleast_2d(samples).tolist()
         extra_kwargs = kwargs.get("extra_kwargs", {"sampler": {}, "meta_data": {}})
         f_low = kwargs.get("f_low", None)
         f_ref = kwargs.get("f_ref", None)
+        f_final = kwargs.get("f_final", None)
+        delta_f = kwargs.get("delta_f", None)
+
+        for param, value in {"f_final": f_final, "delta_f": delta_f}.items():
+            if value is not None and param in extra_kwargs["meta_data"].keys():
+                logger.warning(
+                    base_replace.format(
+                        param, extra_kwargs["meta_data"][param], value
+                    )
+                )
+                extra_kwargs["meta_data"][param] = value
+            elif value is not None:
+                extra_kwargs["meta_data"][param] = value
+            else:
+                logger.warning(
+                    "Could not find {} in input file and one was not passed "
+                    "from the command line. Using {}Hz as default".format(
+                        param, getattr(conf, "default_{}".format(param))
+                    )
+                )
+                extra_kwargs["meta_data"][param] = getattr(
+                    conf, "default_{}".format(param)
+                )
+
         approximant = kwargs.get("approximant", None)
         NRSurrogate = kwargs.get("NRSur_fits", False)
         redshift_method = kwargs.get("redshift_method", "approx")
         cosmology = kwargs.get("cosmology", "Planck15")
         force_non_evolved = kwargs.get("force_non_evolved", False)
-        force_remnant = kwargs.get("force_remnant_computation", False)
+        force_remnant = kwargs.get("force_BBH_remnant_computation", False)
+        force_evolve = kwargs.get("force_BH_spin_evolution", False)
+        disable_remnant = kwargs.get("disable_remnant", False)
         if redshift_method not in ["approx", "exact"]:
             raise ValueError(
                 "'redshift_method' can either be 'approx' corresponding to "
                 "an approximant method, or 'exact' corresponding to an exact "
                 "method of calculating the redshift"
             )
         if isinstance(NRSurrogate, bool) and NRSurrogate:
             raise ValueError(
                 "'NRSur_fits' must be a string corresponding to the "
                 "NRSurrogate model you wish to use to calculate the remnant "
                 "quantities"
             )
         waveform_fits = kwargs.get("waveform_fits", False)
+        evolve_spins_forwards = kwargs.get("evolve_spins_forwards", False)
+        evolve_spins_backwards = kwargs.get("evolve_spins_backwards", False)
+        if disable_remnant and (
+                force_non_evolved or force_remnant
+                or NRSurrogate or waveform_fits or evolve_spins_forwards
+        ):
+            _disable = []
+            if force_non_evolved:
+                _disable.append("force_non_evolved")
+                force_non_evolved = False
+            if force_remnant:
+                _disable.append("force_BBH_remnant_computation")
+                force_remnant = False
+            if NRSurrogate:
+                _disable.append("NRSur_fits")
+                NRSurrogate = False
+            if waveform_fits:
+                _disable.append("waveform_fits")
+                waveform_fits = False
+            if evolve_spins_forwards:
+                _disable.append("evolve_spins_forwards")
+                evolve_spins_forwards = False
+            logger.warning(
+                "Unable to use 'disable_remnant' and {}. Setting "
+                "{} and disabling all remnant quantities from being "
+                "calculated".format(
+                    " or ".join(_disable),
+                    " and ".join(["{}=False".format(_p) for _p in _disable])
+                )
+            )
         if NRSurrogate and waveform_fits:
             raise ValueError(
                 "Unable to use both the NRSurrogate and {} to calculate "
                 "remnant quantities. Please select only one option".format(
                     approximant
                 )
             )
-        evolve_spins = kwargs.get("evolve_spins", False)
-        if isinstance(evolve_spins, bool) and evolve_spins:
+        if isinstance(evolve_spins_forwards, bool) and evolve_spins_forwards:
             raise ValueError(
-                "'evolve_spins' must be a float, the final velocity to "
+                "'evolve_spins_forwards' must be a float, the final velocity to "
                 "evolve the spins up to, or a string, 'ISCO', meaning "
                 "evolve the spins up to the ISCO frequency"
             )
-        if not evolve_spins and (NRSurrogate or waveform_fits):
-            if "eob" in approximant or NRSurrogate:
+        if not evolve_spins_forwards and (NRSurrogate or waveform_fits):
+            if (approximant is not None and "eob" in approximant) or NRSurrogate:
                 logger.warning(
                     "Only evolved spin remnant quantities are returned by the "
                     "{} fits.".format(
                         "NRSurrogate" if NRSurrogate else approximant
                     )
                 )
-        elif evolve_spins and (NRSurrogate or waveform_fits):
-            if "eob" in approximant or NRSurrogate:
+        elif evolve_spins_forwards and (NRSurrogate or waveform_fits):
+            if (approximant is not None and "eob" in approximant) or NRSurrogate:
                 logger.warning(
                     "The {} fits already evolve the spins. Therefore "
                     "additional spin evolution will not be performed.".format(
                         "NRSurrogate" if NRSurrogate else approximant
                     )
                 )
             else:
                 logger.warning(
                     "The {} fits are not applied with spin evolution.".format(
                         approximant
                     )
                 )
-            evolve_spins = False
+            evolve_spins_forwards = False
 
+        multipole_snr = kwargs.get("multipole_snr", False)
+        precessing_snr = kwargs.get("precessing_snr", False)
         if f_low is not None and "f_low" in extra_kwargs["meta_data"].keys():
             logger.warning(
                 base_replace.format(
                     "f_low", extra_kwargs["meta_data"]["f_low"], f_low
                 )
             )
             extra_kwargs["meta_data"]["f_low"] = f_low
         elif f_low is not None:
             extra_kwargs["meta_data"]["f_low"] = f_low
+        else:
+            logger.warning(
+                "Could not find minimum frequency in input file and "
+                "one was not passed from the command line. Using {}Hz "
+                "as default".format(conf.default_flow)
+            )
+            extra_kwargs["meta_data"]["f_low"] = conf.default_flow
         if approximant is not None and "approximant" in extra_kwargs["meta_data"].keys():
             logger.warning(
                 base_replace.format(
                     "approximant", extra_kwargs["meta_data"]["approximant"],
                     approximant
                 )
             )
@@ -1568,19 +377,44 @@
             extra_kwargs["meta_data"]["f_ref"] = f_ref
         elif f_ref is not None:
             extra_kwargs["meta_data"]["f_ref"] = f_ref
         regenerate = kwargs.get("regenerate", None)
         multi_process = kwargs.get("multi_process", None)
         if multi_process is not None:
             multi_process = int(multi_process)
+        psd_default = kwargs.get("psd_default", "aLIGOZeroDetHighPower")
+        psd = kwargs.get("psd", {})
+        if psd is None:
+            psd = {}
+        elif psd is not None and not isinstance(psd, dict):
+            raise ValueError(
+                "'psd' must be a dictionary of frequency series for each detector"
+            )
+        ifos = list(psd.keys())
+        pycbc_psd = copy.deepcopy(psd)
+        if psd != {}:
+            from pesummary.gw.file.psd import PSD
+            if isinstance(psd[ifos[0]], PSD):
+                for ifo in ifos:
+                    try:
+                        pycbc_psd[ifo] = pycbc_psd[ifo].to_pycbc(
+                            extra_kwargs["meta_data"]["f_low"],
+                            f_high=extra_kwargs["meta_data"]["f_final"],
+                            f_high_override=True
+                        )
+                    except (ImportError, IndexError, ValueError):
+                        pass
         obj.__init__(
-            parameters, samples, extra_kwargs, evolve_spins, NRSurrogate,
+            parameters, samples, extra_kwargs, evolve_spins_forwards, NRSurrogate,
             waveform_fits, multi_process, regenerate, redshift_method,
             cosmology, force_non_evolved, force_remnant,
-            kwargs.get("add_zero_spin", False)
+            kwargs.get("add_zero_spin", False), disable_remnant,
+            kwargs.get("return_kwargs", False), kwargs.get("return_dict", True),
+            kwargs.get("resume_file", None), multipole_snr, precessing_snr,
+            pycbc_psd, psd_default, evolve_spins_backwards, force_evolve
         )
         return_kwargs = kwargs.get("return_kwargs", False)
         if kwargs.get("return_dict", True) and return_kwargs:
             return [
                 SamplesDict(obj.parameters, np.array(obj.samples).T),
                 obj.extra_kwargs
             ]
@@ -1588,72 +422,188 @@
             return SamplesDict(obj.parameters, np.array(obj.samples).T)
         elif return_kwargs:
             return obj.parameters, obj.samples, obj.extra_kwargs
         else:
             return obj.parameters, obj.samples
 
     def __init__(
-        self, parameters, samples, extra_kwargs, evolve_spins, NRSurrogate,
+        self, parameters, samples, extra_kwargs, evolve_spins_forwards, NRSurrogate,
         waveform_fits, multi_process, regenerate, redshift_method,
-        cosmology, force_non_evolved, force_remnant, add_zero_spin
+        cosmology, force_non_evolved, force_remnant, add_zero_spin,
+        disable_remnant, return_kwargs, return_dict, resume_file, multipole_snr,
+        precessing_snr, psd, psd_default, evolve_spins_backwards, force_evolve
     ):
         self.parameters = parameters
         self.samples = samples
         self.extra_kwargs = extra_kwargs
+        self.evolve_spins_forwards = evolve_spins_forwards
+        self.evolve_spins_backwards = evolve_spins_backwards
         self.NRSurrogate = NRSurrogate
         self.waveform_fit = waveform_fits
         self.multi_process = multi_process
         self.regenerate = regenerate
         self.redshift_method = redshift_method
         self.cosmology = cosmology
         self.force_non_evolved = force_non_evolved
+        self.force_remnant = force_remnant
+        self.force_evolve = force_evolve
+        self.disable_remnant = disable_remnant
+        self.return_kwargs = return_kwargs
+        self.return_dict = return_dict
+        self.resume_file = resume_file
+        self.multipole_snr = multipole_snr
+        self.precessing_snr = precessing_snr
+        self.psd = psd
+        self.psd_default = psd_default
         self.non_precessing = False
-        if not any(param in self.parameters for param in conf.precessing_angles):
+        cond1 = any(
+            param in self.parameters for param in
+            conf.precessing_angles + conf.precessing_spins
+        )
+        if not cond1:
             self.non_precessing = True
-        if self.non_precessing and evolve_spins:
+        if "chi_p" in self.parameters:
+            _chi_p = self.specific_parameter_samples(["chi_p"])
+            if not np.any(_chi_p):
+                logger.info(
+                    "chi_p = 0 for all samples. Treating this as a "
+                    "non-precessing system"
+                )
+                self.non_precessing = True
+        elif all(param in self.parameters for param in conf.precessing_spins):
+            samples = self.specific_parameter_samples(conf.precessing_spins)
+            if not any(np.array(samples).flatten()):
+                logger.info(
+                    "in-plane spins are zero for all samples. Treating this as "
+                    "a non-precessing system"
+                )
+        cond1 = self.non_precessing and evolve_spins_forwards
+        cond2 = self.non_precessing and evolve_spins_backwards
+        if cond1 or cond2:
             logger.info(
                 "Spin evolution is trivial for a non-precessing system. No additional "
                 "transformation required."
             )
-            evolve_spins = False
-        self.has_tidal = self._check_for_tidal_parameters()
-        self.compute_remnant = True
-        if force_remnant and self.has_tidal:
+            self.evolve_spins_forwards = False
+            self.evolve_spins_backwards = False
+        if not self.non_precessing and multipole_snr:
             logger.warning(
-                "Posterior samples for tidal deformability found in the "
-                "posterior table. Applying BBH remnant fits to this system. "
-                "This may not give sensible results."
-            )
-        elif self.has_tidal:
-            if evolve_spins:
-                msg = (
-                    "Not applying spin evolution as tidal parameters found "
-                    "in the posterior table."
-                )
-                logger.info(msg)
-            logger.debug(
-                "Skipping remnant calculations as tidal deformability "
-                "parameters found in the posterior table."
+                "The calculation for computing the SNR in subdominant "
+                "multipoles assumes that the system is non-precessing. "
+                "Since precessing samples are provided, this may give incorrect "
+                "results"
+            )
+        if self.non_precessing and precessing_snr:
+            logger.info(
+                "Precessing SNR is 0 for a non-precessing system. No additional "
+                "conversion required."
             )
-            self.compute_remnant = False
+            self.precessing_snr = False
+        self.has_tidal = self._check_for_tidal_parameters()
+        self.NSBH = self._check_for_NSBH_system()
+        self.compute_remnant = not self.disable_remnant
+        if self.has_tidal:
+            if force_evolve and (self.evolve_spins_forwards or self.evolve_spins_backwards):
+                logger.warning(
+                    "Posterior samples for tidal deformability found in the "
+                    "posterior table. 'force_evolve' provided so using BH spin "
+                    "evolution methods for this system. This may not give "
+                    "sensible results"
+                )
+            elif self.evolve_spins_forwards or self.evolve_spins_backwards:
+                logger.warning(
+                    "Tidal deformability parameters found in the posterior table. "
+                    "Skipping spin evolution as current methods are only valid "
+                    "for BHs."
+                )
+                self.evolve_spins_forwards = False
+                self.evolve_spins_backwards = False
+
+            if force_remnant and self.NSBH and self.compute_remnant:
+                logger.warning(
+                    "Posterior samples for lambda_2 found in the posterior table "
+                    "but unable to find samples for lambda_1. Assuming this "
+                    "is an NSBH system. 'force_remnant' provided so using BBH remnant "
+                    "fits for this system. This may not give sensible results"
+                )
+            elif self.NSBH and self.compute_remnant:
+                logger.warning(
+                    "Posterior samples for lambda_2 found in the posterior table "
+                    "but unable to find samples for lambda_1. Applying NSBH "
+                    "fits to this system."
+                )
+                self.waveform_fit = True
+            elif force_remnant and self.compute_remnant:
+                logger.warning(
+                    "Posterior samples for tidal deformability found in the "
+                    "posterior table. Applying BBH remnant fits to this system. "
+                    "This may not give sensible results."
+                )
+            elif self.compute_remnant:
+                logger.info(
+                    "Skipping remnant calculations as tidal deformability "
+                    "parameters found in the posterior table."
+                )
+                self.compute_remnant = False
         if self.regenerate is not None:
             for param in self.regenerate:
                 self.remove_posterior(param)
         self.add_zero_spin = add_zero_spin
-        self.generate_all_posterior_samples(evolve_spins=evolve_spins)
+        self.generate_all_posterior_samples()
 
     def _check_for_tidal_parameters(self):
         """Check to see if any tidal parameters are stored in the table
         """
         from pesummary.gw.file.standard_names import tidal_params
 
         if any(param in self.parameters for param in tidal_params):
+            if not all(_ in self.parameters for _ in ["lambda_1", "lambda_2"]):
+                return True
+            _tidal_posterior = self.specific_parameter_samples(
+                ["lambda_1", "lambda_2"]
+            )
+            if not all(np.any(_) for _ in _tidal_posterior):
+                logger.warning(
+                    "Tidal deformability parameters found in the posterior "
+                    "table but they are all exactly 0. Assuming this is a BBH "
+                    "system."
+                )
+                return False
             return True
         return False
 
+    def _check_for_NSBH_system(self):
+        """Check to see if the posterior samples correspond to an NSBH
+        system
+        """
+        if "lambda_2" in self.parameters and "lambda_1" not in self.parameters:
+            _lambda_2 = self.specific_parameter_samples(["lambda_2"])
+            if not np.any(_lambda_2):
+                logger.warning(
+                    "Posterior samples for lambda_2 found in the posterior "
+                    "table but they are all exactly 0. Assuming this is a BBH "
+                    "system."
+                ) 
+                return False
+            return True
+        elif "lambda_2" in self.parameters and "lambda_1" in self.parameters:
+            _lambda_1, _lambda_2 = self.specific_parameter_samples(
+                ["lambda_1", "lambda_2"]
+            )
+            if not np.any(_lambda_1) and not np.any(_lambda_2):
+                return False
+            elif not np.any(_lambda_1):
+                logger.warning(
+                    "Posterior samples for lambda_1 and lambda_2 found in the "
+                    "posterior table but lambda_1 is always exactly 0. "
+                    "Assuming this is an NSBH system."
+                )
+                return True
+        return False
+
     def remove_posterior(self, parameter):
         if parameter in self.parameters:
             logger.info(
                 "Removing the posterior samples for '{}'".format(parameter)
             )
             ind = self.parameters.index(parameter)
             self.parameters.remove(self.parameters[ind])
@@ -1704,14 +654,16 @@
         samples: list
             the list of samples that you would like to append
         """
         if parameter not in self.parameters:
             self.parameters.append(parameter)
             for num, i in enumerate(self.samples):
                 self.samples[num].append(samples[num])
+        if self.resume_file is not None:
+            self.write_current_state()
 
     def _mchirp_from_mchirp_source_z(self):
         samples = self.specific_parameter_samples(["chirp_mass_source", "redshift"])
         chirp_mass = mchirp_from_mchirp_source_z(samples[0], samples[1])
         self.append_data("chirp_mass", chirp_mass)
 
     def _q_from_eta(self):
@@ -1745,14 +697,24 @@
         self.append_data("mass_1", mass_1)
 
     def _m2_from_mchirp_q(self):
         samples = self.specific_parameter_samples(["chirp_mass", "mass_ratio"])
         mass_2 = m2_from_mchirp_q(samples[0], samples[1])
         self.append_data("mass_2", mass_2)
 
+    def _m1_from_mtotal_q(self):
+        samples = self.specific_parameter_samples(["total_mass", "mass_ratio"])
+        mass_1 = m1_from_mtotal_q(samples[0], samples[1])
+        self.append_data("mass_1", mass_1)
+
+    def _m2_from_mtotal_q(self):
+        samples = self.specific_parameter_samples(["total_mass", "mass_ratio"])
+        mass_2 = m2_from_mtotal_q(samples[0], samples[1])
+        self.append_data("mass_2", mass_2)
+
     def _reference_frequency(self):
         nsamples = len(self.samples)
         extra_kwargs = self.extra_kwargs["meta_data"]
         if extra_kwargs != {} and "f_ref" in list(extra_kwargs.keys()):
             self.append_data(
                 "reference_frequency", [float(extra_kwargs["f_ref"])] * nsamples
             )
@@ -1896,40 +858,82 @@
             "mass_1", "mass_2", "spin_1x", "spin_1y", "spin_2x", "spin_2y"]
         samples = self.specific_parameter_samples(parameters)
         chi_p_samples = chi_p(
             samples[0], samples[1], samples[2], samples[3], samples[4],
             samples[5])
         self.append_data("chi_p", chi_p_samples)
 
-    def _chi_eff(self):
-        parameters = ["mass_1", "mass_2", "spin_1z", "spin_2z"]
+    def _chi_p_from_tilts(self, suffix=""):
+        parameters = [
+            "mass_1", "mass_2", "a_1", "tilt_1{}".format(suffix), "a_2",
+            "tilt_2{}".format(suffix)
+        ]
+        samples = self.specific_parameter_samples(parameters)
+        chi_p_samples = chi_p_from_tilts(
+            samples[0], samples[1], samples[2], samples[3], samples[4],
+            samples[5]
+        )
+        self.append_data("chi_p{}".format(suffix), chi_p_samples)
+
+    def _chi_p_2spin(self):
+        parameters = [
+            "mass_1", "mass_2", "spin_1x", "spin_1y", "spin_2x", "spin_2y"]
+        samples = self.specific_parameter_samples(parameters)
+        chi_p_2spin_samples = chi_p_2spin(
+            samples[0], samples[1], samples[2], samples[3], samples[4],
+            samples[5])
+        self.append_data("chi_p_2spin", chi_p_2spin_samples)
+
+    def _chi_eff(self, suffix=""):
+        parameters = [
+            "mass_1", "mass_2", "spin_1z{}".format(suffix),
+            "spin_2z{}".format(suffix)
+        ]
         samples = self.specific_parameter_samples(parameters)
         chi_eff_samples = chi_eff(
             samples[0], samples[1], samples[2], samples[3])
-        self.append_data("chi_eff", chi_eff_samples)
+        self.append_data("chi_eff{}".format(suffix), chi_eff_samples)
+
+    def _aligned_spin_from_magnitude_tilts(
+        self, primary=False, secondary=False, suffix=""
+    ):
+        if primary:
+            parameters = ["a_1", "tilt_1{}".format(suffix)]
+            param_to_add = "spin_1z{}".format(suffix)
+        elif secondary:
+            parameters = ["a_2", "tilt_2{}".format(suffix)]
+            param_to_add = "spin_2z{}".format(suffix)
+        samples = self.specific_parameter_samples(parameters)
+        spin_samples = samples[0] * np.cos(samples[1])
+        self.append_data(param_to_add, spin_samples)
 
     def _cos_tilt_1_from_tilt_1(self):
         samples = self.specific_parameter_samples("tilt_1")
         cos_tilt_1 = np.cos(samples)
         self.append_data("cos_tilt_1", cos_tilt_1)
 
     def _cos_tilt_2_from_tilt_2(self):
         samples = self.specific_parameter_samples("tilt_2")
         cos_tilt_2 = np.cos(samples)
         self.append_data("cos_tilt_2", cos_tilt_2)
 
+    def _viewing_angle(self):
+        samples = self.specific_parameter_samples("theta_jn")
+        viewing_angle = viewing_angle_from_inclination(samples)
+        self.append_data("viewing_angle", viewing_angle)
+
     def _dL_from_z(self):
         samples = self.specific_parameter_samples("redshift")
         distance = dL_from_z(samples, cosmology=self.cosmology)
         self.extra_kwargs["meta_data"]["cosmology"] = self.cosmology
         self.append_data("luminosity_distance", distance)
 
     def _z_from_dL(self):
         samples = self.specific_parameter_samples("luminosity_distance")
-        func = getattr(_Redshift, self.redshift_method)
+        func = getattr(Redshift, self.redshift_method)
         redshift = func(
             samples, cosmology=self.cosmology, multi_process=self.multi_process
         )
         self.extra_kwargs["meta_data"]["cosmology"] = self.cosmology
         self.append_data("redshift", redshift)
 
     def _comoving_distance_from_z(self):
@@ -1939,29 +943,62 @@
         self.append_data("comoving_distance", distance)
 
     def _m1_source_from_m1_z(self):
         samples = self.specific_parameter_samples(["mass_1", "redshift"])
         mass_1_source = m1_source_from_m1_z(samples[0], samples[1])
         self.append_data("mass_1_source", mass_1_source)
 
+    def _m1_from_m1_source_z(self):
+        samples = self.specific_parameter_samples(["mass_1_source", "redshift"])
+        mass_1 = m1_from_m1_source_z(samples[0], samples[1])
+        self.append_data("mass_1", mass_1)
+
     def _m2_source_from_m2_z(self):
         samples = self.specific_parameter_samples(["mass_2", "redshift"])
         mass_2_source = m2_source_from_m2_z(samples[0], samples[1])
         self.append_data("mass_2_source", mass_2_source)
 
+    def _m2_from_m2_source_z(self):
+        samples = self.specific_parameter_samples(["mass_2_source", "redshift"])
+        mass_2 = m2_from_m2_source_z(samples[0], samples[1])
+        self.append_data("mass_2", mass_2)
+
     def _mtotal_source_from_mtotal_z(self):
         samples = self.specific_parameter_samples(["total_mass", "redshift"])
         total_mass_source = m_total_source_from_mtotal_z(samples[0], samples[1])
         self.append_data("total_mass_source", total_mass_source)
 
+    def _mtotal_from_mtotal_source_z(self):
+        samples = self.specific_parameter_samples(["total_mass_source", "redshift"])
+        total_mass = mtotal_from_mtotal_source_z(samples[0], samples[1])
+        self.append_data("total_mass", total_mass)
+
     def _mchirp_source_from_mchirp_z(self):
         samples = self.specific_parameter_samples(["chirp_mass", "redshift"])
         chirp_mass_source = mchirp_source_from_mchirp_z(samples[0], samples[1])
         self.append_data("chirp_mass_source", chirp_mass_source)
 
+    def _beta(self):
+        samples = self.specific_parameter_samples([
+            "mass_1", "mass_2", "phi_jl", "tilt_1", "tilt_2", "phi_12",
+            "a_1", "a_2", "reference_frequency", "phase"
+        ])
+        beta = opening_angle(
+            samples[0], samples[1], samples[2], samples[3], samples[4],
+            samples[5], samples[6], samples[7], samples[8], samples[9]
+        )
+        self.append_data("beta", beta)
+
+    def _psi_J(self):
+        samples = self.specific_parameter_samples([
+            "psi", "theta_jn", "phi_jl", "beta"
+        ])
+        psi = psi_J(samples[0], samples[1], samples[2], samples[3])
+        self.append_data("psi_J", psi)
+
     def _time_in_each_ifo(self):
         detectors = []
         if "IFOs" in list(self.extra_kwargs["meta_data"].keys()):
             detectors = self.extra_kwargs["meta_data"]["IFOs"].split(" ")
         else:
             for i in self.parameters:
                 if "optimal_snr" in i and i != "network_optimal_snr":
@@ -1995,14 +1032,42 @@
     def _delta_lambda_from_lambda1_lambda2(self):
         samples = self.specific_parameter_samples([
             "lambda_1", "lambda_2", "mass_1", "mass_2"])
         delta_lambda = delta_lambda_from_lambda1_lambda2(
             samples[0], samples[1], samples[2], samples[3])
         self.append_data("delta_lambda", delta_lambda)
 
+    def _NS_compactness_from_lambda(self, parameter="lambda_1"):
+        if parameter not in ["lambda_1", "lambda_2"]:
+            logger.warning(
+                "Can only use Love-compactness relation for 'lambda_1' and/or "
+                "'lambda_2'. Skipping conversion"
+            )
+            return
+        ind = parameter.split("lambda_")[1]
+        samples = self.specific_parameter_samples([parameter])
+        compactness = NS_compactness_from_lambda(samples[0])
+        self.append_data("compactness_{}".format(ind), compactness)
+        self.extra_kwargs["meta_data"]["compactness_fits"] = (
+            "YagiYunes2017_with_BBHlimit"
+        )
+
+    def _NS_baryonic_mass(self, primary=True):
+        if primary:
+            params = ["compactness_1", "mass_1"]
+        else:
+            params = ["compactness_2", "mass_2"]
+        samples = self.specific_parameter_samples(params)
+        mass = NS_baryonic_mass(samples[0], samples[1])
+        if primary:
+            self.append_data("baryonic_mass_1", mass)
+        else:
+            self.append_data("baryonic_mass_2", mass)
+        self.extra_kwargs["meta_data"]["baryonic_mass_fits"] = "Breu2016"
+
     def _lambda1_lambda2_from_polytrope_EOS(self):
         samples = self.specific_parameter_samples([
             "log_pressure", "gamma_1", "gamma_2", "gamma_3", "mass_1", "mass_2"
         ])
         lambda_1, lambda_2 = \
             lambda1_lambda2_from_4_parameter_piecewise_polytrope_equation_of_state(
                 *samples, multi_process=self.multi_process
@@ -2067,23 +1132,101 @@
         ]
         if _mf_detectors == _opt_detectors:
             mf_samples = self.specific_parameter_samples(mf_snrs)
             opt_samples = self.specific_parameter_samples(opt_snrs)
             snr = network_matched_filter_snr(mf_samples, opt_samples)
             self.append_data("network_matched_filter_snr", snr)
         else:
-            logger.warn(
+            logger.warning(
                 "Unable to generate 'network_matched_filter_snr' as "
                 "there is an inconsistency in the detector network based on "
                 "the 'optimal_snrs' and the 'matched_filter_snrs'. We find "
                 "that from the 'optimal_snrs', the detector network is: {} "
                 "while we find from the 'matched_filter_snrs', the detector "
                 "network is: {}".format(_opt_detectors, _mf_detectors)
             )
 
+    def _rho_hm(self, multipoles):
+        import copy
+        required = [
+            "mass_1", "mass_2", "spin_1z", "spin_2z", "psi", "iota", "ra",
+            "dec", "geocent_time", "luminosity_distance", "phase",
+            "reference_frequency"
+        ]
+        samples = self.specific_parameter_samples(required)
+        _f_low = self._retrieve_f_low()
+        if isinstance(_f_low, (np.ndarray)):
+            f_low = _f_low() * len(samples[0])
+        else:
+            f_low = [_f_low] * len(samples[0])
+        original_list = copy.deepcopy(multipoles)
+        rho, data_used = multipole_snr(
+            *samples[:-1], f_low=f_low, psd=self.psd, f_ref=samples[-1],
+            f_final=self.extra_kwargs["meta_data"]["f_final"],
+            return_data_used=True, multi_process=self.multi_process,
+            psd_default=self.psd_default, multipole=multipoles,
+            df=self.extra_kwargs["meta_data"]["delta_f"]
+        )
+        for num, mm in enumerate(original_list):
+            self.append_data("network_{}_multipole_snr".format(mm), rho[num])
+        self.extra_kwargs["meta_data"]["multipole_snr"] = data_used
+
+    def _rho_p(self):
+        required = [
+            "mass_1", "mass_2", "beta", "psi_J", "a_1", "a_2", "tilt_1",
+            "tilt_2", "phi_12", "theta_jn", "ra", "dec", "geocent_time",
+            "phi_jl", "reference_frequency", "luminosity_distance", "phase"
+        ]
+        samples = self.specific_parameter_samples(required)
+        try:
+            spins = self.specific_parameter_samples(["spin_1z", "spin_2z"])
+        except ValueError:
+            spins = [None, None]
+        _f_low = self._retrieve_f_low()
+        if isinstance(_f_low, (np.ndarray)):
+            f_low = _f_low() * len(samples[0])
+        else:
+            f_low = [_f_low] * len(samples[0])
+        [rho_p, b_bar, overlap, snrs], data_used = precessing_snr(
+            samples[0], samples[1], samples[2], samples[3], samples[4],
+            samples[5], samples[6], samples[7], samples[8], samples[9], samples[10],
+            samples[11], samples[12], samples[13], samples[15], samples[16], f_low=f_low,
+            spin_1z=spins[0], spin_2z=spins[1], psd=self.psd, return_data_used=True,
+            f_final=self.extra_kwargs["meta_data"]["f_final"], f_ref=samples[14],
+            multi_process=self.multi_process, psd_default=self.psd_default,
+            df=self.extra_kwargs["meta_data"]["delta_f"], debug=True
+        )
+        self.append_data("network_precessing_snr", rho_p)
+        self.append_data("_b_bar", b_bar)
+        self.append_data("_precessing_harmonics_overlap", overlap)
+        nbreakdown = len(np.argwhere(b_bar > 0.3))
+        if nbreakdown > 0:
+            logger.warning(
+                "{}/{} ({}%) samples have b_bar greater than 0.3. For these "
+                "samples, the two-harmonic approximation used to calculate "
+                "the precession SNR may not be valid".format(
+                    nbreakdown, len(b_bar),
+                    np.round((nbreakdown / len(b_bar)) * 100, 2)
+                )
+            )
+        try:
+            _samples = self.specific_parameter_samples("network_optimal_snr")
+            if np.logical_or(
+                    np.median(snrs) > 1.1 * np.median(_samples),
+                    np.median(snrs) < 0.9 * np.median(_samples)
+            ):
+                logger.warning(
+                    "The two-harmonic SNR is different from the stored SNR. "
+                    "This indicates that the provided PSD may be different "
+                    "from the one used in the sampling."
+                )
+        except Exception:
+            pass
+        self.extra_kwargs["meta_data"]["precessing_snr"] = data_used
+
     def _retrieve_f_low(self):
         extra_kwargs = self.extra_kwargs["meta_data"]
         if extra_kwargs != {} and "f_low" in list(extra_kwargs.keys()):
             f_low = extra_kwargs["f_low"]
         else:
             raise ValueError(
                 "Could not find f_low in input file. Please either modify the "
@@ -2098,37 +1241,53 @@
         else:
             raise ValueError(
                 "Unable to find the approximant used to generate the posterior "
                 "samples in the result file."
             )
         return approximant
 
-    def _evolve_spins(self, final_velocity="ISCO"):
-        from pesummary.gw.file.evolve import evolve_spins
+    def _evolve_spins(self, final_velocity="ISCO", forward=True):
+        from .evolve import evolve_spins
 
-        f_low = self._retrieve_f_low()
-        approximant = self._retrieve_approximant()
-        if not hasattr(lalsimulation, approximant):
-            _msg = (
-                'Not evolving spins: approximant {0} unknown to '
-                'lalsimulation'.format(approximant)
-            )
-            logger.warning(_msg)
-            raise EvolveSpinError(_msg)
         parameters = ["tilt_1", "tilt_2", "phi_12", "spin_1z", "spin_2z"]
         samples = self.specific_parameter_samples(
             ["mass_1", "mass_2", "a_1", "a_2", "tilt_1", "tilt_2",
              "phi_12", "reference_frequency"]
         )
-        tilt_1_evolved, tilt_2_evolved, phi_12_evolved = evolve_spins(
-            samples[0], samples[1], samples[2], samples[3], samples[4],
-            samples[5], samples[6], f_low, samples[7][0],
-            approximant, final_velocity=final_velocity,
-            multi_process=self.multi_process
-        )
+        if not forward:
+            [tilt_1_evolved, tilt_2_evolved, phi_12_evolved], fits_used = evolve_spins(
+                samples[0], samples[1], samples[2], samples[3], samples[4],
+                samples[5], samples[6], samples[7][0],
+                evolve_limit="infinite_separation", multi_process=self.multi_process,
+                return_fits_used=True, method=self.evolve_spins_backwards
+            )
+            suffix = ""
+            if self.evolve_spins_backwards.lower() == "precession_averaged":
+                suffix = "_only_prec_avg"
+            self.append_data("tilt_1_infinity{}".format(suffix), tilt_1_evolved)
+            self.append_data("tilt_2_infinity{}".format(suffix), tilt_2_evolved)
+            self.extra_kwargs["meta_data"]["backward_spin_evolution"] = fits_used
+            return
+        else:
+            f_low = self._retrieve_f_low()
+            approximant = self._retrieve_approximant()
+            if not hasattr(lalsimulation, approximant):
+                _msg = (
+                    'Not evolving spins: approximant {0} unknown to '
+                    'lalsimulation'.format(approximant)
+                )
+                logger.warning(_msg)
+                raise EvolveSpinError(_msg)
+            tilt_1_evolved, tilt_2_evolved, phi_12_evolved = evolve_spins(
+                samples[0], samples[1], samples[2], samples[3], samples[4],
+                samples[5], samples[6], f_low, samples[7][0],
+                approximant, final_velocity=final_velocity,
+                multi_process=self.multi_process
+            )
+            self.extra_kwargs["meta_data"]["forward_spin_evolution"] = final_velocity
         spin_1z_evolved = samples[2] * np.cos(tilt_1_evolved)
         spin_2z_evolved = samples[3] * np.cos(tilt_2_evolved)
         self.append_data("tilt_1_evolved", tilt_1_evolved)
         self.append_data("tilt_2_evolved", tilt_2_evolved)
         self.append_data("phi_12_evolved", phi_12_evolved)
         self.append_data("spin_1z_evolved", spin_1z_evolved)
         self.append_data("spin_2z_evolved", spin_2z_evolved)
@@ -2198,32 +1357,35 @@
         return samples
 
     def _peak_luminosity_of_merger(self, evolved=False):
         param = self._evolved_vs_non_evolved_parameter(
             "peak_luminosity", evolved=evolved, non_precessing=self.non_precessing
         )
         spin_1z_param = self._evolved_vs_non_evolved_parameter(
-            "spin_1z", evolved=evolved, core_param=True, non_precessing=self.non_precessing
+            "spin_1z", evolved=evolved, core_param=True,
+            non_precessing=self.non_precessing
         )
         spin_2z_param = self._evolved_vs_non_evolved_parameter(
-            "spin_2z", evolved=evolved, core_param=True, non_precessing=self.non_precessing
+            "spin_2z", evolved=evolved, core_param=True,
+            non_precessing=self.non_precessing
         )
 
         samples = self.specific_parameter_samples([
             "mass_1", "mass_2", spin_1z_param, spin_2z_param
         ])
         peak_luminosity, fits = peak_luminosity_of_merger(
             samples[0], samples[1], samples[2], samples[3],
             return_fits_used=True
         )
         self.append_data(param, peak_luminosity)
         self.extra_kwargs["meta_data"]["peak_luminosity_NR_fits"] = fits
 
     def _final_remnant_properties_from_NRSurrogate(
-        self, non_precessing=False, parameters=["final_mass", "final_spin", "final_kick"]
+        self, non_precessing=False,
+        parameters=["final_mass", "final_spin", "final_kick"]
     ):
         f_low = self._retrieve_f_low()
         approximant = self._retrieve_approximant()
         samples = self._precessing_vs_non_precessing_parameters(
             non_precessing=non_precessing, evolved=False
         )
         frequency_samples = self.specific_parameter_samples([
@@ -2234,16 +1396,76 @@
             properties=parameters, return_fits_used=True,
             approximant=approximant
         )
         for param in parameters:
             self.append_data(param, data[param])
             self.extra_kwargs["meta_data"]["{}_NR_fits".format(param)] = fits
 
+    def _final_remnant_properties_from_NSBH_waveform(
+        self, source=False, parameters=[
+            "baryonic_torus_mass", "final_mass", "final_spin"
+        ]
+    ):
+        approximant = self._retrieve_approximant()
+        if source:
+            sample_params = [
+                "mass_1_source", "mass_2_source", "spin_1z", "lambda_2"
+            ]
+        else:
+            sample_params = ["mass_1", "mass_2", "spin_1z", "lambda_2"]
+        samples = self.specific_parameter_samples(sample_params)
+        _data = _check_NSBH_approximant(
+            approximant, samples[0], samples[1], samples[2], samples[3],
+            _raise=False
+        )
+        if _data is None:
+            return
+        _mapping = {
+            "220_quasinormal_mode_frequency": 0, "tidal_disruption_frequency": 1,
+            "baryonic_torus_mass": 2, "compactness_2": 3,
+            "final_mass": 4, "final_spin": 5
+        }
+        for param in parameters:
+            self.append_data(param, _data[_mapping[param]])
+        if "final_mass" in parameters:
+            self.extra_kwargs["meta_data"]["final_mass_NR_fits"] = "Zappa2019"
+        if "final_spin" in parameters:
+            self.extra_kwargs["meta_data"]["final_spin_NR_fits"] = "Zappa2019"
+        if "baryonic_torus_mass" in parameters:
+            self.extra_kwargs["meta_data"]["baryonic_torus_mass_fits"] = (
+                "Foucart2012"
+            )
+        if "220_quasinormal_mode_frequency" in parameters:
+            self.extra_kwargs["meta_data"]["quasinormal_mode_fits"] = (
+                "London2019"
+            )
+        if "tidal_disruption_frequency" in parameters:
+            probabilities = NSBH_merger_type(
+                samples[0], samples[1], samples[2], samples[3],
+                approximant=approximant,
+                _ringdown=_data[_mapping["220_quasinormal_mode_frequency"]],
+                _disruption=_data[_mapping["tidal_disruption_frequency"]],
+                _torus=_data[_mapping["baryonic_torus_mass"]], percentages=True
+            )
+            self.extra_kwargs["meta_data"]["NSBH_merger_type_probabilities"] = (
+                probabilities
+            )
+            self.extra_kwargs["meta_data"]["tidal_disruption_frequency_fits"] = (
+                "Pannarale2018"
+            )
+            ratio = (
+                _data[_mapping["tidal_disruption_frequency"]]
+                / _data[_mapping["220_quasinormal_mode_frequency"]]
+            )
+            self.append_data(
+                "tidal_disruption_frequency_ratio", ratio
+            )
+
     def _final_remnant_properties_from_waveform(
-        self, non_precessing=False, parameters=["final_mass", "final_spin"]
+        self, non_precessing=False, parameters=["final_mass", "final_spin"],
     ):
         f_low = self._retrieve_f_low()
         approximant = self._retrieve_approximant()
         if "delta_t" in self.extra_kwargs["meta_data"].keys():
             delta_t = self.extra_kwargs["meta_data"]["delta_t"]
         else:
             delta_t = 1. / 4096
@@ -2262,15 +1484,15 @@
             sample_params = [
                 "mass_1", "mass_2", "spin_1x", "spin_1y", "spin_1z",
                 "spin_2x", "spin_2y", "spin_2z", "iota", "luminosity_distance",
                 "phase"
             ]
         samples = self.specific_parameter_samples(sample_params)
         ind = self.parameters.index("spin_1x")
-        _data, fits = _final_from_initial(
+        _data, fits = _final_from_initial_BBH(
             *samples[:8], iota=samples[8], luminosity_distance=samples[9],
             f_ref=[f_low] * len(samples[0]), phi_ref=samples[10],
             delta_t=1. / 4096, approximant=approximant, return_fits_used=True,
             multi_process=self.multi_process
         )
         data = {"final_mass": _data[0], "final_spin": _data[1]}
         for param in parameters:
@@ -2342,14 +1564,23 @@
         else:
             samples = self.specific_parameter_samples(
                 [parameter_to_add.split("cos_")[1]]
             )
             cos_samples = np.cos(samples[0])
         self.append_data(parameter_to_add, cos_samples)
 
+    def source_frame_from_detector_frame(self, detector_frame_parameter):
+        samples = self.specific_parameter_samples(
+            [detector_frame_parameter, "redshift"]
+        )
+        source_frame = _source_from_detector(samples[0], samples[1])
+        self.append_data(
+            "{}_source".format(detector_frame_parameter), source_frame
+        )
+
     def _check_parameters(self):
         params = ["mass_1", "mass_2", "a_1", "a_2", "mass_1_source", "mass_2_source",
                   "mass_ratio", "total_mass", "chirp_mass"]
         for i in params:
             if i in self.parameters:
                 samples = self.specific_parameter_samples([i])
                 if "mass" in i:
@@ -2364,64 +1595,75 @@
                     logger.warning(
                         "Removing %s samples because they have unphysical "
                         "values (%s < 0)" % (len(ind), i)
                     )
                     for i in np.arange(len(ind) - 1, -1, -1):
                         self.samples.remove(list(np.array(self.samples)[ind[i][0]]))
 
-    def generate_all_posterior_samples(self, evolve_spins=False):
+    def generate_all_posterior_samples(self):
         logger.debug("Starting to generate all derived posteriors")
         evolve_condition = (
-            True if evolve_spins and self.compute_remnant else False
+            True if self.evolve_spins_forwards and self.compute_remnant else False
         )
         if "cos_theta_jn" in self.parameters and "theta_jn" not in self.parameters:
             self._cos_angle("theta_jn", reverse=True)
         if "cos_iota" in self.parameters and "iota" not in self.parameters:
             self._cos_angle("iota", reverse=True)
         if "cos_tilt_1" in self.parameters and "tilt_1" not in self.parameters:
             self._cos_angle("tilt_1", reverse=True)
         if "cos_tilt_2" in self.parameters and "tilt_2" not in self.parameters:
             self._cos_angle("tilt_2", reverse=True)
+        angles = [
+            "a_1", "a_2", "a_1_azimuthal", "a_1_polar", "a_2_azimuthal",
+            "a_2_polar"
+        ]
+        if all(i in self.parameters for i in angles):
+            self._component_spins_from_azimuthal_and_polar_angles()
         spin_magnitudes = ["a_1", "a_2"]
         angles = ["phi_jl", "tilt_1", "tilt_2", "phi_12"]
         cartesian = ["spin_1x", "spin_1y", "spin_1z", "spin_2x", "spin_2y", "spin_2z"]
         cond1 = all(i in self.parameters for i in spin_magnitudes)
         cond2 = all(i in self.parameters for i in angles)
         cond3 = all(i in self.parameters for i in cartesian)
-        if cond1 and not cond2:
-            self.parameters.append("tilt_1")
-            self.parameters.append("tilt_2")
-            for num, i in enumerate(self.samples):
-                self.samples[num].append(
-                    np.arccos(np.sign(i[self.parameters.index("a_1")])))
-                self.samples[num].append(
-                    np.arccos(np.sign(i[self.parameters.index("a_2")])))
-            ind_a1 = self.parameters.index("a_1")
-            ind_a2 = self.parameters.index("a_2")
-            for num, i in enumerate(self.samples):
-                self.samples[num][ind_a1] = abs(self.samples[num][ind_a1])
-                self.samples[num][ind_a2] = abs(self.samples[num][ind_a2])
-        elif not cond1 and not cond2 and not cond3 and self.add_zero_spin:
-            parameters = ["a_1", "a_2", "spin_1z", "spin_2z"]
-            for param in parameters:
-                self.parameters.append(param)
+        for _param in spin_magnitudes:
+            if _param in self.parameters and not cond2 and not cond3:
+                _index = _param.split("a_")[1]
+                _spin = self.specific_parameter_samples(_param)
+                _tilt = np.arccos(np.sign(_spin))
+                self.append_data("tilt_{}".format(_index), _tilt)
+                _spin_ind = self.parameters.index(_param)
                 for num, i in enumerate(self.samples):
-                    self.samples[num].append(0)
+                    self.samples[num][_spin_ind] = abs(self.samples[num][_spin_ind])
+
+        if not cond2 and not cond3 and self.add_zero_spin:
+            for _param in spin_magnitudes:
+                if _param not in self.parameters:
+                    _spin = np.zeros(len(self.samples))
+                    self.append_data(_param, _spin)
+                    _index = _param.split("a_")[1]
+                    self.append_data("spin_{}z".format(_index), _spin)
         self._check_parameters()
         if "cos_theta_jn" in self.parameters and "theta_jn" not in self.parameters:
             self._cos_angle("theta_jn", reverse=True)
         if "cos_iota" in self.parameters and "iota" not in self.parameters:
             self._cos_angle("iota", reverse=True)
         if "cos_tilt_1" in self.parameters and "tilt_1" not in self.parameters:
             self._cos_angle("tilt_1", reverse=True)
         if "cos_tilt_2" in self.parameters and "tilt_2" not in self.parameters:
             self._cos_angle("tilt_2", reverse=True)
-        if "chirp_mass" not in self.parameters and "chirp_mass_source" in \
-                self.parameters and "redshift" in self.parameters:
-            self._mchirp_from_mchirp_source_z()
+        if "luminosity_distance" not in self.parameters:
+            if "redshift" in self.parameters:
+                self._dL_from_z()
+        if "redshift" not in self.parameters:
+            if "luminosity_distance" in self.parameters:
+                self._z_from_dL()
+        if "comoving_distance" not in self.parameters:
+            if "redshift" in self.parameters:
+                self._comoving_distance_from_z()
+
         if "mass_ratio" not in self.parameters and "symmetric_mass_ratio" in \
                 self.parameters:
             self._q_from_eta()
         if "mass_ratio" not in self.parameters and "mass_1" in self.parameters \
                 and "mass_2" in self.parameters:
             self._q_from_m1_m2()
         if "mass_ratio" in self.parameters:
@@ -2434,32 +1676,64 @@
             self._invq_from_q()
         if "chirp_mass" not in self.parameters and "total_mass" in self.parameters:
             self._mchirp_from_mtotal_q()
         if "mass_1" not in self.parameters and "chirp_mass" in self.parameters:
             self._m1_from_mchirp_q()
         if "mass_2" not in self.parameters and "chirp_mass" in self.parameters:
             self._m2_from_mchirp_q()
-        if "reference_frequency" not in self.parameters:
-            self._reference_frequency()
-        condition1 = "phi_12" not in self.parameters
-        condition2 = "phi_1" in self.parameters and "phi_2" in self.parameters
-        if condition1 and condition2:
-            self._phi_12_from_phi1_phi2()
-        angles = [
-            "a_1", "a_2", "a_1_azimuthal", "a_1_polar", "a_2_azimuthal",
-            "a_2_polar"]
-        if all(i in self.parameters for i in angles):
-            self._component_spins_from_azimuthal_and_polar_angles()
+        if "mass_1" not in self.parameters and "total_mass" in self.parameters:
+            self._m1_from_mtotal_q()
+        if "mass_2" not in self.parameters and "total_mass" in self.parameters:
+            self._m2_from_mtotal_q()
         if "mass_1" in self.parameters and "mass_2" in self.parameters:
             if "total_mass" not in self.parameters:
                 self._mtotal_from_m1_m2()
             if "chirp_mass" not in self.parameters:
                 self._mchirp_from_m1_m2()
             if "symmetric_mass_ratio" not in self.parameters:
                 self._eta_from_m1_m2()
+        if "redshift" in self.parameters:
+            if "mass_1_source" not in self.parameters:
+                if "mass_1" in self.parameters:
+                    self._m1_source_from_m1_z()
+            if "mass_1_source" in self.parameters:
+                if "mass_1" not in self.parameters:
+                    self._m1_from_m1_source_z()
+            if "mass_2_source" not in self.parameters:
+                if "mass_2" in self.parameters:
+                    self._m2_source_from_m2_z()
+            if "mass_2_source" in self.parameters:
+                if "mass_2" not in self.parameters:
+                    self._m2_from_m2_source_z()
+            if "total_mass_source" not in self.parameters:
+                if "total_mass" in self.parameters:
+                    self._mtotal_source_from_mtotal_z()
+            if "total_mass_source" in self.parameters:
+                if "total_mass" not in self.parameters:
+                    self._mtotal_from_mtotal_source_z()
+            if "chirp_mass_source" not in self.parameters:
+                if "chirp_mass" in self.parameters:
+                    self._mchirp_source_from_mchirp_z()
+            if "chirp_mass_source" in self.parameters:
+                if "chirp_mass" not in self.parameters:
+                    self._mchirp_from_mchirp_source_z()
+
+        if "reference_frequency" not in self.parameters:
+            self._reference_frequency()
+        condition1 = "phi_12" not in self.parameters
+        condition2 = "phi_1" in self.parameters and "phi_2" in self.parameters
+        if condition1 and condition2:
+            self._phi_12_from_phi1_phi2()
+
+        check_for_evolved_parameter = lambda suffix, param, params: (
+            param not in params and param + suffix not in params if
+            len(suffix) else param not in params
+        )
+
+        if "mass_1" in self.parameters and "mass_2" in self.parameters:
             spin_components = [
                 "spin_1x", "spin_1y", "spin_1z", "spin_2x", "spin_2y", "spin_2z",
                 "iota"
             ]
             angles = ["a_1", "a_2", "tilt_1", "tilt_2", "theta_jn"]
             if all(i in self.parameters for i in spin_components):
                 self._spin_angles()
@@ -2476,20 +1750,59 @@
                         self._component_spins()
             cond1 = "spin_1x" in self.parameters and "spin_1y" in self.parameters
             if "phi_1" not in self.parameters and cond1:
                 self._phi1_from_spins()
             cond1 = "spin_2x" in self.parameters and "spin_2y" in self.parameters
             if "phi_2" not in self.parameters and cond1:
                 self._phi2_from_spins()
-            if "chi_eff" not in self.parameters:
-                if all(i in self.parameters for i in spin_components):
-                    self._chi_eff()
-            if "chi_p" not in self.parameters:
-                if all(i in self.parameters for i in spin_components):
-                    self._chi_p()
+            evolve_spins_params = ["tilt_1", "tilt_2", "phi_12"]
+            if self.evolve_spins_backwards:
+                if all(i in self.parameters for i in evolve_spins_params):
+                    self._evolve_spins(forward=False)
+            for suffix in ["_infinity", "_infinity_only_prec_avg", ""]:
+                if "spin_1z{}".format(suffix) not in self.parameters:
+                    _params = ["a_1", "tilt_1{}".format(suffix)]
+                    if all(i in self.parameters for i in _params):
+                        self._aligned_spin_from_magnitude_tilts(
+                            primary=True, suffix=suffix
+                        )
+                if "spin_2z{}".format(suffix) not in self.parameters:
+                    _params = ["a_2", "tilt_2{}".format(suffix)]
+                    if all(i in self.parameters for i in _params):
+                        self._aligned_spin_from_magnitude_tilts(
+                            secondary=True, suffix=suffix
+                        )
+                if "chi_eff{}".format(suffix) not in self.parameters:
+                    _params = ["spin_1z{}".format(suffix), "spin_2z{}".format(suffix)]
+                    if all(i in self.parameters for i in _params):
+                        self._chi_eff(suffix=suffix)
+                if any(
+                        _p.format(suffix) not in self.parameters for _p in
+                        ["chi_p{}", "chi_p_2spin"]
+                ):
+                    _params = [
+                        "a_1", "tilt_1{}".format(suffix), "a_2",
+                        "tilt_2{}".format(suffix)
+                    ]
+                    _cartesian_params = ["spin_1x", "spin_1y", "spin_2x", "spin_2y"]
+                    if "chi_p{}".format(suffix) not in self.parameters:
+                        if all(i in self.parameters for i in _params):
+                            self._chi_p_from_tilts(suffix=suffix)
+                        elif all(i in self.parameters for i in _cartesian_params):
+                            self._chi_p()
+                    if "chi_p_2spin" not in self.parameters:
+                        if all(i in self.parameters for i in _cartesian_params):
+                            self._chi_p_2spin()
+            if "beta" not in self.parameters:
+                beta_components = [
+                    "mass_1", "mass_2", "phi_jl", "tilt_1", "tilt_2", "phi_12",
+                    "a_1", "a_2", "reference_frequency", "phase"
+                ]
+                if all(i in self.parameters for i in beta_components):
+                    self._beta()
             polytrope_params = ["log_pressure", "gamma_1", "gamma_2", "gamma_3"]
             if all(param in self.parameters for param in polytrope_params):
                 if "lambda_1" not in self.parameters or "lambda_2" not in self.parameters:
                     self._lambda1_lambda2_from_polytrope_EOS()
             spectral_params = [
                 "spectral_decomposition_gamma_{}".format(num) for num in
                 np.arange(4)
@@ -2502,44 +1815,44 @@
             if "lambda_2" not in self.parameters and "lambda_1" in self.parameters:
                 self._lambda2_from_lambda1()
             if "lambda_1" in self.parameters and "lambda_2" in self.parameters:
                 if "lambda_tilde" not in self.parameters:
                     self._lambda_tilde_from_lambda1_lambda2()
                 if "delta_lambda" not in self.parameters:
                     self._delta_lambda_from_lambda1_lambda2()
+            if "psi" in self.parameters:
+                dpsi_parameters = ["theta_jn", "phi_jl", "beta"]
+                if all(i in self.parameters for i in dpsi_parameters):
+                    if "psi_J" not in self.parameters:
+                        self._psi_J()
 
             evolve_suffix = "_non_evolved"
             final_spin_params = ["a_1", "a_2"]
             non_precessing_NR_params = ["spin_1z", "spin_2z"]
             if evolve_condition:
                 final_spin_params += [
                     "tilt_1_evolved", "tilt_2_evolved", "phi_12_evolved"
                 ]
                 non_precessing_NR_params = [
                     "{}_evolved".format(i) for i in non_precessing_NR_params
                 ]
                 evolve_suffix = "_evolved"
-                evolve_spins_params = ["tilt_1", "tilt_2", "phi_12"]
                 if all(i in self.parameters for i in evolve_spins_params):
                     try:
-                        self._evolve_spins(final_velocity=evolve_spins)
+                        self._evolve_spins(final_velocity=self.evolve_spins_forwards)
                     except EvolveSpinError:
                         # Raised when approximant is unknown to lalsimulation or
                         # lalsimulation.SimInspiralGetSpinFreqFromApproximant is
                         # equal to lalsimulation.SIM_INSPIRAL_SPINS_CASEBYCASE
                         evolve_condition = False
                 else:
                     evolve_condition = False
             else:
                 final_spin_params += ["tilt_1", "tilt_2", "phi_12"]
 
-            check_for_evolved_parameter = lambda suffix, param, params: (
-                param not in params and param + suffix not in params if
-                len(suffix) else param not in params
-            )
             condition_peak_luminosity = check_for_evolved_parameter(
                 evolve_suffix, "peak_luminosity", self.parameters
             )
             condition_final_spin = check_for_evolved_parameter(
                 evolve_suffix, "final_spin", self.parameters
             )
             condition_final_mass = check_for_evolved_parameter(
@@ -2557,72 +1870,107 @@
                         "spin_2y", "spin_2z"
                     ]
                     function = self._final_remnant_properties_from_waveform
 
                 for param in _default:
                     if param not in self.parameters:
                         parameters.append(param)
-                if all(i in self.parameters for i in final_spin_params):
+                # We already know that lambda_2 is in the posterior table if
+                # self.NSBH = True
+                if self.NSBH and "spin_1z" in self.parameters:
+                    self._final_remnant_properties_from_NSBH_waveform()
+                elif all(i in self.parameters for i in final_spin_params):
                     function(non_precessing=False, parameters=parameters)
                 elif all(i in self.parameters for i in non_precessing_NR_params):
                     function(non_precessing=True, parameters=parameters)
                 if all(i in self.parameters for i in non_precessing_NR_params):
                     if condition_peak_luminosity or self.force_non_evolved:
-                        self._peak_luminosity_of_merger(evolved=evolve_condition)
+                        if not self.NSBH:
+                            self._peak_luminosity_of_merger(evolved=evolve_condition)
             elif self.compute_remnant:
                 if all(i in self.parameters for i in final_spin_params):
                     if condition_final_spin or self.force_non_evolved:
                         self._final_spin_of_merger(evolved=evolve_condition)
                 elif all(i in self.parameters for i in non_precessing_NR_params):
                     if condition_final_spin or self.force_non_evolved:
                         self._final_spin_of_merger(
                             non_precessing=True, evolved=False
                         )
                 if all(i in self.parameters for i in non_precessing_NR_params):
                     if condition_peak_luminosity or self.force_non_evolved:
                         self._peak_luminosity_of_merger(evolved=evolve_condition)
                     if condition_final_mass or self.force_non_evolved:
                         self._final_mass_of_merger(evolved=evolve_condition)
-        if "cos_tilt_1" not in self.parameters and "tilt_1" in self.parameters:
-            self._cos_tilt_1_from_tilt_1()
-        if "cos_tilt_2" not in self.parameters and "tilt_2" in self.parameters:
-            self._cos_tilt_2_from_tilt_2()
-        if "luminosity_distance" not in self.parameters and "redshift" in self.parameters:
-            self._dL_from_z()
-        if "redshift" not in self.parameters and "luminosity_distance" in self.parameters:
-            self._z_from_dL()
-        if "comoving_distance" not in self.parameters and "redshift" in self.parameters:
-            self._comoving_distance_from_z()
 
+            # if NSBH system and self.compute_remnant = False and/or BBH fits
+            # fits used, only calculate baryonic_torus_mass
+            if self.NSBH and "spin_1z" in self.parameters:
+                if "baryonic_torus_mass" not in self.parameters:
+                    self._final_remnant_properties_from_NSBH_waveform(
+                        parameters=["baryonic_torus_mass"]
+                    )
+        # calculate compactness from Love-compactness relation
+        if "lambda_1" in self.parameters and "compactness_1" not in self.parameters:
+            self._NS_compactness_from_lambda(parameter="lambda_1")
+            if "mass_1" in self.parameters and "baryonic_mass_1" not in self.parameters:
+                self._NS_baryonic_mass(primary=True)
+        if "lambda_2" in self.parameters and "compactness_2" not in self.parameters:
+            self._NS_compactness_from_lambda(parameter="lambda_2")
+            if "mass_2" in self.parameters and "baryonic_mass_2" not in self.parameters:
+                self._NS_baryonic_mass(primary=False)
+        for suffix in ["_infinity", "_infinity_only_prec_avg", ""]:
+            for tilt in ["tilt_1", "tilt_2"]:
+                cond1 = "cos_{}{}".format(tilt, suffix) not in self.parameters
+                cond2 = "{}{}".format(tilt, suffix) in self.parameters
+                if cond1 and cond2:
+                    self._cos_angle("cos_{}{}".format(tilt, suffix))
         evolve_suffix = "_non_evolved"
         if evolve_condition or self.NRSurrogate or self.waveform_fit or self.non_precessing:
             evolve_suffix = ""
             evolve_condition = True
         if "redshift" in self.parameters:
-            if "mass_1_source" not in self.parameters and "mass_1" in self.parameters:
-                self._m1_source_from_m1_z()
-            if "mass_2_source" not in self.parameters and "mass_2" in self.parameters:
-                self._m2_source_from_m2_z()
-            if "total_mass_source" not in self.parameters and "total_mass" in self.parameters:
-                self._mtotal_source_from_mtotal_z()
-            if "chirp_mass_source" not in self.parameters and "chirp_mass" in self.parameters:
-                self._mchirp_source_from_mchirp_z()
             condition_final_mass_source = check_for_evolved_parameter(
                 evolve_suffix, "final_mass_source", self.parameters
             )
             if condition_final_mass_source or self.force_non_evolved:
                 if "final_mass{}".format(evolve_suffix) in self.parameters:
                     self._final_mass_source(evolved=evolve_condition)
+            if "baryonic_torus_mass" in self.parameters:
+                if "baryonic_torus_mass_source" not in self.parameters:
+                    self.source_frame_from_detector_frame(
+                        "baryonic_torus_mass"
+                    )
+            if "baryonic_mass_1" in self.parameters:
+                if "baryonic_mass_1_source" not in self.parameters:
+                    self.source_frame_from_detector_frame(
+                        "baryonic_mass_1"
+                    )
+            if "baryonic_mass_2" in self.parameters:
+                if "baryonic_mass_2_source" not in self.parameters:
+                    self.source_frame_from_detector_frame(
+                        "baryonic_mass_2"
+                    )
         if "total_mass_source" in self.parameters:
             if "final_mass_source{}".format(evolve_suffix) in self.parameters:
                 condition_radiated_energy = check_for_evolved_parameter(
                     evolve_suffix, "radiated_energy", self.parameters
                 )
                 if condition_radiated_energy or self.force_non_evolved:
                     self._radiated_energy(evolved=evolve_condition)
+        if self.NSBH and "spin_1z" in self.parameters:
+            if all(_p in self.parameters for _p in ["mass_1_source", "mass_2_source"]):
+                _NSBH_parameters = []
+                if "tidal_disruption_frequency" not in self.parameters:
+                    _NSBH_parameters.append("tidal_disruption_frequency")
+                if "220_quasinormal_mode_frequency" not in self.parameters:
+                    _NSBH_parameters.append("220_quasinormal_mode_frequency")
+                if len(_NSBH_parameters):
+                    self._final_remnant_properties_from_NSBH_waveform(
+                        parameters=_NSBH_parameters, source=True
+                    )
         location = ["geocent_time", "ra", "dec"]
         if all(i in self.parameters for i in location):
             try:
                 self._time_in_each_ifo()
             except Exception as e:
                 logger.warning(
                     "Failed to generate posterior samples for the time in each "
@@ -2633,16 +1981,72 @@
                 self._ifo_snr()
         if any("_optimal_snr" in i for i in self.parameters):
             if "network_optimal_snr" not in self.parameters:
                 self._optimal_network_snr()
             if any("_matched_filter_snr" in i for i in self.parameters):
                 if "network_matched_filter_snr" not in self.parameters:
                     self._matched_filter_network_snr()
+        if self.multipole_snr:
+            rho_hm_parameters = [
+                "mass_1", "mass_2", "spin_1z", "spin_2z", "psi", "iota", "ra",
+                "dec", "geocent_time", "luminosity_distance", "phase"
+            ]
+            cond = [
+                int(mm) for mm in ['21', '33', '44'] if
+                "network_{}_multipole_snr".format(mm) not in self.parameters
+            ]
+            if all(i in self.parameters for i in rho_hm_parameters) and len(cond):
+                try:
+                    logger.warning(
+                        "Starting to calculate the SNR in the {} multipole{}. "
+                        "This may take some time".format(
+                            " and ".join([str(mm) for mm in cond]),
+                            "s" if len(cond) > 1 else ""
+                        )
+                    )
+                    self._rho_hm(cond)
+                except ImportError as e:
+                    logger.warning(e)
+            elif len(cond):
+                logger.warning(
+                    "Unable to calculate the multipole SNR because it requires "
+                    "samples for {}".format(
+                        ", ".join(
+                            [i for i in rho_hm_parameters if i not in self.parameters]
+                        )
+                    )
+                )
+        if "network_precessing_snr" not in self.parameters and self.precessing_snr:
+            rho_p_parameters = [
+                "mass_1", "mass_2", "beta", "psi_J", "a_1", "a_2", "tilt_1",
+                "tilt_2", "phi_12", "theta_jn", "phi_jl", "ra", "dec", "geocent_time",
+                "phi_jl"
+            ]
+            if all(i in self.parameters for i in rho_p_parameters):
+                try:
+                    logger.warning(
+                        "Starting to calculate the precessing SNR. This may take "
+                        "some time"
+                    )
+                    self._rho_p()
+                except ImportError as e:
+                    logger.warning(e)
+            else:
+                logger.warning(
+                    "Unable to calculate the precessing SNR because requires "
+                    "samples for {}".format(
+                        ", ".join(
+                            [i for i in rho_p_parameters if i not in self.parameters]
+                        )
+                    )
+                )
         if "theta_jn" in self.parameters and "cos_theta_jn" not in self.parameters:
             self._cos_angle("cos_theta_jn")
+        if "theta_jn" in self.parameters and "viewing_angle" not in self.parameters:
+            self._viewing_angle()
         if "iota" in self.parameters and "cos_iota" not in self.parameters:
             self._cos_angle("cos_iota")
         remove_parameters = [
             "tilt_1_evolved", "tilt_2_evolved", "phi_12_evolved",
             "spin_1z_evolved", "spin_2z_evolved", "reference_frequency",
             "minimum_frequency"
         ]
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/__init__.py` & `pesummary-1.0.0/pesummary/core/js/expert.js`

 * *Files 24% similar despite different names*

```diff
@@ -1,88 +1,63 @@
-00000000: 2320 436f 7079 7269 6768 7420 2843 2920  # Copyright (C) 
-00000010: 3230 3138 2020 4368 6172 6c69 6520 486f  2018  Charlie Ho
-00000020: 7920 3c63 6861 726c 6965 2e68 6f79 406c  y <charlie.hoy@l
-00000030: 6967 6f2e 6f72 673e 0a23 2054 6869 7320  igo.org>.# This 
-00000040: 7072 6f67 7261 6d20 6973 2066 7265 6520  program is free 
-00000050: 736f 6674 7761 7265 3b20 796f 7520 6361  software; you ca
-00000060: 6e20 7265 6469 7374 7269 6275 7465 2069  n redistribute i
-00000070: 7420 616e 642f 6f72 206d 6f64 6966 7920  t and/or modify 
-00000080: 6974 0a23 2075 6e64 6572 2074 6865 2074  it.# under the t
-00000090: 6572 6d73 206f 6620 7468 6520 474e 5520  erms of the GNU 
-000000a0: 4765 6e65 7261 6c20 5075 626c 6963 204c  General Public L
-000000b0: 6963 656e 7365 2061 7320 7075 626c 6973  icense as publis
-000000c0: 6865 6420 6279 2074 6865 0a23 2046 7265  hed by the.# Fre
-000000d0: 6520 536f 6674 7761 7265 2046 6f75 6e64  e Software Found
-000000e0: 6174 696f 6e3b 2065 6974 6865 7220 7665  ation; either ve
-000000f0: 7273 696f 6e20 3320 6f66 2074 6865 204c  rsion 3 of the L
-00000100: 6963 656e 7365 2c20 6f72 2028 6174 2079  icense, or (at y
-00000110: 6f75 720a 2320 6f70 7469 6f6e 2920 616e  our.# option) an
-00000120: 7920 6c61 7465 7220 7665 7273 696f 6e2e  y later version.
-00000130: 0a23 0a23 2054 6869 7320 7072 6f67 7261  .#.# This progra
-00000140: 6d20 6973 2064 6973 7472 6962 7574 6564  m is distributed
-00000150: 2069 6e20 7468 6520 686f 7065 2074 6861   in the hope tha
-00000160: 7420 6974 2077 696c 6c20 6265 2075 7365  t it will be use
-00000170: 6675 6c2c 2062 7574 0a23 2057 4954 484f  ful, but.# WITHO
-00000180: 5554 2041 4e59 2057 4152 5241 4e54 593b  UT ANY WARRANTY;
-00000190: 2077 6974 686f 7574 2065 7665 6e20 7468   without even th
-000001a0: 6520 696d 706c 6965 6420 7761 7272 616e  e implied warran
-000001b0: 7479 206f 660a 2320 4d45 5243 4841 4e54  ty of.# MERCHANT
-000001c0: 4142 494c 4954 5920 6f72 2046 4954 4e45  ABILITY or FITNE
-000001d0: 5353 2046 4f52 2041 2050 4152 5449 4355  SS FOR A PARTICU
-000001e0: 4c41 5220 5055 5250 4f53 452e 2020 5365  LAR PURPOSE.  Se
-000001f0: 6520 7468 6520 474e 5520 4765 6e65 7261  e the GNU Genera
-00000200: 6c0a 2320 5075 626c 6963 204c 6963 656e  l.# Public Licen
-00000210: 7365 2066 6f72 206d 6f72 6520 6465 7461  se for more deta
-00000220: 696c 732e 0a23 0a23 2059 6f75 2073 686f  ils..#.# You sho
-00000230: 756c 6420 6861 7665 2072 6563 6569 7665  uld have receive
-00000240: 6420 6120 636f 7079 206f 6620 7468 6520  d a copy of the 
-00000250: 474e 5520 4765 6e65 7261 6c20 5075 626c  GNU General Publ
-00000260: 6963 204c 6963 656e 7365 2061 6c6f 6e67  ic License along
-00000270: 0a23 2077 6974 6820 7468 6973 2070 726f  .# with this pro
-00000280: 6772 616d 3b20 6966 206e 6f74 2c20 7772  gram; if not, wr
-00000290: 6974 6520 746f 2074 6865 2046 7265 6520  ite to the Free 
-000002a0: 536f 6674 7761 7265 2046 6f75 6e64 6174  Software Foundat
-000002b0: 696f 6e2c 2049 6e63 2e2c 0a23 2035 3120  ion, Inc.,.# 51 
-000002c0: 4672 616e 6b6c 696e 2053 7472 6565 742c  Franklin Street,
-000002d0: 2046 6966 7468 2046 6c6f 6f72 2c20 426f   Fifth Floor, Bo
-000002e0: 7374 6f6e 2c20 4d41 2020 3032 3131 302d  ston, MA  02110-
-000002f0: 3133 3031 2c20 5553 412e 0a0a 6672 6f6d  1301, USA...from
-00000300: 2070 6573 756d 6d61 7279 2e75 7469 6c73   pesummary.utils
-00000310: 2e75 7469 6c73 2069 6d70 6f72 7420 6c6f  .utils import lo
-00000320: 6767 6572 0a66 726f 6d20 6173 7472 6f70  gger.from astrop
-00000330: 792e 7574 696c 7320 696d 706f 7274 2069  y.utils import i
-00000340: 6572 730a 0a0a 6465 6620 6368 6563 6b5f  ers...def check_
-00000350: 4945 5253 2829 3a0a 2020 2020 2222 2243  IERS():.    """C
-00000360: 6865 636b 2074 6861 7420 7468 6520 6c61  heck that the la
-00000370: 7465 7374 2049 4552 5320 6461 7461 2063  test IERS data c
-00000380: 616e 2062 6520 646f 776e 6c6f 6164 6564  an be downloaded
-00000390: 0a20 2020 2022 2222 0a20 2020 2074 7279  .    """.    try
-000003a0: 3a0a 2020 2020 2020 2020 6965 7273 2e63  :.        iers.c
-000003b0: 6f6e 662e 6175 746f 5f64 6f77 6e6c 6f61  onf.auto_downloa
-000003c0: 6420 3d20 5472 7565 0a20 2020 2020 2020  d = True.       
-000003d0: 2069 6572 735f 6120 3d20 6965 7273 2e49   iers_a = iers.I
-000003e0: 4552 535f 4175 746f 2e6f 7065 6e28 290a  ERS_Auto.open().
-000003f0: 2020 2020 6578 6365 7074 2045 7863 6570      except Excep
-00000400: 7469 6f6e 3a0a 2020 2020 2020 2020 6c6f  tion:.        lo
-00000410: 6767 6572 2e77 6172 6e69 6e67 2822 556e  gger.warning("Un
-00000420: 6162 6c65 2074 6f20 646f 776e 6c6f 6164  able to download
-00000430: 206c 6174 6573 7420 4945 5253 2064 6174   latest IERS dat
-00000440: 612e 2054 6865 2062 756e 646c 6564 2049  a. The bundled I
-00000450: 4552 532d 4220 220a 2020 2020 2020 2020  ERS-B ".        
-00000460: 2020 2020 2020 2020 2020 2020 2020 2022                 "
-00000470: 6461 7461 2077 6869 6368 2063 6f76 6572  data which cover
-00000480: 7320 7468 6520 7469 6d65 2072 616e 6765  s the time range
-00000490: 2066 726f 6d20 3139 3632 2074 6f20 6a75   from 1962 to ju
-000004a0: 7374 2062 6566 6f72 6520 220a 2020 2020  st before ".    
-000004b0: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000004c0: 2020 2022 7468 6520 6173 7472 6f70 7920     "the astropy 
-000004d0: 7265 6c65 6173 6520 6461 7420 7769 6c6c  release dat will
-000004e0: 2062 6520 7573 6564 2e20 416e 7920 7472   be used. Any tr
-000004f0: 616e 7366 6f72 6d61 7469 6f6e 7320 220a  ansformations ".
-00000500: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00000510: 2020 2020 2020 2022 6f75 7473 6964 6520         "outside 
-00000520: 6f66 2074 6869 7320 7261 6e67 6520 7769  of this range wi
-00000530: 6c6c 206e 6f74 2062 6520 616c 6c6f 7765  ll not be allowe
-00000540: 642e 2229 0a20 2020 2020 2020 2069 6572  d.").        ier
-00000550: 732e 636f 6e66 2e61 7574 6f5f 646f 776e  s.conf.auto_down
-00000560: 6c6f 6164 203d 2046 616c 7365 0a0a 0a63  load = False...c
-00000570: 6865 636b 5f49 4552 5328 290a            heck_IERS().
+00000000: 2f2f 2043 6f70 7972 6967 6874 2028 4329  // Copyright (C)
+00000010: 2032 3031 3820 2043 6861 726c 6965 2048   2018  Charlie H
+00000020: 6f79 203c 6368 6172 6c69 652e 686f 7940  oy <charlie.hoy@
+00000030: 6c69 676f 2e6f 7267 3e0a 2f2f 2054 6869  ligo.org>.// Thi
+00000040: 7320 7072 6f67 7261 6d20 6973 2066 7265  s program is fre
+00000050: 6520 736f 6674 7761 7265 3b20 796f 7520  e software; you 
+00000060: 6361 6e20 7265 6469 7374 7269 6275 7465  can redistribute
+00000070: 2069 7420 616e 642f 6f72 206d 6f64 6966   it and/or modif
+00000080: 7920 6974 0a2f 2f20 756e 6465 7220 7468  y it.// under th
+00000090: 6520 7465 726d 7320 6f66 2074 6865 2047  e terms of the G
+000000a0: 4e55 2047 656e 6572 616c 2050 7562 6c69  NU General Publi
+000000b0: 6320 4c69 6365 6e73 6520 6173 2070 7562  c License as pub
+000000c0: 6c69 7368 6564 2062 7920 7468 650a 2f2f  lished by the.//
+000000d0: 2046 7265 6520 536f 6674 7761 7265 2046   Free Software F
+000000e0: 6f75 6e64 6174 696f 6e3b 2065 6974 6865  oundation; eithe
+000000f0: 7220 7665 7273 696f 6e20 3320 6f66 2074  r version 3 of t
+00000100: 6865 204c 6963 656e 7365 2c20 6f72 2028  he License, or (
+00000110: 6174 2079 6f75 720a 2f2f 206f 7074 696f  at your.// optio
+00000120: 6e29 2061 6e79 206c 6174 6572 2076 6572  n) any later ver
+00000130: 7369 6f6e 2e0a 2f2f 0a2f 2f20 5468 6973  sion..//.// This
+00000140: 2070 726f 6772 616d 2069 7320 6469 7374   program is dist
+00000150: 7269 6275 7465 6420 696e 2074 6865 2068  ributed in the h
+00000160: 6f70 6520 7468 6174 2069 7420 7769 6c6c  ope that it will
+00000170: 2062 6520 7573 6566 756c 2c20 6275 740a   be useful, but.
+00000180: 2f2f 2057 4954 484f 5554 2041 4e59 2057  // WITHOUT ANY W
+00000190: 4152 5241 4e54 593b 2077 6974 686f 7574  ARRANTY; without
+000001a0: 2065 7665 6e20 7468 6520 696d 706c 6965   even the implie
+000001b0: 6420 7761 7272 616e 7479 206f 660a 2f2f  d warranty of.//
+000001c0: 204d 4552 4348 414e 5441 4249 4c49 5459   MERCHANTABILITY
+000001d0: 206f 7220 4649 544e 4553 5320 464f 5220   or FITNESS FOR 
+000001e0: 4120 5041 5254 4943 554c 4152 2050 5552  A PARTICULAR PUR
+000001f0: 504f 5345 2e20 2053 6565 2074 6865 2047  POSE.  See the G
+00000200: 4e55 2047 656e 6572 616c 0a2f 2f20 5075  NU General.// Pu
+00000210: 626c 6963 204c 6963 656e 7365 2066 6f72  blic License for
+00000220: 206d 6f72 6520 6465 7461 696c 732e 0a2f   more details../
+00000230: 2f0a 2f2f 2059 6f75 2073 686f 756c 6420  /.// You should 
+00000240: 6861 7665 2072 6563 6569 7665 6420 6120  have received a 
+00000250: 636f 7079 206f 6620 7468 6520 474e 5520  copy of the GNU 
+00000260: 4765 6e65 7261 6c20 5075 626c 6963 204c  General Public L
+00000270: 6963 656e 7365 2061 6c6f 6e67 0a2f 2f20  icense along.// 
+00000280: 7769 7468 2074 6869 7320 7072 6f67 7261  with this progra
+00000290: 6d3b 2069 6620 6e6f 742c 2077 7269 7465  m; if not, write
+000002a0: 2074 6f20 7468 6520 4672 6565 2053 6f66   to the Free Sof
+000002b0: 7477 6172 6520 466f 756e 6461 7469 6f6e  tware Foundation
+000002c0: 2c20 496e 632e 2c0a 2f2f 2035 3120 4672  , Inc.,.// 51 Fr
+000002d0: 616e 6b6c 696e 2053 7472 6565 742c 2046  anklin Street, F
+000002e0: 6966 7468 2046 6c6f 6f72 2c20 426f 7374  ifth Floor, Bost
+000002f0: 6f6e 2c20 4d41 2020 3032 3131 302d 3133  on, MA  02110-13
+00000300: 3031 2c20 5553 412e 0a0a 6675 6e63 7469  01, USA...functi
+00000310: 6f6e 2073 686f 775f 6578 7065 7274 5f64  on show_expert_d
+00000320: 6976 2829 7b0a 2020 7661 7220 6964 3d64  iv(){.  var id=d
+00000330: 6f63 756d 656e 742e 6765 7445 6c65 6d65  ocument.getEleme
+00000340: 6e74 4279 4964 2827 6578 7065 7274 5f64  ntById('expert_d
+00000350: 6976 2729 0a20 2076 6172 2076 6973 6962  iv').  var visib
+00000360: 696c 6974 793d 6964 2e73 7479 6c65 2e76  ility=id.style.v
+00000370: 6973 6962 696c 6974 790a 2020 6966 2028  isibility.  if (
+00000380: 2069 642e 7374 796c 652e 6469 7370 6c61   id.style.displa
+00000390: 7920 3d3d 2027 6e6f 6e65 2720 2920 7b0a  y == 'none' ) {.
+000003a0: 2020 2020 6964 2e73 7479 6c65 2e64 6973      id.style.dis
+000003b0: 706c 6179 203d 2027 626c 6f63 6b27 0a20  play = 'block'. 
+000003c0: 207d 2065 6c73 6520 7b0a 2020 2020 6964   } else {.    id
+000003d0: 2e73 7479 6c65 2e64 6973 706c 6179 203d  .style.display =
+000003e0: 2027 6e6f 6e65 273b 0a20 207d 0a7d 0a     'none';.  }.}.
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/skymap.py` & `pesummary-1.0.0/pesummary/gw/file/skymap.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,27 +1,16 @@
-# Copyright (C) 2020  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import numpy as np
 from pesummary.utils.utils import check_file_exists_and_rename, Empty
 from pesummary import conf
 from pesummary.utils.dict import Dict
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class SkyMapDict(Dict):
     """Class to handle a dictionary of skymaps
 
     Parameters
     ----------
     labels: list
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/nrutils.py` & `pesummary-1.0.0/pesummary/gw/conversions/nrutils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,30 +1,25 @@
-# Copyright (C) 2020  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import numpy as np
 from pesummary.utils.utils import logger
-from lalinference.imrtgr import nrutils
-import lalsimulation
-from lalsimulation import (
-    SimInspiralGetSpinFreqFromApproximant, SIM_INSPIRAL_SPINS_CASEBYCASE,
-    SIM_INSPIRAL_SPINS_FLOW
-)
+from pesummary.utils.decorators import bound_samples
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
+try:
+    from lalinference.imrtgr import nrutils
+    import lalsimulation
+    from lalsimulation import (
+        SimInspiralGetSpinFreqFromApproximant, SIM_INSPIRAL_SPINS_CASEBYCASE,
+        SIM_INSPIRAL_SPINS_FLOW
+    )
+except ImportError:
+    pass
+
 try:
     from lalsimulation.nrfits.eval_fits import eval_nrfit as _eval_nrfit
     NRSUR_MODULE = True
 except (ModuleNotFoundError, ImportError):
     NRSUR_MODULE = False
 
 LPEAK_FITS = ["UIB2016", "Healyetal"]
@@ -45,28 +40,30 @@
         float/array of masses for the primary object
     mass_2: float/np.ndarray
         float/array of masses for the secondary object
     """
     return nrutils.bbh_final_mass_non_spinning_Panetal(*args)
 
 
+@bound_samples(minimum=-1., maximum=1., logger_level="debug")
 def bbh_final_spin_non_spinning_Panetal(*args):
     """Return the final spin of the BH resulting from the merger of a non
     spinning BBH using the fit from Pan et al: Phys Rev D 84, 124052 (2011).
 
     Parameters
     ----------
     mass_1: float/np.ndarray
         float/array of masses for the primary object
     mass_2: float/np.ndarray
         float/array of masses for the secondary object
     """
     return nrutils.bbh_final_spin_non_spinning_Panetal(*args)
 
 
+@bound_samples(minimum=-1., maximum=1., logger_level="debug")
 def bbh_final_spin_non_precessing_Healyetal(*args, **kwargs):
     """Return the final spin of the BH resulting from the merger of a BBH for an
     aligned-spin system using the fit from Healy and Lousto: arXiv:1610.09713
 
     Parameters
     ----------
     mass_1: float/np.ndarray
@@ -128,14 +125,15 @@
         float/array of primary spin aligned with the orbital angular momentum
     spin_2z: float/np.ndarray
         float/array of secondary spin aligned with the orbital angular momentum
     """
     return nrutils.bbh_final_mass_non_precessing_Husaetal(*args)
 
 
+@bound_samples(minimum=-1., maximum=1., logger_level="debug")
 def bbh_final_spin_non_precessing_Husaetal(*args):
     """Return the final spin of the BH resulting from the merger of a BBH for an
     aligned-spin system using the fit from Husa et al: arXiv:1508.07250
 
     Parameters
     ----------
     mass_1: float/np.ndarray
@@ -166,14 +164,15 @@
         float/array of secondary spin aligned with the orbital angular momentum
     version: str, optional
         version of the fitting coefficients you wish to use
     """
     return nrutils.bbh_final_mass_non_precessing_UIB2016(*args, **kwargs)
 
 
+@bound_samples(minimum=-1., maximum=1., logger_level="debug")
 def bbh_final_spin_non_precessing_UIB2016(*args, **kwargs):
     """Return the final spin of the BH resulting from the merger of a BBH for an
     aligned-spin system using the fit from https://arxiv.org/abs/1611.00332
 
     Parameters
     ----------
     mass_1: float/np.ndarray
@@ -186,14 +185,15 @@
         float/array of secondary spin aligned with the orbital angular momentum
     version: str, optional
         version of the fitting coefficients you wish to use
     """
     return nrutils.bbh_final_spin_non_precessing_UIB2016(*args, **kwargs)
 
 
+@bound_samples(minimum=-1., maximum=1., logger_level="debug")
 def bbh_final_spin_non_precessing_HBR2016(*args, **kwargs):
     """Return the final spin of the BH resulting from the merger of a BBH for an
     aligned-spin system using the fit from Hofmann, Barausse, and Rezzolla
     ApJL 825, L19 (2016)
 
     Parameters
     ----------
@@ -207,14 +207,15 @@
         float/array of secondary spin aligned with the orbital angular momentum
     version: str, optional
         version of the fitting coefficients you wish to use
     """
     return nrutils.bbh_final_spin_non_precessing_HBR2016(*args, **kwargs)
 
 
+@bound_samples(maximum=1., logger_level="debug")
 def _bbh_final_spin_precessing_using_non_precessing_fit(
     mass_1, mass_2, spin_1z, spin_2z, fit
 ):
     """Return the final spin of a BH results from the merger of a BH for a
     precessing system using non_precessing fits
 
     Parameters
@@ -290,14 +291,15 @@
         float/array of primary spin aligned with the orbital angular momentum
     spin_2z: float/np.ndarray
         float/array of secondary spin aligned with the orbital angular momentum
     """
     return _bbh_final_spin_precessing_using_non_precessing_fit(*args, "UIB2016")
 
 
+@bound_samples(maximum=1., logger_level="debug")
 def _bbh_final_spin_precessing_projected(
     mass_1, mass_2, a_1, a_2, tilt_1, tilt_2, phi_12, function=None
 ):
     """Project the precessing spins along the orbital angular momentum and
     calculate the final spin of the BH with an aligned-spin fit from the
     literature augmenting it with the leading contribution from the in-plane
     spins
@@ -391,14 +393,15 @@
         components
     """
     return _bbh_final_spin_precessing_projected(
         *args, function=bbh_final_spin_precessing_UIB2016
     )
 
 
+@bound_samples(maximum=1., logger_level="debug")
 def bbh_final_spin_precessing_HBR2016(*args, **kwargs):
     """Return the final spin of the BH resulting from the merger of a BBH for a
     precessing system using the fit from Hofmann, Barausse, and Rezzolla ApJL
     825, L19 (2016)
 
     Parameters
     ----------
@@ -753,17 +756,16 @@
     approximant: str, optional
         The approximant that was used to generate the posterior samples
     kwargs: dict, optional
         optional kwargs that are passed directly to the
         `lalsimulation.nrfits.eval_fits.eval_nrfit` function
     """
     from lal import MSUN_SI, C_SI
-    from pesummary.gw.file.conversions import (
-        component_spins, magnitude_from_vector
-    )
+    from .spins import component_spins
+    from .utils import magnitude_from_vector
     from pesummary.utils.utils import iterator
     import copy
 
     if not NRSUR_MODULE:
         raise ImportError(
             "Unable to import `lalsimulation.nrfits.eval_fits`. This is likely "
             "due to the installed version of lalsimulation. Please update."
@@ -787,24 +789,38 @@
             f_ref, phi_ref
         )
     )
     a_1_vec = np.array([spins.T[1], spins.T[2], spins.T[3]]).T
     a_2_vec = np.array([spins.T[4], spins.T[5], spins.T[6]]).T
     mass_1 *= MSUN_SI
     mass_2 *= MSUN_SI
-    _fits = [
-        eval_nrfit(
-            mass_1[num], mass_2[num], a_1_vec[num], a_2_vec[num], model, converted_fits,
-            f_low=f_low, f_ref=f_ref[num], approximant=approximant,
-            extra_params_dict=kwargs
-        ) for num in iterator(
-            range(len(mass_1)), desc=description, tqdm=True, total=len(mass_1),
-            logger=logger
+    try:
+        _fits = [
+            eval_nrfit(
+                mass_1[num], mass_2[num], a_1_vec[num], a_2_vec[num], model,
+                converted_fits, f_low=f_low, f_ref=f_ref[num],
+                approximant=approximant, extra_params_dict=kwargs
+            ) for num in iterator(
+                range(len(mass_1)), desc=description, tqdm=True, total=len(mass_1),
+                logger=logger
+            )
+        ]
+    except ValueError as e:
+        base = (
+            "Failed to generate remnant quantities with the NRSurrogate "
+            "remnant model. {}"
         )
-    ]
+        if "symbol not found" in str(e):
+            raise NameError(
+                base.format(
+                    "This could be because the 'LAL_DATA_PATH' has not been "
+                    "set."
+                )
+            )
+        raise ValueError(base.format(""))
     nr_fits = {key: np.array([dic[key] for dic in _fits]) for key in _fits[0]}
     if fits_map["final_mass"] in nr_fits.keys():
         nr_fits[fits_map["final_mass"]] = np.array(
             [final_mass[0] for final_mass in nr_fits[fits_map["final_mass"]]]
         ) / MSUN_SI
     if fits_map["final_kick"] in nr_fits.keys():
         nr_fits[fits_map["final_kick"]] *= C_SI / 1000
```

### Comparing `pesummary-0.9.1/pesummary/gw/file/standard_names.py` & `pesummary-1.0.0/pesummary/gw/file/standard_names.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,22 +1,10 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 _IFOS = ["H1", "L1", "V1", "K1", "E1"]
 tidal_params = ["lambda_1", "lambda_2", "delta_lambda", "lambda_tilde"]
 
 
 lalinference_map = {
     "logl": "log_likelihood",
     "logprior": "log_prior",
@@ -45,14 +33,15 @@
     "mtotal_source": "total_mass_source",
     "mc_source": "chirp_mass_source",
     "phi1": "phi_1",
     "phi2": "phi_2",
     "costilt1": "cos_tilt_1",
     "costilt2": "cos_tilt_2",
     "costheta_jn": "cos_theta_jn",
+    "cosiota": "cos_iota",
     "lambda1": "lambda_1",
     "lambda2": "lambda_2",
     "lambdaT": "lambda_tilde",
     "dLambdaT": "delta_lambda",
     "logp1": "log_pressure",
     "gamma1": "gamma_1",
     "gamma2": "gamma_2",
@@ -71,14 +60,15 @@
     "mf_source_nonevol": "final_mass_source_non_evolved",
     "af_nonevol": "final_spin_non_evolved",
     "af_evol_avg": "final_spin",
     "l_peak_evol_avg": "peak_luminosity",
     "l_peak_nonevol": "peak_luminosity_non_evolved",
     "e_rad_nonevol": "radiated_energy_non_evolved",
     "e_rad_evol_avg": "radiated_energy",
+    "beta": "beta"
 }
 
 
 for detector in _IFOS:
     lalinference_map["{}_cplx_snr_amp".format(detector.lower())] = (
         "{}_matched_filter_abs_snr".format(detector)
     )
@@ -148,32 +138,77 @@
         "{}_optimal_snr".format(detector)
     )
 
 
 pycbc_map = {
     "mchirp": "chirp_mass",
     "coa_phase": "phase",
+    "loglikelihood": "log_likelihood",
 }
 
 
 pesummary_map = {
+    "network_21_multipole_snr": "network_21_multipole_snr",
+    "network_33_multipole_snr": "network_33_multipole_snr",
+    "network_44_multipole_snr": "network_44_multipole_snr",
+    "network_precessing_snr": "network_precessing_snr",
     "chirp_mass_source": "chirp_mass_source",
     "delta_lambda": "delta_lambda",
+    "viewing_angle": "viewing_angle",
+    "tilt_1_infinity": "tilt_1_infinity",
+    "spin_1z_infinity": "spin_1z_infinity",
+    "spin_1z_infinity_only_prec_avg": "spin_1z_infinity_only_prec_avg",
+    "tilt_2_infinity": "tilt_2_infinity",
+    "spin_2z_infinity": "spin_2z_infinity",
+    "spin_2z_infinity_only_prec_avg": "spin_2z_infinity_only_prec_avg",
+    "tilt_1_infinity_only_prec_avg": "tilt_1_infinity_only_prec_avg",
+    "tilt_2_infinity_only_prec_avg": "tilt_2_infinity_only_prec_avg",
+    "chi_eff_infinity": "chi_eff_infinity",
+    "chi_eff_infinity_only_prec_avg": "chi_eff_infinity_only_prec_avg",
+    "chi_p_infinity": "chi_p_infinity",
+    "chi_p_infinity_only_prec_avg": "chi_p_infinity_only_prec_avg",
+    "cos_tilt_1_infinity": "cos_tilt_1_infinity",
+    "cos_tilt_2_infinity": "cos_tilt_2_infinity",
+    "cos_tilt_1_infinity_only_prec_avg": "cos_tilt_1_infinity_only_prec_avg",
+    "cos_tilt_2_infinity_only_prec_avg": "cos_tilt_2_infinity_only_prec_avg",
     "spin_1z": "spin_1z",
     "spin_2z": "spin_2z",
+    "chi_p_2spin": "chi_p_2spin",
     "peak_luminosity": "peak_luminosity",
     "peak_luminosity_non_evolved": "peak_luminosity_non_evolved",
     "final_mass": "final_mass",
     "final_mass_non_evolved": "final_mass_non_evolved",
     "final_spin": "final_spin",
     "final_spin_non_evolved": "final_spin_non_evolved",
     "radiated_energy": "radiated_energy",
     "radiated_energy_non_evolved": "radiated_energy_non_evolved",
     "weights": "weights",
-    "final_kick": "final_kick"
+    "psi_J": "psi_J",
+    "polarization_J": "psi_J",
+    "opening_angle": "beta",
+    "beta0": "beta",
+    "rho_21": "network_21_multipole_snr",
+    "network_rho_21_perp": "network_21_multipole_snr",
+    "rho_33": "network_33_multipole_snr",
+    "network_rho_33_perp": "network_33_multipole_snr",
+    "rho_44": "network_44_multipole_snr",
+    "network_rho_44_perp": "network_44_multipole_snr",
+    "rho_p": "network_precessing_snr",
+    "final_kick": "final_kick",
+    "tidal_disruption_frequency": "tidal_disruption_frequency",
+    "tidal_disruption_frequency_ratio": "tidal_disruption_frequency_ratio",
+    "220_quasinormal_mode_frequency": "220_quasinormal_mode_frequency",
+    "baryonic_torus_mass": "baryonic_torus_mass",
+    "baryonic_torus_mass_source": "baryonic_torus_mass_source",
+    "compactness_1": "compactness_1",
+    "compactness_2": "compactness_2",
+    "baryonic_mass_1": "baryonic_mass_1",
+    "baryonic_mass_1_source": "baryonic_mass_1_source",
+    "baryonic_mass_2": "baryonic_mass_2",
+    "baryonic_mass_2_source": "baryonic_mass_2_source"
 }
 
 
 for detector in _IFOS:
     pesummary_map["{}_matched_filter_snr".format(detector)] = (
         "{}_matched_filter_snr".format(detector)
     )
@@ -226,14 +261,16 @@
     "spin2": "a_2",
     "spin2_a": "a_2",
     "a2x": "spin_2x",
     "a2y": "spin_2y",
     "spin2x": "spin_2x",
     "spin2y": "spin_2y",
     "spin2z": "spin_2z",
+    "theta1": "tilt_1",
+    "theta2": "tilt_2",
     "phiorb": "phase",
     "phi0": "phase",
     "distance": "luminosity_distance",
     "luminosity_distance_Mpc": "luminosity_distance",
     "chirpmass": "chirp_mass",
     "tc": "geocent_time",
     "geocent_end_time": "geocent_time",
@@ -292,37 +329,78 @@
         "{}_matched_filter_snr_angle".format(detector)
     )
 
 
 standard_names = {}
 standard_names.update(lalinference_map)
 standard_names.update(bilby_map)
+standard_names.update(pycbc_map)
 standard_names.update(other_map)
 
-
 descriptive_names = {
     "log_likelihood": (
         "the logarithm of the likelihood"
     ),
     "tilt_1": (
-        "the zenith angle between the total orbital angular momentum, L, and "
+        "the zenith angle between the Newtonian orbital angular momentum, L, and "
         "the primary spin, S1"
     ),
     "tilt_2": (
-        "the zenith angle between the total orbital angular momentum, L, and "
+        "the zenith angle between the Newtonian orbital angular momentum, L, and "
         "the secondary spin, S2"
     ),
+    "tilt_1_infinity_only_prec_avg": (
+        "the zenith angle between the Newtonian orbital angular momentum, L, and "
+        "the primary spin, S1, defined at infinite binary separation computed "
+        "using only the precession-averaged approximation"
+    ),
+    "tilt_2_infinity_only_prec_avg": (
+        "the zenith angle between the Newtonian orbital angular momentum, L, and "
+        "the secondary spin, S2, defined at infinite binary separation computed "
+        "using only the precession-averaged approximation"
+    ),
+    "tilt_1_infinity": (
+        "the zenith angle between the Newtonian orbital angular momentum, L, and "
+        "the primary spin, S1, defined at infinite binary separation"
+    ),
+    "tilt_2_infinity": (
+        "the zenith angle between the Newtonian orbital angular momentum, L, and "
+        "the secondary spin, S2, defined at infinite binary separation"
+    ),
     "cos_tilt_1": (
-        "the cosine of the zenith angle between the total orbital angular "
+        "the cosine of the zenith angle between the Newtonian orbital angular momentum "
         "momentum, L, and the primary spin, S1"
     ),
     "cos_tilt_2": (
-        "the cosine of the zenith angle between the total orbital angular "
+        "the cosine of the zenith angle between the Newtonian orbital angular momentum "
         "momentum, L, and the secondary spin, S2"
     ),
+    "cos_tilt_1_infinity": (
+        "the cosine of the zenith angle between the Newtonian orbital angular momentum "
+        "momentum, L, and the primary spin, S1, defined at infinite binary separation"
+    ),
+    "cos_tilt_2_infinity": (
+        "the cosine of the zenith angle between the Newtonian orbital angular momentum "
+        "momentum, L, and the secondary spin, S2, defined at infinite binary separation"
+    ),
+    "cos_tilt_1_infinity_only_prec_avg": (
+        "the cosine of the zenith angle between the Newtonian orbital angular momentum "
+        "momentum, L, and the primary spin, S1, defined at infinite binary separation "
+        "computed using only the precession-averaged approximation"
+    ),
+    "cos_tilt_2_infinity_only_prec_avg": (
+        "the cosine of the zenith angle between the Newtonian orbital angular momentum "
+        "momentum, L, and the secondary spin, S2, defined at infinite binary separation "
+        "computed using only the precession-averaged approximation"
+    ),
+    "beta": (
+        "the zenith angle between the total orbital angular momentum, L, and "
+        "the total angular momentum J. For a non-precessing system, beta is "
+        "zero by definition"
+    ),
     "redshift": (
         "the redshift depending on specified cosmology"
     ),
     "network_optimal_snr": (
         "the optimal signal to noise ratio in the gravitational wave detector "
         "network"
     ),
@@ -370,62 +448,98 @@
         "the azimuthal angle of the spin vector of the secondary object"
     ),
     "psi": (
         "the polarization angle of the source"
     ),
     "phi_12": (
         "the difference between the azimuthal angles of the individual spin "
-        "vectors of the primary and secondary objects"
+        "vectors of the primary and secondary object's"
     ),
     "phi_jl": (
         "the difference between total and orbital angular momentum azimuthal "
         "angles"
     ),
     "a_1": (
         "the dimensionless spin magnitude of the primary object"
     ),
     "spin_1x": (
-        "the xth component of the primary objects spin in Euclidean coordinates"
+        "the x-component of the primary object's spin in Euclidean coordinates"
     ),
     "spin_1y": (
-        "the yth component of the primary objects spin in Euclidean coordinates"
+        "the y-component of the primary object's spin in Euclidean coordinates"
     ),
     "spin_1z": (
-        "the zth component of the primary objects spin in Euclidean coordinates"
+        "the z-component of the primary object's spin in Euclidean coordinates"
+    ),
+    "spin_1z_infinity": (
+        "the z-component of the primary object's spin in Euclidean coordinates "
+        "defined at infinite binary separation"
+    ),
+    "spin_1z_infinity_only_prec_avg": (
+        "the z-component of the primary object's spin in Euclidean coordinates "
+        "defined at infinite binary separation computed using only the "
+        "precession-averaged approximation"
     ),
     "a_2": (
         "the dimensionless spin magnitude of the secondary object"
     ),
     "spin_2x": (
-        "the xth component of the secondary objects spin in Euclidean "
+        "the x-component of the secondary object's spin in Euclidean "
         "coordinates"
     ),
     "spin_2y": (
-        "the yth component of the secondary objects spin in Euclidean "
+        "the y-component of the secondary object's spin in Euclidean "
         "coordinates"
     ),
     "spin_2z": (
-        "the zth component of the secondary objects spin in Euclidean "
+        "the z-component of the secondary object's spin in Euclidean "
         "coordinates"
     ),
+    "spin_2z_infinity": (
+        "the z-component of the secondary object's spin in Euclidean coordinates "
+        "defined at infinite binary separation"
+    ),
+    "spin_2z_infinity_only_prec_avg": (
+        "the z-component of the secondary object's spin in Euclidean coordinates "
+        "defined at infinite binary separation computed using only the "
+        "precession-averaged approximation"
+    ),
     "chi_p": (
         "the effective precession spin parameter"
     ),
+    "chi_p_infinity": (
+        "the effective precession spin parameter defined at infinite binary separation"
+    ),
+    "chi_p_infinity_only_prec_avg": (
+        "the effective precession spin parameter defined at infinite binary separation "
+        "computed using only the precession-averaged approximation"
+    ),
+    "chi_p_2spin": (
+        "a modified effective precession spin parameter accounting for "
+        "precessing spin information from both compact objects."
+    ),
     "phase": (
         "the binary phase defined at a given reference frequency"
     ),
     "luminosity_distance": (
         "the luminosity distance of the source"
     ),
     "chirp_mass": (
         "the detector-frame chirp mass"
     ),
     "chi_eff": (
         "the effective inspiral spin parameter"
     ),
+    "chi_eff_infinity": (
+        "the effective inspiral spin parameter defined at infinite binary separation"
+    ),
+    "chi_eff_infinity_only_prec_avg": (
+        "the effective inspiral spin parameter defined at infinite binary separation "
+        "computed using only the precession-averaged approximation"
+    ),
     "total_mass_source": (
         "the source-frame combined mass of the primary and secondary masses "
     ),
     "total_mass": (
         "the detector-frame combined mass of the primary and secondary masses "
     ),
     "mass_ratio": (
@@ -517,16 +631,16 @@
         "the ISCO frequency"
     ),
     "final_mass_source": (
         "the source-frame remnant mass estimated using the spins evolved to "
         "the ISCO frequency"
     ),
     "final_mass_non_evolved": (
-        "the detector-frame remnant mass estimated using the spins evolved to "
-        "the ISCO frequency"
+        "the detector-frame remnant mass estimated using the spins defined at "
+        "the reference frequency"
     ),
     "final_mass_source_non_evolved": (
         "the source-frame remnant mass estimated using the spins defined at "
         "the reference frequency"
     ),
     "final_spin": (
         "the spin of the remnant object estimated using the spins evolved to "
@@ -542,14 +656,65 @@
         "mass was estimated using the spins evolved at the ISCO frequency"
     ),
     "radiated_energy_non_evolved": (
         "the energy radiated in gravitational waves. Defined as the difference "
         "between the source total and source remant mass. The source remnant "
         "mass was estimated using the spins defined at the reference frequency"
     ),
+    "tidal_disruption_frequency": (
+        "the gravitational wave detector-frame frequency at which tidal forces "
+        "dominate over the self-gravity forces, invoking mass shedding"
+    ),
+    "tidal_disruption_frequency_ratio": (
+        "the ratio of the tidal disruption and the 220 quasinormal mode "
+        "frequency of the system. In NSBH models this ratio describes whether the "
+        "system is disruptive or non-disruptive. If the ratio is less than 1, the "
+        "system is characterised as either mildly disruptive or disruptive. If the ratio "
+        "is greater than 1, the system is characterised as non-disruptive meaning "
+        "the secondary object remains intact as it plunges into the primary."
+    ),
+    "220_quasinormal_mode_frequency": (
+        "the detector-frame 220 quasinormal mode (QNM) frequency of the "
+        "remnant object"
+    ),
+    "baryonic_torus_mass": (
+        "the detector-frame (redshifted) baryonic mass of the torus formed "
+        "around the primary object. If the baryonic torus mass is 0, the system "
+        "is characterised as either mildly disruptive or non-disruptive."
+    ),
+    "baryonic_torus_mass_source": (
+        "the source-frame baryonic mass of the torus formed around the primary "
+        "object"
+    ),
+    "compactness_1": "the compactness of the primary object",
+    "compactness_2": "the compactness of the secondary object",
+    "baryonic_mass_1": (
+        "the detector-frame (redshifted) baryonic mass of the primary object"
+    ),
+    "baryonic_mass_1_source": (
+        "the source-frame baryonic mass of the primary object"
+    ),
+    "baryonic_mass_2": (
+        "the detector-frame (redshifted) baryonic mass of the secondary object"
+    ),
+    "baryonic_mass_2_source": (
+        "the source-frame baryonic mass of the secondary object"
+    ),
+    "network_21_multipole_snr": (
+        "the network SNR in the 21 subdominant multipole when assuming that the "
+        "system is non-precessing"
+    ),
+    "network_33_multipole_snr": (
+        "the network SNR in the 33 subdominant multipole when assuming that the "
+        "system is non-precessing"
+    ),
+    "network_44_multipole_snr": (
+        "the network SNR in the 44 subdominant multipole when assuming that the "
+        "system is non-precessing"
+    )
 }
 
 for detector in _IFOS:
     descriptive_names["{}_optimal_snr".format(detector)] = (
         "the optimal signal to noise ratio in the %s gravitational wave "
         "detector" % (detector)
     )
```

### Comparing `pesummary-0.9.1/pesummary/gw/gracedb.py` & `pesummary-1.0.0/pesummary/gw/gracedb.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,26 +1,16 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org> This program is free
-# software; you can redistribute it and/or modify it under the terms of the GNU
-# General Public License as published by the Free Software Foundation; either
-# version 3 of the License, or (at your option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but WITHOUT
-# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
-# FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
-# details.
-#
-# You should have received a copy of the GNU General Public License along with
-# this program; if not, write to the Free Software Foundation, Inc., 51
-# Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from ligo.gracedb.rest import GraceDb
 from ligo.gracedb.exceptions import HTTPError
 from pesummary.utils.utils import logger
 from pesummary import conf
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 def get_gracedb_data(
     gracedb_id, superevent=False, info=None, json=None,
     service_url=conf.gracedb_server
 ):
     """Grab data from GraceDB for a specific event.
```

### Comparing `pesummary-0.9.1/pesummary/gw/notebook/public.py` & `pesummary-1.0.0/pesummary/gw/notebook/public.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,30 +1,18 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
-from pesummary import __version__
 from pesummary.io import read
 from pesummary.core.notebook import (
     NoteBook, imports, pesummary_read, posterior_samples,
     samples_dict_plot
 )
 from .notebook import psd_plot
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 def make_public_notebook(
     pesummary_file, publication_title, dcc_link=".", event="",
     default_analysis=None, default_parameter="mass_1",
     corner_parameters=["mass_1", "mass_2", "luminosity_distance", "iota"],
     filename="posterior_samples.ipynb", outdir="./", comparison_analysis=None
 ):
```

### Comparing `pesummary-0.9.1/pesummary/gw/plots/public.py` & `pesummary-1.0.0/pesummary/gw/plots/public.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,29 +1,16 @@
 #! /usr/bin/env python
 
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from pesummary.core.plots.latex_labels import latex_labels
 from .latex_labels import GWlatex_labels, public_GWlatex_labels
 from .main import _PlotGeneration as _GWPlotGeneration
 
-
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 latex_labels.update(GWlatex_labels)
 latex_labels.update(public_GWlatex_labels)
 
 
 class _PlotGeneration(_GWPlotGeneration):
     def __init__(
         self, savedir=None, webdir=None, labels=None, samples=None,
@@ -35,15 +22,16 @@
         nsamples_for_skymap=None, detectors=None, maxL_samples=None,
         gwdata=None, calibration=None, psd=None,
         multi_threading_for_skymap=None, approximant=None,
         pepredicates_probs=None, include_prior=False, publication=False,
         existing_approximant=None, existing_psd=None, existing_calibration=None,
         existing_weights=None, weights=None, disable_comparison=False,
         linestyles=None, disable_interactive=False, disable_corner=False,
-        publication_kwargs={}, multi_process=1, corner_params=None
+        publication_kwargs={}, multi_process=1, corner_params=None,
+        preliminary_pages=False, expert_plots=False, checkpoint=False
     ):
         super(_PlotGeneration, self).__init__(
             savedir=savedir, webdir=webdir, labels=labels,
             samples=samples, kde_plot=kde_plot, existing_labels=existing_labels,
             existing_injection_data=existing_injection_data,
             existing_file_kwargs=existing_file_kwargs,
             existing_samples=existing_samples,
@@ -63,9 +51,11 @@
             existing_psd=existing_psd,
             existing_calibration=existing_calibration,
             existing_weights=existing_weights, weights=weights,
             disable_comparison=disable_comparison, linestyles=linestyles,
             disable_interactive=disable_interactive,
             disable_corner=disable_corner,
             publication_kwargs=publication_kwargs,
-            multi_process=multi_process, corner_params=corner_params
+            multi_process=multi_process, corner_params=corner_params,
+            preliminary_pages=preliminary_pages, expert_plots=expert_plots,
+            checkpoint=checkpoint
         )
```

### Comparing `pesummary-0.9.1/pesummary/gw/plots/plot.py` & `pesummary-1.0.0/pesummary/gw/plots/plot.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,48 +1,30 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from pesummary.utils.utils import (
     logger, number_of_columns_for_legend, _check_latex_install,
-    get_matplotlib_style_file
 )
 from pesummary.utils.decorators import no_latex_plot
 from pesummary.gw.plots.bounds import default_bounds
-from pesummary.core.plots.kde import kdeplot
 from pesummary.core.plots.figure import figure, subplots, ExistingFigure
 from pesummary.core.plots.plot import _default_legend_kwargs
 from pesummary import conf
 
 import os
 import matplotlib.style
-import matplotlib.lines as mlines
-import corner
 import numpy as np
 import math
 from scipy.ndimage import gaussian_filter
 from astropy.time import Time
-from gwpy.timeseries import TimeSeries
-from gwpy.plot.colors import GW_OBSERVATORY_COLORS
 
-matplotlib.style.use(get_matplotlib_style_file())
 _check_latex_install()
 
 from lal import MSUN_SI, PC_SI
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 try:
     import lalsimulation as lalsim
     LALSIMULATION = True
 except ImportError:
     LALSIMULATION = None
 
 
@@ -70,121 +52,162 @@
                 else:
                     xhigh = np.max(samples)
             else:
                 xhigh = bounds["high"]
     return xlow, xhigh
 
 
-def _1d_histogram_plot(param, samples, latex_label, inj_value=None, kde=False,
-                       prior=None, weights=None, bins=50, grid=True):
-    """Generate the 1d histogram plot for a given parameter for a given
-    approximant.
+def _add_default_bounds_to_kde_kwargs_dict(
+    kde_kwargs, param, samples, comparison=False
+):
+    """Add default kde bounds to the a dictionary of kwargs
 
     Parameters
     ----------
+    kde_kwargs: dict
+        dictionary of kwargs to pass to the kde class
     param: str
-        name of the parameter that you wish to plot
+        name of the parameter you wish to plot
     samples: list
         list of samples for param
-    latex_label: str
-        latex label for param
-    inj_value: float
-        value that was injected
-    kde: Bool
-        if true, a kde is plotted instead of a histogram
-    prior: list
-        list of prior samples for param
-    weights: list
-        list of weights for each sample
-    bins: int, optional
-        number of bins to use for histogram
-    grid: Bool, optional
-        if True, plot a grid
+    """
+    from pesummary.core.plots.bounded_1d_kde import bounded_1d_kde
+
+    xlow, xhigh = _return_bounds(param, samples, comparison=comparison)
+    kde_kwargs["xlow"] = xlow
+    kde_kwargs["xhigh"] = xhigh
+    kde_kwargs["kde_kernel"] = bounded_1d_kde
+    return kde_kwargs
+
+
+def _1d_histogram_plot(
+    param, samples, *args, kde_kwargs={}, bounded=True, **kwargs
+):
+    """Generate the 1d histogram plot for a given parameter for a given
+    approximant.
+
+    Parameters
+    ----------
+    *args: tuple
+        all args passed directly to pesummary.core.plots.plot._1d_histogram_plot
+        function
+    kde_kwargs: dict, optional
+        optional kwargs passed to the kde class
+    bounded: Bool, optional
+        if True, pass default 'xlow' and 'xhigh' arguments to the kde class
+    **kwargs: dict, optional
+        all additional kwargs passed to the
+        pesummary.core.plots.plot._1d_histogram_plot function
     """
     from pesummary.core.plots.plot import _1d_histogram_plot
 
-    xlow, xhigh = _return_bounds(param, samples)
+    if bounded:
+        kde_kwargs = _add_default_bounds_to_kde_kwargs_dict(
+            kde_kwargs, param, samples
+        )
     return _1d_histogram_plot(
-        param, samples, latex_label, inj_value=inj_value, kde=kde, prior=prior,
-        weights=weights, xlow=xlow, xhigh=xhigh, bins=bins, grid=grid
+        param, samples, *args, kde_kwargs=kde_kwargs, **kwargs
     )
 
 
 def _1d_histogram_plot_mcmc(
-    param, samples, latex_label, inj_value=None, kde=False, prior=None,
-    weights=None, grid=True
+    param, samples, *args, kde_kwargs={}, bounded=True, **kwargs
 ):
     """Generate the 1d histogram plot for a given parameter for set of
     mcmc chains
 
     Parameters
     ----------
-    param: str
-        name of the parameter that you wish to plot
-    samples: np.ndarray
-        2d array of samples for param for each mcmc chain
-    latex_label: str
-        latex label for param
-    inj_value: float
-        value that was injected
-    kde: Bool
-        if true, a kde is plotted instead of a histogram
-    prior: list
-        list of prior samples for param
-    weights: list
-        list of weights for each sample
-    grid: Bool, optional
-        if True, plot a grid
+    *args: tuple
+        all args passed directly to
+        pesummary.core.plots.plot._1d_histogram_plot_mcmc function
+    kde_kwargs: dict, optional
+        optional kwargs passed to the kde class
+    bounded: Bool, optional
+        if True, pass default 'xlow' and 'xhigh' arguments to the kde class
+    **kwargs: dict, optional
+        all additional kwargs passed to the
+        pesummary.core.plots.plot._1d_histogram_plot_mcmc function
     """
     from pesummary.core.plots.plot import _1d_histogram_plot_mcmc
 
-    xlow, xhigh = _return_bounds(param, samples, comparison=True)
+    if bounded:
+        kde_kwargs = _add_default_bounds_to_kde_kwargs_dict(
+            kde_kwargs, param, samples, comparison=True
+        )
     return _1d_histogram_plot_mcmc(
-        param, samples, latex_label, inj_value=inj_value, kde=kde, prior=prior,
-        weights=weights, xlow=xlow, xhigh=xhigh, grid=grid
+        param, samples, *args, kde_kwargs=kde_kwargs, **kwargs
     )
 
 
-def _1d_comparison_histogram_plot(param, samples, colors,
-                                  latex_label, labels, kde=False,
-                                  linestyles=None, max_vline=2, grid=True,
-                                  legend_kwargs=_default_legend_kwargs):
-    """Generate the a plot to compare the 1d_histogram plots for a given
-    parameter for different approximants.
+def _1d_histogram_plot_bootstrap(
+    param, samples, *args, kde_kwargs={}, bounded=True, **kwargs
+):
+    """Generate a bootstrapped 1d histogram plot for a given parameter
 
     Parameters
     ----------
     param: str
         name of the parameter that you wish to plot
-    approximants: list
-        list of approximant names that you would like to compare
-    samples: 2d list
-        list of samples for param for each approximant
-    colors: list
-        list of colors to be used to differentiate the different approximants
-    latex_label: str
-        latex label for param
-    approximant_labels: list, optional
-        label to prepend the approximant in the legend
-    kde: Bool
-        if true, a kde is plotted instead of a histogram
-    linestyles: list
-        list of linestyles for each set of samples
-    grid: Bool, optional
-        if True, plot a grid
-    legend_kwargs: dict, optional
-        optional kwargs to pass to ax.legend()
+    samples: np.ndarray
+        array of samples for param
+    args: tuple
+        all args passed to
+        pesummary.core.plots.plot._1d_histogram_plot_bootstrap function
+    kde_kwargs: dict, optional
+        optional kwargs passed to the kde class
+    bounded: Bool, optional
+        if True, pass default 'xlow' and 'xhigh' arguments to the kde class
+    **kwargs: dict, optional
+        all additional kwargs passed to the
+        pesummary.core.plots.plot._1d_histogram_plot_bootstrap function
+    """
+    from pesummary.core.plots.plot import _1d_histogram_plot_bootstrap
+
+    if bounded:
+        kde_kwargs = _add_default_bounds_to_kde_kwargs_dict(
+            kde_kwargs, param, samples
+        )
+    return _1d_histogram_plot_bootstrap(
+        param, samples, *args, kde_kwargs=kde_kwargs, **kwargs
+    )
+
+
+def _1d_comparison_histogram_plot(
+    param, samples, *args, kde_kwargs={}, bounded=True, max_vline=2,
+    legend_kwargs=_default_legend_kwargs, **kwargs
+):
+    """Generate the a plot to compare the 1d_histogram plots for a given
+    parameter for different approximants.
+
+    Parameters
+    ----------
+    *args: tuple
+        all args passed directly to
+        pesummary.core.plots.plot._1d_comparisonhistogram_plot function
+    kde_kwargs: dict, optional
+        optional kwargs passed to the kde class
+    bounded: Bool, optional
+        if True, pass default 'xlow' and 'xhigh' arguments to the kde class
+    max_vline: int, optional
+        if number of peaks < max_vline draw peaks as vertical lines rather
+        than histogramming the data
+    **kwargs: dict, optional
+        all additional kwargs passed to the
+        pesummary.core.plots.plot._1d_comparison_histogram_plot function
     """
     from pesummary.core.plots.plot import _1d_comparison_histogram_plot
 
-    xlow, xhigh = _return_bounds(param, samples, comparison=True)
+    if bounded:
+        kde_kwargs = _add_default_bounds_to_kde_kwargs_dict(
+            kde_kwargs, param, samples, comparison=True
+        )
     return _1d_comparison_histogram_plot(
-        param, samples, colors, latex_label, labels, kde=kde,
-        linestyles=linestyles, xlow=xlow, xhigh=xhigh,
-        max_vline=max_vline, grid=grid, legend_kwargs=legend_kwargs
+        param, samples, *args, kde_kwargs=kde_kwargs, max_vline=max_vline,
+        legend_kwargs=legend_kwargs, **kwargs
     )
 
 
 def _make_corner_plot(samples, latex_labels, corner_parameters=None, **kwargs):
     """Generate the corner plots for a given approximant
 
     Parameters
@@ -302,16 +325,23 @@
     dec: float
         declination of the source
     psi: float
         polarisation of the source
     time_gps: float
         gps time of merger
     """
-    gmst = Time(time_gps, format='gps', location=(0, 0))
-    corrected_ra = gmst.sidereal_time('mean').rad - ra
+    # Following 8 lines taken from pycbc.detector.Detector
+    from astropy.units.si import sday
+    reference_time = 1126259462.0
+    gmst_reference = Time(
+        reference_time, format='gps', scale='utc', location=(0, 0)
+    ).sidereal_time('mean').rad
+    dphase = (time_gps - reference_time) / float(sday.si.scale) * (2.0 * np.pi)
+    gmst = (gmst_reference + dphase) % (2.0 * np.pi)
+    corrected_ra = gmst - ra
     if not LALSIMULATION:
         raise Exception("lalsimulation could not be imported. please install "
                         "lalsuite to be able to use all features")
     detector = lalsim.DetectorPrefixToLALDetector(str(name))
 
     x0 = -np.cos(psi) * np.sin(corrected_ra) - \
         np.sin(psi) * np.cos(corrected_ra) * np.sin(dec)
@@ -325,16 +355,20 @@
         np.cos(psi) * np.cos(corrected_ra) * np.sin(dec)
     y1 = np.sin(psi) * np.cos(corrected_ra) + \
         np.cos(psi) * np.sin(corrected_ra) * np.sin(dec)
     y2 = np.cos(psi) * np.cos(dec)
     y = np.array([y0, y1, y2])
     dy = detector.response.dot(y)
 
-    fplus = (x * dx - y * dy).sum()
-    fcross = (x * dy + y * dx).sum()
+    if hasattr(dx, "shape"):
+        fplus = (x * dx - y * dy).sum(axis=0)
+        fcross = (x * dy + y * dx).sum(axis=0)
+    else:
+        fplus = (x * dx - y * dy).sum()
+        fcross = (x * dy + y * dx).sum()
 
     return fplus, fcross
 
 
 @no_latex_plot
 def _waveform_plot(detectors, maxL_params, **kwargs):
     """Plot the maximum likelihood waveform for a given approximant.
@@ -344,14 +378,15 @@
     detectors: list
         list of detectors that you want to generate waveforms for
     maxL_params: dict
         dictionary of maximum likelihood parameter values
     kwargs: dict
         dictionary of optional keyword arguments
     """
+    from gwpy.plot.colors import GW_OBSERVATORY_COLORS
     if math.isnan(maxL_params["mass_1"]):
         return
     logger.debug("Generating the maximum likelihood waveform plot")
     if not LALSIMULATION:
         raise Exception("lalsimulation could not be imported. please install "
                         "lalsuite to be able to use all features")
     delta_frequency = kwargs.get("delta_f", 1. / 256)
@@ -390,15 +425,15 @@
                                 maxL_params["psi"], maxL_params["geocent_time"])
         ax.plot(frequency_array, abs(h_plus * ar[0] + h_cross * ar[1]),
                 color=colors[num], linewidth=1.0, label=i)
     ax.set_xscale("log")
     ax.set_yscale("log")
     ax.set_xlabel(r"Frequency $[Hz]$")
     ax.set_ylabel(r"Strain")
-    ax.grid(b=True)
+    ax.grid(visible=True)
     ax.legend(loc="best")
     fig.tight_layout()
     return fig
 
 
 @no_latex_plot
 def _waveform_comparison_plot(maxL_params_list, colors, labels,
@@ -458,25 +493,26 @@
         h_cross = h_cross[:len(frequency_array)]
         ar = __antenna_response("H1", i["ra"], i["dec"], i["psi"],
                                 i["geocent_time"])
         ax.plot(frequency_array, abs(h_plus * ar[0] + h_cross * ar[1]),
                 color=colors[num], label=labels[num], linewidth=2.0)
     ax.set_xscale("log")
     ax.set_yscale("log")
-    ax.grid(b=True)
+    ax.grid(visible=True)
     ax.legend(loc="best")
     ax.set_xlabel(r"Frequency $[Hz]$")
     ax.set_ylabel(r"Strain")
     fig.tight_layout()
     return fig
 
 
 def _ligo_skymap_plot(ra, dec, dist=None, savedir="./", nprocess=1,
                       downsampled=False, label="pesummary", time=None,
-                      distance_map=True, multi_resolution=True, **kwargs):
+                      distance_map=True, multi_resolution=True,
+                      injection=None, **kwargs):
     """Plot the sky location of the source for a given approximant using the
     ligo.skymap package
 
     Parameters
     ----------
     ra: list
         list of samples for right ascension
@@ -490,22 +526,22 @@
         Boolean for whether to use multithreading or not
     downsampled: Bool
         Boolean for whether the samples have been downsampled or not
     distance_map: Bool
         Boolean for whether or not to produce a distance map
     multi_resolution: Bool
         Boolean for whether or not to generate a multiresolution HEALPix map
+    injection: list, optional
+        List containing RA and DEC of the injection. Both must be in radians
     kwargs: dict
         optional keyword arguments
     """
-    import healpy as hp
-    from ligo.skymap import plot, postprocess, io
     from ligo.skymap.bayestar import rasterize
+    from ligo.skymap import io
     from ligo.skymap.kde import Clustered2DSkyKDE, Clustered2Plus1DSkyKDE
-    from astropy.time import Time
 
     if dist is not None and distance_map:
         pts = np.column_stack((ra, dec, dist))
         cls = Clustered2Plus1DSkyKDE
     else:
         pts = np.column_stack((ra, dec))
         cls = Clustered2DSkyKDE
@@ -529,21 +565,21 @@
     io.write_sky_map(
         os.path.join(savedir, "%s_skymap.fits" % (label)), hpmap, nest=True
     )
     skymap, metadata = io.fits.read_sky_map(
         os.path.join(savedir, "%s_skymap.fits" % (label)), nest=None
     )
     return _ligo_skymap_plot_from_array(
-        skymap, nsamples=len(ra), downsampled=downsampled
+        skymap, nsamples=len(ra), downsampled=downsampled, injection=injection
     )[0]
 
 
 def _ligo_skymap_plot_from_array(
     skymap, nsamples=None, downsampled=False, contour=[50, 90],
-    annotate=True, ax=None, colors="k"
+    annotate=True, ax=None, colors="k", injection=None
 ):
     """Generate a skymap with `ligo.skymap` based on an array of probabilities
 
     Parameters
     ----------
     skymap: np.array
         array of probabilities
@@ -556,25 +592,24 @@
     annotate: Bool, optional
         If True, annotate the figure by adding the 90% and 50% sky areas
         by default
     ax: matplotlib.axes._subplots.AxesSubplot, optional
         Existing axis to add the plot to
     colors: str/list
         colors to use for the contours
+    injection: list, optional
+        List containing RA and DEC of the injection. Both must be in radians
     """
     import healpy as hp
-    from ligo.skymap import plot, io
-    from ligo.skymap.bayestar import rasterize
-    from ligo.skymap.kde import Clustered2DSkyKDE, Clustered2Plus1DSkyKDE
-    from astropy.time import Time
+    from ligo.skymap import plot
 
     if ax is None:
         fig = figure(gca=False)
         ax = fig.add_subplot(111, projection='astro hours mollweide')
-        ax.grid(b=True)
+        ax.grid(visible=True)
 
     nside = hp.npix2nside(len(skymap))
     deg2perpix = hp.nside2pixarea(nside, degrees=True)
     probperdeg2 = skymap / deg2perpix
 
     if downsampled:
         ax.set_title("Downsampled to %s" % (nsamples), fontdict={'fontsize': 11})
@@ -589,19 +624,30 @@
         ii = np.round(
             np.searchsorted(np.sort(cls), contour) * deg2perpix).astype(int)
         for i, p in zip(ii, pp):
             text.append(u'{:d}% area: {:d} deg²'.format(p, i, grouping=True))
         ax.text(1, 1.05, '\n'.join(text), transform=ax.transAxes, ha='right',
                 fontsize=10)
     plot.outline_text(ax)
+    if injection is not None and len(injection) == 2:
+        from astropy.coordinates import SkyCoord
+        from astropy import units as u
+
+        _inj = SkyCoord(*injection, unit=u.rad)
+        ax.scatter(
+            _inj.ra.value, _inj.dec.value, marker="*", color="orange",
+            edgecolors='k', linewidth=1.75, s=100, zorder=100,
+            transform=ax.get_transform('world')
+        )
     return ExistingFigure(fig), ax
 
 
 def _ligo_skymap_comparion_plot_from_array(
-    skymaps, colors, labels, contour=[50, 90], show_probability_map=False
+    skymaps, colors, labels, contour=[50, 90], show_probability_map=False,
+    injection=None
 ):
     """Generate a skymap with `ligo.skymap` based which compares arrays of
     probabilities
 
     Parameters
     ----------
     skymaps: list
@@ -611,26 +657,26 @@
     labels: list
         list of labels associated with each skymap
     contour: list, optional
         contours you wish to display on the comparison plot
     show_probability_map: int, optional
         the index of the skymap you wish to show the probability
         map for. Default False
+    injection: list, optional
+        List containing RA and DEC of the injection. Both must be in radians
     """
-    from ligo.skymap import plot
-
     ncols = number_of_columns_for_legend(labels)
     fig = figure(gca=False)
     ax = fig.add_subplot(111, projection='astro hours mollweide')
-    ax.grid(b=True)
+    ax.grid(visible=True)
     for num, skymap in enumerate(skymaps):
         if isinstance(show_probability_map, int) and show_probability_map == num:
             _, ax = _ligo_skymap_plot_from_array(
                 skymap, nsamples=None, downsampled=False, contour=contour,
-                annotate=False, ax=ax, colors=colors[num]
+                annotate=False, ax=ax, colors=colors[num], injection=injection,
             )
         cls, cs = _ligo_skymap_contours(
             ax, skymap, contour=contour, colors=colors[num]
         )
         cs.collections[0].set_label(labels[num])
     ax.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, borderaxespad=0.,
               mode="expand", ncol=ncols)
@@ -665,27 +711,32 @@
 
     Parameters
     ----------
     ra: list
         list of samples for right ascension
     dec: list
         list of samples for declination
+    injection: list, optional
+        list containing the injected value of ra and dec
     kwargs: dict
         optional keyword arguments
     """
+    from .cmap import register_cylon, unregister_cylon
+    # register the cylon cmap
+    register_cylon()
     ra = [-i + np.pi for i in ra]
     logger.debug("Generating the sky map plot")
     fig, ax = figure(gca=True)
     ax = fig.add_subplot(
         111, projection="mollweide",
         facecolor=(1.0, 0.939165516411, 0.880255669068)
     )
     ax.cla()
     ax.set_title("Preliminary", fontdict={'fontsize': 11})
-    ax.grid(b=True)
+    ax.grid(visible=True)
     ax.set_xticklabels([
         r"$2^{h}$", r"$4^{h}$", r"$6^{h}$", r"$8^{h}$", r"$10^{h}$",
         r"$12^{h}$", r"$14^{h}$", r"$16^{h}$", r"$18^{h}$", r"$20^{h}$",
         r"$22^{h}$"])
     levels = [0.9, 0.5]
 
     if weights is None:
@@ -753,15 +804,17 @@
     xticks = np.arange(-np.pi, np.pi + np.pi / 6, np.pi / 4)
     ax.set_xticks(xticks)
     ax.set_yticks([-np.pi / 3, -np.pi / 6, 0, np.pi / 6, np.pi / 3])
     labels = [r"$%s^{h}$" % (int(np.round((i + np.pi) * 3.82, 1))) for i in xticks]
     ax.set_xticklabels(labels[::-1], fontsize=10)
     ax.set_yticklabels([r"$-60^{\circ}$", r"$-30^{\circ}$", r"$0^{\circ}$",
                         r"$30^{\circ}$", r"$60^{\circ}$"], fontsize=10)
-    ax.grid(b=True)
+    ax.grid(visible=True)
+    # unregister the cylon cmap
+    unregister_cylon()
     return fig
 
 
 def _sky_map_comparison_plot(ra_list, dec_list, labels, colors, **kwargs):
     """Generate a plot that compares the sky location for multiple approximants
 
     Parameters
@@ -783,15 +836,15 @@
     logger.debug("Generating the sky map comparison plot")
     fig = figure(gca=False)
     ax = fig.add_subplot(
         111, projection="mollweide",
         facecolor=(1.0, 0.939165516411, 0.880255669068)
     )
     ax.cla()
-    ax.grid(b=True)
+    ax.grid(visible=True)
     ax.set_xticklabels([
         r"$2^{h}$", r"$4^{h}$", r"$6^{h}$", r"$8^{h}$", r"$10^{h}$",
         r"$12^{h}$", r"$14^{h}$", r"$16^{h}$", r"$18^{h}$", r"$20^{h}$",
         r"$22^{h}$"])
     levels = [0.9, 0.5]
     for num, i in enumerate(ra_list):
         H, X, Y = np.histogram2d(i, dec_list[num], bins=50)
@@ -839,15 +892,15 @@
     xticks = np.arange(-np.pi, np.pi + np.pi / 6, np.pi / 4)
     ax.set_xticks(xticks)
     ax.set_yticks([-np.pi / 3, -np.pi / 6, 0, np.pi / 6, np.pi / 3])
     labels = [r"$%s^{h}$" % (int(np.round((i + np.pi) * 3.82, 1))) for i in xticks]
     ax.set_xticklabels(labels[::-1], fontsize=10)
     ax.set_yticklabels([r"$-60^\degree$", r"$-30^\degree$", r"$0^\degree$",
                         r"$30^\degree$", r"$60^\degree$"], fontsize=10)
-    ax.grid(b=True)
+    ax.grid(visible=True)
     return fig
 
 
 def __get_cutoff_indices(flow, fhigh, df, N):
     """
     Gets the indices of a frequency series at which to stop an overlap
     calculation.
@@ -951,15 +1004,15 @@
         for i in network:
             numerator += sum(i**2 for i in ar[i])
             denominator += SNR[i]**2
         N[ind[1]][ind[0]] = (((numerator / denominator)**0.5))
     fig = figure(gca=False)
     ax = fig.add_subplot(111, projection="hammer")
     ax.cla()
-    ax.grid(b=True)
+    ax.grid(visible=True)
     ax.pcolormesh(X, Y, N)
     ax.set_xticklabels([
         r"$22^{h}$", r"$20^{h}$", r"$18^{h}$", r"$16^{h}$", r"$14^{h}$",
         r"$12^{h}$", r"$10^{h}$", r"$8^{h}$", r"$6^{h}$", r"$4^{h}$",
         r"$2^{h}$"])
     return fig
 
@@ -975,14 +1028,16 @@
     detectors: list
         list of detectors that you want to generate waveforms for
     maxL_params: dict
         dictionary of maximum likelihood parameter values
     kwargs: dict
         dictionary of optional keyword arguments
     """
+    from gwpy.timeseries import TimeSeries
+    from gwpy.plot.colors import GW_OBSERVATORY_COLORS
     if math.isnan(maxL_params["mass_1"]):
         return
     logger.debug("Generating the maximum likelihood waveform time domain plot")
     if not LALSIMULATION:
         raise Exception("lalsimulation could not be imported. please install "
                         "lalsuite to be able to use all features")
     delta_t = 1. / 4096.
@@ -1020,15 +1075,15 @@
         h_t = TimeSeries(h_t[:], dt=h_plus.deltaT, t0=h_plus.epoch)
         h_t.times = [float(np.array(i)) + t_start for i in h_t.times]
         ax.plot(h_t.times, h_t,
                 color=colors[num], linewidth=1.0, label=i)
         ax.set_xlim([t_start - 3, t_start + 0.5])
     ax.set_xlabel(r"Time $[s]$")
     ax.set_ylabel(r"Strain")
-    ax.grid(b=True)
+    ax.grid(visible=True)
     ax.legend(loc="best")
     fig.tight_layout()
     return fig
 
 
 @no_latex_plot
 def _time_domain_waveform_comparison_plot(maxL_params_list, colors, labels,
@@ -1044,14 +1099,15 @@
     colors: list
         list of colors to be used to differentiate the different approximants
     approximant_labels: list, optional
         label to prepend the approximant in the legend
     kwargs: dict
         dictionary of optional keyword arguments
     """
+    from gwpy.timeseries import TimeSeries
     logger.debug("Generating the maximum likelihood time domain waveform "
                  "comparison plot for H1")
     if not LALSIMULATION:
         raise Exception("LALSimulation could not be imported. Please install "
                         "LALSuite to be able to use all features")
     delta_t = 1. / 4096.
     minimum_frequency = kwargs.get("f_min", 5.)
@@ -1091,15 +1147,15 @@
         h_t.times = [float(np.array(i)) + t_start for i in h_t.times]
 
         ax.plot(h_t.times, h_t,
                 color=colors[num], label=labels[num], linewidth=2.0)
     ax.set_xlabel(r"Time $[s]$")
     ax.set_ylabel(r"Strain")
     ax.set_xlim([t_start - 3, t_start + 0.5])
-    ax.grid(b=True)
+    ax.grid(visible=True)
     ax.legend(loc="best")
     fig.tight_layout()
     return fig
 
 
 def _psd_plot(frequencies, strains, colors=None, labels=None, fmin=None):
     """Superimpose all PSD plots onto a single figure.
@@ -1113,14 +1169,15 @@
     colors: optional, list
         list of colors to be used to differentiate the different PSDs
     labels: optional, list
         list of lavels for each PSD
     fmin: optional, float
         starting frequency of the plot
     """
+    from gwpy.plot.colors import GW_OBSERVATORY_COLORS
     fig, ax = figure(gca=True)
     if not colors and all(i in GW_OBSERVATORY_COLORS.keys() for i in labels):
         colors = [GW_OBSERVATORY_COLORS[i] for i in labels]
     elif not colors:
         colors = ['r', 'b', 'orange', 'c', 'g', 'purple']
         while len(colors) <= len(labels):
             colors += colors
@@ -1155,31 +1212,33 @@
         list of IFOs that are associated with the calibration envelopes
     colors: list, optional
         list of colors to be used to differentiate the different calibration
         envelopes
     prior: list, optional
         list containing the prior calibration envelope data for different IFOs
     """
+    from gwpy.plot.colors import GW_OBSERVATORY_COLORS
+
     def interpolate_calibration(data):
         """Interpolate the calibration data using spline
 
         Parameters
         ----------
         data: np.ndarray
             array containing the calibration data
         """
         interp = [
             np.interp(frequency, data[:, 0], data[:, j], left=k, right=k)
             for j, k in zip(range(1, 7), [1, 0, 1, 0, 1, 0])
         ]
-        amp_median = (1 - interp[0]) * 100
+        amp_median = (interp[0] - 1) * 100
         phase_median = interp[1] * 180. / np.pi
-        amp_lower_sigma = (1 - interp[2]) * 100
+        amp_lower_sigma = (interp[2] - 1) * 100
         phase_lower_sigma = interp[3] * 180. / np.pi
-        amp_upper_sigma = (1 - interp[4]) * 100
+        amp_upper_sigma = (interp[4] - 1) * 100
         phase_upper_sigma = interp[5] * 180. / np.pi
         data_dict = {
             "amplitude": {
                 "median": amp_median,
                 "lower": amp_lower_sigma,
                 "upper": amp_upper_sigma
             },
@@ -1249,43 +1308,60 @@
     ----------
     strain: gwpy.timeseries
         timeseries containing the strain data
     maxL_samples: dict
         dictionary of maximum likelihood parameter values
     """
     logger.debug("Generating the strain plot")
-    from pesummary.gw.file.conversions import time_in_each_ifo
+    from pesummary.gw.conversions import time_in_each_ifo
+    from gwpy.timeseries import TimeSeries
 
-    fig, ax = figure(gca=True)
+    fig, axs = subplots(nrows=len(strain.keys()), sharex=True)
     time = maxL_params["geocent_time"]
     delta_t = 1. / 4096.
     minimum_frequency = kwargs.get("f_min", 5.)
     t_start = time - 15.0
     t_finish = time + 0.06
     time_array = np.arange(t_start, t_finish, delta_t)
 
     approx = lalsim.GetApproximantFromString(maxL_params["approximant"])
     mass_1 = maxL_params["mass_1"] * MSUN_SI
     mass_2 = maxL_params["mass_2"] * MSUN_SI
     luminosity_distance = maxL_params["luminosity_distance"] * PC_SI * 10**6
-    if "phi_jl" in maxL_params.keys():
-        iota, S1x, S1y, S1z, S2x, S2y, S2z = \
-            lalsim.SimInspiralTransformPrecessingNewInitialConditions(
-                maxL_params["theta_jn"], maxL_params["phi_jl"], maxL_params["tilt_1"],
-                maxL_params["tilt_2"], maxL_params["phi_12"], maxL_params["a_1"],
-                maxL_params["a_2"], mass_1, mass_2, kwargs.get("f_ref", 10.),
-                maxL_params["phase"])
-    else:
-        iota, S1x, S1y, S1z, S2x, S2y, S2z = maxL_params["iota"], 0., 0., 0., \
-            0., 0., 0.
     phase = maxL_params["phase"] if "phase" in maxL_params.keys() else 0.0
+    cartesian = [
+        "iota", "spin_1x", "spin_1y", "spin_1z", "spin_2x", "spin_2y", "spin_2z"
+    ]
+    if not all(param in maxL_params.keys() for param in cartesian):
+        if "phi_jl" in maxL_params.keys():
+            iota, S1x, S1y, S1z, S2x, S2y, S2z = \
+                lalsim.SimInspiralTransformPrecessingNewInitialConditions(
+                    maxL_params["theta_jn"], maxL_params["phi_jl"],
+                    maxL_params["tilt_1"], maxL_params["tilt_2"],
+                    maxL_params["phi_12"], maxL_params["a_1"],
+                    maxL_params["a_2"], mass_1, mass_2, kwargs.get("f_ref", 10.),
+                    phase
+                )
+        else:
+            iota, S1x, S1y, S1z, S2x, S2y, S2z = maxL_params["iota"], 0., 0., \
+                0., 0., 0., 0.
+    else:
+        iota, S1x, S1y, S1z, S2x, S2y, S2z = [
+            maxL_params[param] for param in cartesian
+        ]
     h_plus, h_cross = lalsim.SimInspiralChooseTDWaveform(
         mass_1, mass_2, S1x, S1y, S1z, S2x, S2y, S2z, luminosity_distance, iota,
         phase, 0.0, 0.0, 0.0, delta_t, minimum_frequency,
         kwargs.get("f_ref", 10.), None, approx)
+    h_plus = TimeSeries(
+        h_plus.data.data[:], dt=h_plus.deltaT, t0=h_plus.epoch
+    )
+    h_cross = TimeSeries(
+        h_cross.data.data[:], dt=h_cross.deltaT, t0=h_cross.epoch
+    )
 
     for num, key in enumerate(list(strain.keys())):
         ifo_time = time_in_each_ifo(key, maxL_params["ra"], maxL_params["dec"],
                                     maxL_params["geocent_time"])
 
         asd = strain[key].asd(8, 4, method="median")
         strain_data_frequency = strain[key].fft()
@@ -1294,41 +1370,39 @@
         strain_data_time = (strain_data_frequency / asd_interp).ifft()
         strain_data_time = strain_data_time.highpass(30)
         strain_data_time = strain_data_time.lowpass(300)
 
         ar = __antenna_response(key, maxL_params["ra"], maxL_params["dec"],
                                 maxL_params["psi"], maxL_params["geocent_time"])
 
-        h_t = ar[0] * h_plus.data.data + ar[1] * h_cross.data.data
-        h_t = TimeSeries(h_t[:], dt=h_plus.deltaT, t0=h_plus.epoch)
+        h_t = ar[0] * h_plus + ar[1] * h_cross
         h_t_frequency = h_t.fft()
         asd_interp = asd.interpolate(float(np.array(h_t_frequency.df)))
         asd_interp = asd_interp[:len(h_t_frequency)]
         h_t_time = (h_t_frequency / asd_interp).ifft()
         h_t_time = h_t_time.highpass(30)
         h_t_time = h_t_time.lowpass(300)
         h_t_time.times = [float(np.array(i)) + ifo_time for i in h_t.times]
 
-        strain_data_crop = strain_data_time.crop(ifo_time - 0.1, ifo_time + 0.06)
+        strain_data_crop = strain_data_time.crop(ifo_time - 0.2, ifo_time + 0.06)
         try:
-            h_t_time = h_t_time.crop(ifo_time - 0.1, ifo_time + 0.06)
+            h_t_time = h_t_time.crop(ifo_time - 0.2, ifo_time + 0.06)
         except Exception:
             pass
         max_strain = np.max(strain_data_crop).value
 
-        _, ax = subplots(len(strain.keys()), 1, num + 1)
-        ax.plot(strain_data_crop, color='grey', alpha=0.75, label="data")
-        ax.plot(h_t_time, color='orange', label="template")
-        ax.set_xlim([ifo_time - 0.1, ifo_time + 0.06])
+        axs[num].plot(strain_data_crop, color='grey', alpha=0.75, label="data")
+        axs[num].plot(h_t_time, color='orange', label="template")
+        axs[num].set_xlim([ifo_time - 0.2, ifo_time + 0.06])
         if not math.isnan(max_strain):
-            ax.set_ylim([-max_strain * 1.5, max_strain * 1.5])
-        ax.set_ylabel("Whitened %s strain" % (key), fontsize=8)
-        ax.grid(False)
-        ax.legend(loc="best", prop={'size': 8})
-    ax._set_xlabel("Time $[s]$", fontsize=16)
+            axs[num].set_ylim([-max_strain * 1.5, max_strain * 1.5])
+        axs[num].set_ylabel("Whitened %s strain" % (key), fontsize=8)
+        axs[num].grid(False)
+        axs[num].legend(loc="best", prop={'size': 8})
+    axs[-1].set_xlabel("Time $[s]$", fontsize=16)
     fig.tight_layout()
     return fig
 
 
 def _format_prob(prob):
     """Format the probabilities for use with _classification_plot
     """
```

### Comparing `pesummary-0.9.1/pesummary/gw/plots/cmap.py` & `pesummary-1.0.0/pesummary/gw/plots/cmap.py`

 * *Files 9% similar despite different names*

```diff
@@ -15,26 +15,35 @@
 
 from matplotlib import cm
 from matplotlib import colors
 import numpy as np
 import pesummary
 
 
-def cylon():
+def register_cylon():
     # Read in color map RGB data.
     path = pesummary.__file__[:-12]
     with open(path + "/gw/plots/cylon.csv") as f:
         data = np.loadtxt(f, delimiter=',')
 
     # Create color map.
     cmap = colors.LinearSegmentedColormap.from_list("cylon", data)
     # Assign in module.
     locals().update({"cylon": cmap})
     # Register with Matplotlib.
     cm.register_cmap(cmap=cmap)
+    # Create inverse color map
+    cmap_r = colors.LinearSegmentedColormap.from_list("cylon_r", data[::-1])
+    locals().update({"cylon_r": cmap_r})
+    cm.register_cmap(cmap=cmap_r)
+
+
+def unregister_cylon():
+    cm.unregister_cmap("cylon")
+    cm.unregister_cmap("cylon_r")
 
 
 def colormap_with_fixed_hue(color, N=10):
     """Create a linear colormap with fixed hue
 
     Parameters
     ----------
```

### Comparing `pesummary-0.9.1/pesummary/gw/plots/bounded_2d_kde.py` & `pesummary-1.0.0/pesummary/core/plots/bounded_2d_kde.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,21 +1,25 @@
 import numpy as np
 from scipy.stats import gaussian_kde as kde
-from pesummary.gw.plots.bounds import default_bounds
+
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
 
 class Bounded_2d_kde(kde):
     """Class to generate a two-dimensional KDE for a probability distribution
     functon that exists on a bounded domain
     """
     def __init__(self, pts, xlow=None, xhigh=None, ylow=None, yhigh=None,
-                 *args, **kwargs):
+                 transform=None, *args, **kwargs):
         pts = np.atleast_2d(pts)
         assert pts.ndim == 2, 'Bounded_kde can only be two-dimensional'
-        super(Bounded_2d_kde, self).__init__(pts.T, *args, **kwargs)
+        self._transform = transform
+        if transform is not None:
+            pts = transform(pts)
+        super(Bounded_2d_kde, self).__init__(pts, *args, **kwargs)
         self._xlow = xlow
         self._xhigh = xhigh
         self._ylow = ylow
         self._yhigh = yhigh
 
     @property
     def xlow(self):
@@ -40,19 +44,21 @@
         """The upper bound of the y domain
         """
         return self._yhigh
 
     def evaluate(self, pts):
         """Return an estimate of the density evaluated at the given
         points."""
-        pts = np.atleast_2d(pts)
-        assert pts.ndim == 2, 'points must be two-dimensional'
+        _pts = np.atleast_2d(pts)
+        assert _pts.ndim == 2, 'points must be two-dimensional'
+        if _pts.shape[0] != 2 and _pts.shape[1] == 2:
+            _pts = _pts.T
 
-        x, y = pts.T
-        pdf = super(Bounded_2d_kde, self).evaluate(pts.T)
+        x, y = _pts
+        pdf = super(Bounded_2d_kde, self).evaluate(_pts)
         if self.xlow is not None:
             pdf += super(Bounded_2d_kde, self).evaluate([2 * self.xlow - x, y])
 
         if self.xhigh is not None:
             pdf += super(Bounded_2d_kde, self).evaluate([2 * self.xhigh - x, y])
 
         if self.ylow is not None:
@@ -76,22 +82,25 @@
                     [2 * self.xhigh - x, 2 * self.ylow - y])
             if self.yhigh is not None:
                 pdf += super(Bounded_2d_kde, self).evaluate(
                     [2 * self.xhigh - x, 2 * self.yhigh - y])
         return pdf
 
     def __call__(self, pts):
-        pts = np.atleast_2d(pts)
-        out_of_bounds = np.zeros(pts.shape[0], dtype='bool')
-
+        _pts = np.atleast_2d(pts)
+        if _pts.shape[0] != 2 and _pts.shape[1] == 2:
+            _pts = _pts.T
+        if self._transform is not None:
+            _pts = self._transform(_pts)
+        transpose = _pts.T
+        out_of_bounds = np.zeros(transpose.shape[0], dtype='bool')
         if self.xlow is not None:
-            out_of_bounds[pts[:, 0] < self.xlow] = True
+            out_of_bounds[transpose[:, 0] < self.xlow] = True
         if self.xhigh is not None:
-            out_of_bounds[pts[:, 0] > self.xhigh] = True
+            out_of_bounds[transpose[:, 0] > self.xhigh] = True
         if self.ylow is not None:
-            out_of_bounds[pts[:, 1] < self.ylow] = True
+            out_of_bounds[transpose[:, 1] < self.ylow] = True
         if self.yhigh is not None:
-            out_of_bounds[pts[:, 1] > self.yhigh] = True
-
-        results = self.evaluate(pts)
+            out_of_bounds[transpose[:, 1] > self.yhigh] = True
+        results = self.evaluate(_pts)
         results[out_of_bounds] = 0.
         return results
```

### Comparing `pesummary-0.9.1/pesummary/gw/plots/cylon.csv` & `pesummary-1.0.0/pesummary/gw/plots/cylon.csv`

 * *Files identical despite different names*

### Comparing `pesummary-0.9.1/pesummary/gw/plots/publication.py` & `pesummary-1.0.0/pesummary/gw/plots/publication.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,220 +1,219 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-#                     Michael Puerrer <michael.puerrer@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
-
-from pesummary.utils.utils import (
-    logger, number_of_columns_for_legend, get_matplotlib_backend
-)
-import matplotlib
-import matplotlib.lines as mlines
+# Licensed under an MIT style license -- see LICENSE.md
+
+from pesummary.utils.utils import logger, number_of_columns_for_legend
 import seaborn
 from pesummary.core.plots.figure import figure
-from pesummary.gw.plots import violin
-from pesummary.gw.plots.bounded_2d_kde import Bounded_2d_kde
+from pesummary.core.plots.seaborn import violin
+from pesummary.core.plots.bounded_2d_kde import Bounded_2d_kde
 from pesummary.gw.plots.bounds import default_bounds
 from pesummary.gw.plots.cmap import colormap_with_fixed_hue
-from pesummary.gw.file.conversions import mchirp_from_m1_m2, q_from_m1_m2
+from pesummary.gw.conversions import mchirp_from_m1_m2, q_from_m1_m2
 import numpy as np
 import copy
 
-seaborn.set(style="whitegrid")
+__author__ = [
+    "Charlie Hoy <charlie.hoy@ligo.org>",
+    "Michael Puerrer <michael.puerrer@ligo.org>"
+]
 
 
 def chirp_mass_and_q_from_mass1_mass2(pts):
     """Transform the component masses to chirp mass and mass ratio
 
     Parameters
     ----------
     pts: numpy.array
         array containing the mass1 and mass2 samples
     """
     pts = np.atleast_2d(pts)
 
-    m1 = pts[:, 0]
-    m2 = pts[:, 1]
+    m1, m2 = pts
     mc = mchirp_from_m1_m2(m1, m2)
     q = q_from_m1_m2(m1, m2)
-    return np.column_stack([mc, q])
+    return np.vstack([mc, q])
 
 
-def estimate_2d_posterior(samples, xlow=None, xhigh=None, ylow=None,
-                          yhigh=None, transform=None, gridsize=100):
-    """Estimate a 2 dimensional posterior distribution
+def _return_bounds(parameters, T=True):
+    """Return bounds for KDE
 
     Parameters
     ----------
-    samples: nd list
-        2d list of samples
-    xlow: float
-        bound the KDE to not take x values below xlow
-    xhigh: float
-        bound the KDE to not take x values above xhigh
-    ylow: float
-        bound the KDE to not take y values below ylow
-    yhigh: float
-        bound the KDE to not take y valuesabove ylow
-    transform: func
-        function to transform the parameters
-    gridsize: int
-        the number of points to use when estimating the KDE
+    parameters: list
+        list of parameters being plotted
+    T: Bool, optional
+        if True, modify the parameter bounds if a transform is required
     """
-    x = np.array(samples[0])
-    y = np.array(samples[1])
-
-    if transform is None:
-        transform = lambda x: x
-
-    deltax = 0.1 * (x.max() - x.min())
-    deltay = 0.1 * (y.max() - y.min())
-    x_pts = np.linspace(x.min() - deltax, x.max() + deltax, gridsize)
-    y_pts = np.linspace(y.min() - deltay, y.max() + deltay, gridsize)
-    xx, yy = np.meshgrid(x_pts, y_pts)
-
-    positions = np.column_stack([xx.ravel(), yy.ravel()])
-    pts = np.array([x, y]).T
-    selected_indices = np.random.choice(len(pts), len(pts) // 2, replace=False)
-    kde_sel = np.zeros(len(pts), dtype=bool)
-    kde_sel[selected_indices] = True
-    kde_pts = transform(pts[kde_sel])
-    untransformed_den_pts = pts[~kde_sel]
-    den_pts = transform(untransformed_den_pts)
-    Nden = den_pts.shape[0]
-
-    post_kde = Bounded_2d_kde(
-        kde_pts, xlow=xlow, xhigh=xhigh, ylow=ylow, yhigh=yhigh)
-    den = post_kde(den_pts)
-    inds = np.argsort(den)[::-1]
-    den = den[inds]
-
-    z = np.reshape(post_kde(transform(positions)), xx.shape)
-    return {'xx': xx, 'yy': yy, 'z': z, 'kde': den, 'kde_sel': kde_sel}
+    transform = xlow = xhigh = ylow = yhigh = None
+    if parameters[0] in list(default_bounds.keys()):
+        if "low" in list(default_bounds[parameters[0]].keys()):
+            xlow = default_bounds[parameters[0]]["low"]
+        if "high" in list(default_bounds[parameters[0]].keys()):
+            if isinstance(default_bounds[parameters[0]]["high"], str) and T:
+                if "mass_1" in default_bounds[parameters[0]]["high"]:
+                    transform = chirp_mass_and_q_from_mass1_mass2
+                    xhigh = 1.
+            elif isinstance(default_bounds[parameters[0]]["high"], str):
+                xhigh = None
+            else:
+                xhigh = default_bounds[parameters[0]]["high"]
+    if parameters[1] in list(default_bounds.keys()):
+        if "low" in list(default_bounds[parameters[1]].keys()):
+            ylow = default_bounds[parameters[1]]["low"]
+        if "high" in list(default_bounds[parameters[1]].keys()):
+            if isinstance(default_bounds[parameters[1]]["high"], str) and T:
+                if "mass_1" in default_bounds[parameters[1]]["high"]:
+                    transform = chirp_mass_and_q_from_mass1_mass2
+                    yhigh = 1.
+            elif isinstance(default_bounds[parameters[1]]["high"], str):
+                yhigh = None
+            else:
+                yhigh = default_bounds[parameters[1]]["high"]
+    return transform, xlow, xhigh, ylow, yhigh
 
 
 def twod_contour_plots(
     parameters, samples, labels, latex_labels, colors=None, linestyles=None,
-    gridsize=100, return_ax=False
+    return_ax=False, plot_datapoints=False, smooth=None, latex_friendly=False,
+    levels=[0.9], legend_kwargs={
+        "bbox_to_anchor": (0., 1.02, 1., .102), "loc": 3, "handlelength": 3,
+        "mode": "expand", "borderaxespad": 0., "handleheight": 1.75
+    }, **kwargs
 ):
     """Generate 2d contour plots for a set of samples for given parameters
 
     Parameters
     ----------
     parameters: list
         names of the parameters that you wish to plot
     samples: nd list
         list of samples for each parameter
     labels: list
         list of labels corresponding to each set of samples
     latex_labels: dict
         dictionary of latex labels
     """
+    from pesummary.core.plots.publication import (
+        comparison_twod_contour_plot as core
+    )
     from matplotlib.patches import Polygon
 
     logger.debug("Generating 2d contour plots for %s" % ("_and_".join(parameters)))
     if colors is None:
         palette = seaborn.color_palette(palette="pastel", n_colors=len(samples))
     else:
         palette = colors
     if linestyles is None:
         linestyles = ["-"] * len(samples)
     fig, ax1 = figure(gca=True)
-    transform = xlow = xhigh = ylow = yhigh = None
-    handles = []
-    for num, i in enumerate(samples):
-        if parameters[0] in list(default_bounds.keys()):
-            if "low" in list(default_bounds[parameters[0]].keys()):
-                xlow = default_bounds[parameters[0]]["low"]
-            if "high" in list(default_bounds[parameters[0]].keys()):
-                if isinstance(default_bounds[parameters[0]]["high"], str):
-                    if "mass_1" in default_bounds[parameters[0]]["high"]:
-                        transform = chirp_mass_and_q_from_mass1_mass2
-                        xhigh = 1.
-                else:
-                    xhigh = default_bounds[parameters[0]]["high"]
-        if parameters[1] in list(default_bounds.keys()):
-            if "low" in list(default_bounds[parameters[1]].keys()):
-                ylow = default_bounds[parameters[1]]["low"]
-            if "high" in list(default_bounds[parameters[1]].keys()):
-                if isinstance(default_bounds[parameters[1]]["high"], str):
-                    if "mass_1" in default_bounds[parameters[1]]["high"]:
-                        transform = chirp_mass_and_q_from_mass1_mass2
-                        yhigh = 1.
-                else:
-                    yhigh = default_bounds[parameters[1]]["high"]
-
-        contour_data = estimate_2d_posterior(
-            i, transform=transform, xlow=xlow, xhigh=xhigh, ylow=ylow,
-            yhigh=yhigh, gridsize=gridsize)
-        xx = contour_data['xx']
-        yy = contour_data['yy']
-        z = contour_data['z']
-        den = contour_data['kde']
-        Nden = len(den)
-        kde_sel = contour_data['kde_sel']
-
-        pts = np.array([i[0], i[1]]).T
-        levels = [0.9]
-        zvalues = np.empty(len(levels))
-        for i, level in enumerate(levels):
-            ilevel = int(np.ceil(Nden * level))
-            ilevel = min(ilevel, Nden - 1)
-            zvalues[i] = den[ilevel]
-        zvalues.sort()
-
-        cs = ax1.contour(
-            xx, yy, z, levels=zvalues, colors=[palette[num]], linewidths=1.5,
-            linestyles=[linestyles[num]]
-        )
-        handles.append(
-            mlines.Line2D([], [], color=palette[num], label=labels[num])
-        )
+    transform, xlow, xhigh, ylow, yhigh = _return_bounds(parameters)
+    kwargs.update(
+        {
+            "kde": Bounded_2d_kde, "kde_kwargs": {
+                "transform": transform, "xlow": xlow, "xhigh": xhigh,
+                "ylow": ylow, "yhigh": yhigh
+            }
+        }
+    )
+    fig = core(
+        [i[0] for i in samples], [i[1] for i in samples], colors=colors,
+        labels=labels, xlabel=latex_labels[parameters[0]], smooth=smooth,
+        ylabel=latex_labels[parameters[1]], linestyles=linestyles,
+        plot_datapoints=plot_datapoints, levels=levels, **kwargs
+    )
+    ax1 = fig.gca()
     if all("mass_1" in i or "mass_2" in i for i in parameters):
-
         reg = Polygon([[0, 0], [0, 1000], [1000, 1000]], color='gray', alpha=0.75)
         ax1.add_patch(reg)
-    ax1.set_xlabel(latex_labels[parameters[0]])
-    ax1.set_ylabel(latex_labels[parameters[1]])
-
-    _limits = lambda prop, ind: getattr(np, prop)(
-        [getattr(np, prop)(i[ind]) for i in samples]
-    )
-    _xlow, _xhigh = _limits("min", 0), _limits("max", 0)
-    _ylow, _yhigh = _limits("min", 1), _limits("max", 1)
-    _maximum = np.max([np.max(i) for i in samples])
-    ax1.set_xlim(0.9 * _xlow, 1.1 * _xhigh)
-    ax1.set_ylim(0.9 * _ylow, 1.1 * _yhigh)
-
     ncols = number_of_columns_for_legend(labels)
-    legend = ax1.legend(
-        handles=handles, bbox_to_anchor=(0., 1.02, 1., .102), loc=3,
-        handlelength=3, ncol=ncols, mode="expand", borderaxespad=0.
-    )
-    for num, legobj in enumerate(legend.legendHandles):
-        legobj.set_linewidth(1.75)
-        legobj.set_linestyle(linestyles[num])
+    legend_kwargs.update({"ncol": ncols})
+    legend = ax1.legend(**legend_kwargs)
+    for leg in legend.get_lines():
+        leg.set_linewidth(legend_kwargs.get("handleheight", 1.))
     fig.tight_layout()
     if return_ax:
         return fig, ax1
     return fig
 
 
+def _setup_triangle_plot(parameters, kwargs):
+    """Modify a dictionary of kwargs for bounded KDEs
+
+    Parameters
+    ----------
+    parameters: list
+        list of parameters being plotted
+    kwargs: dict
+        kwargs to be passed to pesummary.gw.plots.publication.triangle_plot
+        or pesummary.gw.plots.publication.reverse_triangle_plot
+    """
+    from pesummary.core.plots.bounded_1d_kde import bounded_1d_kde
+
+    if not len(parameters):
+        raise ValueError("Please provide a list of parameters")
+    transform, xlow, xhigh, ylow, yhigh = _return_bounds(parameters)
+    kwargs.update(
+        {
+            "kde_2d": Bounded_2d_kde, "kde_2d_kwargs": {
+                "transform": transform, "xlow": xlow, "xhigh": xhigh,
+                "ylow": ylow, "yhigh": yhigh
+            }, "kde": bounded_1d_kde
+        }
+    )
+    _, xlow, xhigh, ylow, yhigh = _return_bounds(parameters, T=False)
+    kwargs["kde_kwargs"] = {
+        "x_axis": {"xlow": xlow, "xhigh": xhigh},
+        "y_axis": {"xlow": ylow, "xhigh": yhigh}
+    }
+    return kwargs
+
+
+def triangle_plot(*args, parameters=[], **kwargs):
+    """Generate a triangular plot made of 3 axis. One central axis showing the
+    2d marginalized posterior and two smaller axes showing the marginalized 1d
+    posterior distribution (above and to the right of central axis)
+
+    Parameters
+    ----------
+    *args: tuple
+        all args passed to pesummary.core.plots.publication.triangle_plot
+    parameters: list
+        list of parameters being plotted
+    kwargs: dict, optional
+        all kwargs passed to pesummary.core.plots.publication.triangle_plot
+    """
+    from pesummary.core.plots.publication import triangle_plot as core
+    kwargs = _setup_triangle_plot(parameters, kwargs)
+    return core(*args, **kwargs)
+
+
+def reverse_triangle_plot(*args, parameters=[], **kwargs):
+    """Generate a triangular plot made of 3 axis. One central axis showing the
+    2d marginalized posterior and two smaller axes showing the marginalized 1d
+    posterior distribution (below and to the left of central axis). Only two
+    axes are plotted, each below the 1d marginalized posterior distribution
+
+    Parameters
+    ----------
+    *args: tuple
+        all args passed to
+        pesummary.core.plots.publication.reverse_triangle_plot
+    parameters: list
+        list of parameters being plotted
+    kwargs: dict, optional
+        all kwargs passed to
+        pesummary.core.plots.publication.reverse_triangle_plot
+    """
+    from pesummary.core.plots.publication import reverse_triangle_plot as core
+    kwargs = _setup_triangle_plot(parameters, kwargs)
+    return core(*args, **kwargs)
+
+
 def violin_plots(
-    parameter, samples, labels, latex_labels, cut=0,
+    parameter, samples, labels, latex_labels, inj_values=None, cut=0,
     _default_kwargs={"palette": "pastel", "inner": "line", "outer": "percent: 90"},
     latex_friendly=True, **kwargs
 ):
     """Generate violin plots for a set of parameters and samples
 
     Parameters
     ----------
@@ -222,20 +221,22 @@
         the name of the parameter that you wish to plot
     samples: nd list
         list of samples for each parameter
     labels: list
         list of labels corresponding to each set of samples
     latex_labels: dict
         dictionary of latex labels
+    inj_values: list
+        list of injected values for each set of samples
     """
     logger.debug("Generating violin plots for %s" % (parameter))
     fig, ax1 = figure(gca=True)
     _default_kwargs.update(kwargs)
     ax1 = violin.violinplot(
-        data=samples, cut=cut, ax=ax1, scale="width", **_default_kwargs
+        data=samples, cut=cut, ax=ax1, scale="width", inj=inj_values, **_default_kwargs
     )
     if latex_friendly:
         labels = copy.deepcopy(labels)
         for num, _ in enumerate(labels):
             labels[num] = labels[num].replace("_", "\_")
     ax1.set_xticklabels(labels)
     for label in ax1.get_xmajorticklabels():
@@ -267,65 +268,63 @@
     annotate: Bool, optional
         if True, label the magnitude and tilt directions
     show_label: Bool, optional
         if True, add labels indicating which side of the spin disk corresponds
         to which binary component
     """
     logger.debug("Generating spin distribution plots for %s" % (label))
-    from matplotlib.ticker import FixedLocator, Formatter
     from matplotlib.projections import PolarAxes
     from matplotlib.transforms import Affine2D
     from matplotlib.patches import Wedge
     from matplotlib import patheffects as PathEffects
     from matplotlib.collections import PatchCollection
     from matplotlib.transforms import ScaledTranslation
 
     from mpl_toolkits.axisartist.grid_finder import MaxNLocator
     import mpl_toolkits.axisartist.floating_axes as floating_axes
     import mpl_toolkits.axisartist.angle_helper as angle_helper
-    from matplotlib.colors import LinearSegmentedColormap
 
     if color is not None and cmap is None:
         cmap = colormap_with_fixed_hue(color)
     elif color is None and cmap is None:
         raise ValueError(
             "Please provide either a single color or a cmap to use for plotting"
         )
 
     spin1 = samples[parameters.index("a_1")]
     spin2 = samples[parameters.index("a_2")]
     costheta1 = samples[parameters.index("cos_tilt_1")]
     costheta2 = samples[parameters.index("cos_tilt_2")]
 
-    pts = np.array([spin1, costheta1]).T
-    selected_indices = np.random.choice(len(pts), len(pts) // 2, replace=False)
-    kde_sel = np.zeros(len(pts), dtype=bool)
+    pts = np.array([spin1, costheta1])
+    selected_indices = np.random.choice(pts.shape[1], pts.shape[1] // 2, replace=False)
+    kde_sel = np.zeros(pts.shape[1], dtype=bool)
     kde_sel[selected_indices] = True
-    kde_pts = pts[kde_sel]
+    kde_pts = pts[:, kde_sel]
     spin1 = Bounded_2d_kde(kde_pts, xlow=0, xhigh=.99, ylow=-1, yhigh=1)
-    pts = np.array([spin2, costheta2]).T
-    selected_indices = np.random.choice(len(pts), len(pts) // 2, replace=False)
-    kde_sel = np.zeros(len(pts), dtype=bool)
+    pts = np.array([spin2, costheta2])
+    selected_indices = np.random.choice(pts.shape[1], pts.shape[1] // 2, replace=False)
+    kde_sel = np.zeros(pts.shape[1], dtype=bool)
     kde_sel[selected_indices] = True
-    kde_pts = pts[kde_sel]
+    kde_pts = pts[:, kde_sel]
     spin2 = Bounded_2d_kde(kde_pts, xlow=0, xhigh=.99, ylow=-1, yhigh=1)
 
     rs = np.linspace(0, .99, 25)
     dr = np.abs(rs[1] - rs[0])
     costs = np.linspace(-1, 1, 25)
     dcost = np.abs(costs[1] - costs[0])
     COSTS, RS = np.meshgrid(costs[:-1], rs[:-1])
     X = np.arccos(COSTS) * 180 / np.pi + 90.
     Y = RS
 
     scale = np.exp(1.0)
     spin1_PDF = spin1(
-        np.column_stack([RS.ravel() + dr / 2, COSTS.ravel() + dcost / 2]))
+        np.vstack([RS.ravel() + dr / 2, COSTS.ravel() + dcost / 2]))
     spin2_PDF = spin2(
-        np.column_stack([RS.ravel() + dr / 2, COSTS.ravel() + dcost / 2]))
+        np.vstack([RS.ravel() + dr / 2, COSTS.ravel() + dcost / 2]))
     H1 = np.log(1.0 + scale * spin1_PDF)
     H2 = np.log(1.0 + scale * spin2_PDF)
 
     rect = 121
 
     tr = Affine2D().translate(90, 0) + Affine2D().scale(np.pi / 180., 1.) + \
         PolarAxes.PolarTransform()
```

### Comparing `pesummary-0.9.1/pesummary/gw/plots/violin.py` & `pesummary-1.0.0/pesummary/core/plots/seaborn/violin.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,64 +1,47 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-#                     Seaborn authors
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 from seaborn.categorical import _ViolinPlotter
 import matplotlib as mpl
-from textwrap import dedent
 import colorsys
 import numpy as np
-from scipy import stats
+import math
 import pandas as pd
-from matplotlib.collections import PatchCollection
-import matplotlib.patches as Patches
 import matplotlib.pyplot as plt
-import warnings
 
 from seaborn import utils
-from seaborn.utils import iqr, categorical_order, remove_na
-from seaborn.algorithms import bootstrap
+from seaborn.utils import remove_na
 from seaborn.palettes import color_palette, husl_palette, light_palette, dark_palette
-from seaborn.axisgrid import FacetGrid, _facet_docs
 from scipy.stats import gaussian_kde
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>", "Seaborn authors"]
+
 
 class ViolinPlotter(_ViolinPlotter):
     """A class to extend the _ViolinPlotter class provided by Seaborn
     """
     def __init__(self, x=None, y=None, hue=None, data=None, order=None, hue_order=None,
                  bw="scott", cut=2, scale="area", scale_hue=True, gridsize=100,
                  width=.8, inner="box", split=False, dodge=True, orient=None,
                  linewidth=None, color=None, palette=None, saturation=.75,
-                 ax=None, outer=None, kde=gaussian_kde, kde_kwargs={},
+                 ax=None, outer=None, inj=None, kde=gaussian_kde, kde_kwargs={},
                  weights=None, **kwargs):
         self.multi_color = False
         self.kde = kde
         self.kde_kwargs = kde_kwargs
         self.establish_variables(
             x, y, hue, data, orient, order, hue_order, weights=weights
         )
         self.establish_colors(color, palette, saturation)
         self.estimate_densities(bw, cut, scale, scale_hue, gridsize)
 
         self.gridsize = gridsize
         self.width = width
         self.dodge = dodge
+        self.inj = inj
 
         if inner is not None:
             if not any([inner.startswith("quart"),
                         inner.startswith("box"),
                         inner.startswith("stick"),
                         inner.startswith("point"),
                         inner.startswith("line")]):
@@ -129,15 +112,15 @@
                 else:
                     error = "weights can have no more than 2 dimensions"
                     raise ValueError(error)
             elif np.isscalar(weights[0]):
                 weights_data = [weights]
             else:
                 weights_data = weights
-            weights_data = [np.asarray(d, np.float) for d in weights_data]
+            weights_data = [np.asarray(d, float) for d in weights_data]
         self.weights_data = weights_data
 
     def establish_colors(self, color, palette, saturation):
         """Get a list of colors for the main component of the plots."""
         if self.hue_names is None:
             n_colors = len(self.plot_data)
 
@@ -414,14 +397,22 @@
 
                 if self.outer is None:
                     continue
 
                 else:
                     self.draw_external_range(ax, violin_data, support, density, i)
 
+                if self.inj is None:
+                    continue
+
+                else:
+                    self.draw_injected_line(
+                        ax, self.inj[i], violin_data, support, density, i
+                    )
+
             # Option 2: we have nested grouping by a hue variable
             # ---------------------------------------------------
 
             else:
                 offsets = self.hue_offsets
                 for j, hue_level in enumerate(self.hue_names):
                     support, density = self.support[i][j], self.density[i][j]
@@ -490,14 +481,23 @@
 
                         else:
                             self.draw_external_range(ax, violin_data,
                                                      support, density, i,
                                                      ["left", "right"][j],
                                                      weights=self.weights_data[i][hue_mask])
 
+                        if self.inj is None:
+                            continue
+
+                        else:
+                            self.draw_injected_line(
+                                ax, self.inj[i], violin_data, support, density, i,
+                                ["left", "right"][j]
+                            )
+
                         # The box and point interior plots are drawn for
                         # all data at the group level, so we just do that once
                         if not j:
                             continue
 
                         # Get the whole vector for this group level
                         violin_data = remove_na(group_data)
@@ -645,15 +645,15 @@
 
         if isinstance(self.outer, dict):
             if "percentage" in list(self.outer.keys()):
                 percent = float(self.outer["percentage"])
                 if weights is None:
                     lower, upper = np.percentile(data, [100 - percent, percent])
                 else:
-                    from pesummary.utils.samples_dict import Array
+                    from pesummary.utils.array import Array
 
                     _data = Array(data, weights=weights)
                     lower, upper = _data.confidence_interval(
                         [100 - percent, percent]
                     )
                 h1 = np.min(data[data >= (upper)])
                 h2 = np.max(data[data <= (lower)])
@@ -681,15 +681,15 @@
                 percent = self.outer.split("percent:")[1]
                 percent = float(self._flatten_string(percent))
                 percent += (100 - percent) / 2.
 
                 if weights is None:
                     lower, upper = np.percentile(data, [100 - percent, percent])
                 else:
-                    from pesummary.utils.samples_dict import Array
+                    from pesummary.utils.array import Array
 
                     _data = Array(data, weights=weights)
                     lower, upper = _data.confidence_interval(
                         [100 - percent, percent]
                     )
                 h1 = np.min(data[data >= (upper)])
                 h2 = np.max(data[data <= (lower)])
@@ -703,34 +703,42 @@
 
                 injection = self.outer.split("injection:")[1]
 
                 self._plot_single_line(
                     ax, center, injection, density, split=split, color="r"
                 )
 
+    def draw_injected_line(self, ax, inj, data, support, density,
+                           center, split=None):
+        """Mark the injected value on the violin"""
+        width = self.dwidth * np.max(density) * 1.1
+        if math.isnan(inj):
+            return
+        self._plot_single_line(ax, center, inj, density, split=split, color='r')
+
     def plot(self, ax):
         """Make the violin plot."""
         self.draw_violins(ax)
         self.annotate_axes(ax)
         if self.orient == "h":
             ax.invert_yaxis()
 
 
 def violinplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None,
                bw="scott", cut=2, scale="area", scale_hue=True, gridsize=100,
                width=.8, inner="box", split=False, dodge=True, orient=None,
                linewidth=None, color=None, palette=None, saturation=.75,
-               ax=None, outer=None, kde=gaussian_kde, kde_kwargs={},
+               ax=None, outer=None, inj=None, kde=gaussian_kde, kde_kwargs={},
                weights=None, **kwargs):
 
     plotter = ViolinPlotter(x, y, hue, data, order, hue_order,
                             bw, cut, scale, scale_hue, gridsize,
                             width, inner, split, dodge, orient, linewidth,
                             color, palette, saturation, outer=outer,
-                            kde=kde, kde_kwargs=kde_kwargs, weights=weights)
+                            inj=inj, kde=kde, kde_kwargs=kde_kwargs, weights=weights)
 
     if ax is None:
         ax = plt.gca()
 
     plotter.plot(ax)
     return ax
```

### Comparing `pesummary-0.9.1/pesummary/gw/plots/main.py` & `pesummary-1.0.0/pesummary/core/file/formats/base_read.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,1144 +1,1088 @@
-#! /usr/bin/env python
-
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
-
-from pesummary.core.plots.main import _PlotGeneration as _BasePlotGeneration
-from pesummary.core.plots.latex_labels import latex_labels
-from pesummary.core.plots import interactive
-from pesummary.core.plots.figure import figure
-from pesummary.core.plots.bounded_1d_kde import Bounded_1d_kde
-from pesummary.gw.plots.latex_labels import GWlatex_labels
-from pesummary.utils.utils import (
-    logger, resample_posterior_distribution, get_matplotlib_backend
-)
-from pesummary.utils.decorators import no_latex_plot
-from pesummary.gw.plots import publication
-from pesummary.gw.plots import plot as gw
-from pesummary import conf
-
-import matplotlib
-matplotlib.use(get_matplotlib_backend(parallel=True))
-import multiprocessing as mp
 import numpy as np
+import h5py
+from pesummary.utils.parameters import MultiAnalysisParameters, Parameters
+from pesummary.utils.samples_dict import (
+    MultiAnalysisSamplesDict, SamplesDict, MCMCSamplesDict, Array
+)
+from pesummary.utils.utils import logger
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 
-latex_labels.update(GWlatex_labels)
 
+def _downsample(samples, number, extra_kwargs=None):
+    """Downsample a posterior table
 
-class _PlotGeneration(_BasePlotGeneration):
+    Parameters
+    ----------
+    samples: 2d list
+        list of posterior samples where the columns correspond to a given
+        parameter
+    number: int
+        number of posterior samples you wish to downsample to
+    extra_kwargs: dict, optional
+        dictionary of kwargs to modify
+    """
+    from pesummary.utils.utils import resample_posterior_distribution
+    import copy
+
+    _samples = np.array(samples).T
+    if number > len(_samples[0]):
+        raise ValueError(
+            "Failed to downsample the posterior samples to {} because "
+            "there are only {} samples stored in the file.".format(
+                number, len(_samples[0])
+            )
+        )
+    _samples = np.array(resample_posterior_distribution(_samples, number))
+    if extra_kwargs is None:
+        return _samples.T.tolist()
+    _extra_kwargs = copy.deepcopy(extra_kwargs)
+    _extra_kwargs["sampler"]["nsamples"] = number
+    return _samples.T.tolist(), _extra_kwargs
+
+
+class Read(object):
+    """Base class to read in a results file
+
+    Parameters
+    ----------
+    path_to_results_file: str
+        path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
+
+    Attributes
+    ----------
+    parameters: list
+        list of parameters stored in the result file
+    samples: 2d list
+        list of samples stored in the result file
+    samples_dict: dict
+        dictionary of samples stored in the result file keyed by parameters
+    input_version: str
+        version of the result file passed.
+    extra_kwargs: dict
+        dictionary of kwargs that were extracted from the result file
+    pe_algorithm: str
+        name of the algorithm used to generate the posterior samples
+
+    Methods
+    -------
+    downsample:
+        downsample the posterior samples stored in the result file
+    to_dat:
+        save the posterior samples to a .dat file
+    to_latex_table:
+        convert the posterior samples to a latex table
+    generate_latex_macros:
+        generate a set of latex macros for the stored posterior samples
+    """
     def __init__(
-        self, savedir=None, webdir=None, labels=None, samples=None,
-        kde_plot=False, existing_labels=None, existing_injection_data=None,
-        existing_file_kwargs=None, existing_samples=None,
-        existing_metafile=None, same_parameters=None, injection_data=None,
-        result_files=None, file_kwargs=None, colors=None, custom_plotting=None,
-        add_to_existing=False, priors={}, no_ligo_skymap=False,
-        nsamples_for_skymap=None, detectors=None, maxL_samples=None,
-        gwdata=None, calibration=None, psd=None,
-        multi_threading_for_skymap=None, approximant=None,
-        pepredicates_probs=None, include_prior=False, publication=False,
-        existing_approximant=None, existing_psd=None, existing_calibration=None,
-        existing_weights=None, weights=None, disable_comparison=False,
-        linestyles=None, disable_interactive=False, disable_corner=False,
-        publication_kwargs={}, multi_process=1, mcmc_samples=False,
-        skymap=None, existing_skymap=None, corner_params=None
+        self, path_to_results_file, remove_nan_likelihood_samples=True, **kwargs
     ):
-        super(_PlotGeneration, self).__init__(
-            savedir=savedir, webdir=webdir, labels=labels,
-            samples=samples, kde_plot=kde_plot, existing_labels=existing_labels,
-            existing_injection_data=existing_injection_data,
-            existing_samples=existing_samples,
-            existing_weights=existing_weights,
-            same_parameters=same_parameters,
-            injection_data=injection_data, mcmc_samples=mcmc_samples,
-            colors=colors, custom_plotting=custom_plotting,
-            add_to_existing=add_to_existing, priors=priors,
-            include_prior=include_prior, weights=weights,
-            disable_comparison=disable_comparison, linestyles=linestyles,
-            disable_interactive=disable_interactive, disable_corner=disable_corner,
-            multi_process=multi_process, corner_params=corner_params
-        )
-        self.package = "gw"
-        self.file_kwargs = file_kwargs
-        self.existing_file_kwargs = existing_file_kwargs
-        self.no_ligo_skymap = no_ligo_skymap
-        self.nsamples_for_skymap = nsamples_for_skymap
-        self.detectors = detectors
-        self.maxL_samples = maxL_samples
-        self.gwdata = gwdata
-        if skymap is None:
-            skymap = {label: None for label in self.labels}
-        self.skymap = skymap
-        self.existing_skymap = skymap
-        self.calibration = calibration
-        self.existing_calibration = existing_calibration
-        self.psd = psd
-        self.existing_psd = existing_psd
-        self.multi_threading_for_skymap = multi_threading_for_skymap
-        self.approximant = approximant
-        self.existing_approximant = existing_approximant
-        self.pepredicates_probs = pepredicates_probs
-        self.publication = publication
-        self.publication_kwargs = publication_kwargs
-        self._ligo_skymap_PID = {}
-
-        self.plot_type_dictionary.update({
-            "psd": self.psd_plot,
-            "calibration": self.calibration_plot,
-            "skymap": self.skymap_plot,
-            "waveform_fd": self.waveform_fd_plot,
-            "waveform_td": self.waveform_td_plot,
-            "data": self.gwdata_plots,
-            "violin": self.violin_plot,
-            "spin_disk": self.spin_dist_plot,
-            "pepredicates": self.pepredicates_plot
-        })
-        if self.make_comparison:
-            self.plot_type_dictionary.update({
-                "skymap_comparison": self.skymap_comparison_plot,
-                "waveform_comparison_fd": self.waveform_comparison_fd_plot,
-                "waveform_comparison_td": self.waveform_comparison_td_plot,
-                "2d_comparison_contour": self.twod_comparison_contour_plot,
-            })
-
-    @property
-    def ligo_skymap_PID(self):
-        return self._ligo_skymap_PID
+        self.path_to_results_file = path_to_results_file
+        self.mcmc_samples = False
+        self.remove_nan_likelihood_samples = remove_nan_likelihood_samples
+        self.extension = self.extension_from_path(self.path_to_results_file)
+        self.converted_parameters = []
+
+    @classmethod
+    def load_file(cls, path, **kwargs):
+        """Initialize the class with a file
 
-    def generate_plots(self):
-        """Generate all plots for all result files
-        """
-        for i in self.labels:
-            logger.debug("Starting to generate plots for {}".format(i))
-            self._generate_plots(i)
-            if self.make_interactive:
-                logger.debug(
-                    "Starting to generate interactive plots for {}".format(i)
-                )
-                self._generate_interactive_plots(i)
-        if self.calibration or "calibration" in list(self.priors.keys()):
-            self.try_to_make_a_plot("calibration")
-        if self.psd:
-            self.try_to_make_a_plot("psd")
-        if self.add_to_existing:
-            self.add_existing_data()
-        if self.make_comparison:
-            logger.debug("Starting to generate comparison plots")
-            self._generate_comparison_plots()
-
-    def _generate_plots(self, label):
-        """Generate all plots for a given result file
-        """
-        super(_PlotGeneration, self)._generate_plots(label)
-        self.try_to_make_a_plot("skymap", label=label)
-        self.try_to_make_a_plot("waveform_td", label=label)
-        self.try_to_make_a_plot("waveform_fd", label=label)
-        if self.pepredicates_probs[label] is not None:
-            self.try_to_make_a_plot("pepredicates", label=label)
-        if self.gwdata:
-            self.try_to_make_a_plot("data", label=label)
-
-    def _generate_comparison_plots(self):
-        """Generate all comparison plots
-        """
-        super(_PlotGeneration, self)._generate_comparison_plots()
-        self.try_to_make_a_plot("skymap_comparison")
-        self.try_to_make_a_plot("waveform_comparison_td")
-        self.try_to_make_a_plot("waveform_comparison_fd")
-        if self.publication:
-            self.try_to_make_a_plot("2d_comparison_contour")
-            self.try_to_make_a_plot("violin")
-            self.try_to_make_a_plot("spin_disk")
+        Parameters
+        ----------
+        path: str
+            path to the result file you wish to load
+        **kwargs: dict, optional
+            all kwargs passed to the class
+        """
+        if not os.path.isfile(path):
+            raise FileNotFoundError("%s does not exist" % (path))
+        return cls(path, **kwargs)
 
     @staticmethod
-    def _corner_plot(savedir, label, samples, latex_labels, webdir, params):
-        """Generate a corner plot for a given set of samples
+    def load_from_function(function, path_to_file, **kwargs):
+        """Load a file according to a given function
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        label: str
-            the label corresponding to the results file
-        samples: dict
-            dictionary of samples for a given result file
-        latex_labels: dict
-            dictionary of latex labels
-        webdir: str
-            directory where the javascript is written
-        """
-        import warnings
-
-        with warnings.catch_warnings():
-            warnings.simplefilter("ignore")
-            fig, params, data = gw._make_corner_plot(
-                samples, latex_labels, corner_parameters=params
-            )
-            fig.savefig(
-                os.path.join(
-                    savedir, "corner", "{}_all_density_plots.png".format(
-                        label
-                    )
-                )
-            )
-            fig.close()
-            combine_corner = open(
-                os.path.join(webdir, "js", "combine_corner.js")
-            )
-            combine_corner = combine_corner.readlines()
-            params = [str(i) for i in params]
-            ind = [
-                linenumber for linenumber, line in enumerate(combine_corner)
-                if "var list = {}" in line
-            ][0]
-            combine_corner.insert(
-                ind + 1, "    list['{}'] = {};\n".format(label, params)
-            )
-            new_file = open(
-                os.path.join(webdir, "js", "combine_corner.js"), "w"
-            )
-            new_file.writelines(combine_corner)
-            new_file.close()
-            combine_corner = open(
-                os.path.join(webdir, "js", "combine_corner.js")
-            )
-            combine_corner = combine_corner.readlines()
-            params = [str(i) for i in params]
-            ind = [
-                linenumber for linenumber, line in enumerate(combine_corner)
-                if "var data = {}" in line
-            ][0]
-            combine_corner.insert(
-                ind + 1, "    data['{}'] = {};\n".format(label, data)
-            )
-            new_file = open(
-                os.path.join(webdir, "js", "combine_corner.js"), "w"
-            )
-            new_file.writelines(combine_corner)
-            new_file.close()
-            fig = gw._make_source_corner_plot(samples, latex_labels)
-            fig.savefig(
-                os.path.join(
-                    savedir, "corner", "{}_sourceframe.png".format(label)
-                )
-            )
-            fig.close()
-            fig = gw._make_extrinsic_corner_plot(samples, latex_labels)
-            fig.savefig(
-                os.path.join(
-                    savedir, "corner", "{}_extrinsic.png".format(label)
-                )
-            )
-            fig.close()
+        function: func
+            callable function that will load in your file
+        path_to_file: str
+            path to the file that you wish to load
+        kwargs: dict
+            all kwargs are passed to the function
+        """
+        return function(path_to_file, **kwargs)
 
-    def skymap_plot(self, label):
-        """Generate a skymap plot for a given result file
+    @staticmethod
+    def check_for_nan_likelihoods(parameters, samples, remove=False):
+        """Check to see if there are any samples with log_likelihood='nan' in
+        the posterior table and remove if requested
 
         Parameters
         ----------
-        label: str
-            the label for the results file that you wish to plot
+        parameters: list
+            list of parameters stored in the result file
+        samples: np.ndarray
+            array of samples for each parameter
+        remove: Bool, optional
+            if True, remove samples with log_likelihood='nan' from samples
         """
-        try:
-            import ligo.skymap
-            SKYMAP = True
-        except ImportError:
-            SKYMAP = False
-
-        if self.mcmc_samples:
-            samples = self.samples[label].combine
+        import math
+        if "log_likelihood" not in parameters:
+            return parameters, samples
+        ind = parameters.index("log_likelihood")
+        likelihoods = np.array(samples).T[ind]
+        inds = np.array(
+            [math.isnan(_) for _ in likelihoods], dtype=bool
+        )
+        if not sum(inds):
+            return parameters, samples
+        msg = (
+            "Posterior table contains {} samples with 'nan' log likelihood. "
+        )
+        if remove:
+            msg += "Removing samples from posterior table."
+            samples = np.array(samples)[~inds].tolist()
         else:
-            samples = self.samples[label]
-        self._skymap_plot(
-            self.savedir, samples["ra"], samples["dec"], label,
-            self.weights[label],
-            [self.injection_data[label]["ra"], self.injection_data[label]["dec"]]
-        )
-
-        if SKYMAP and not self.no_ligo_skymap and self.skymap[label] is None:
-            from pesummary.utils.utils import RedirectLogger
-
-            logger.info("Launching subprocess to generate skymap plot with "
-                        "ligo.skymap")
-            try:
-                _time = samples["geocent_time"]
-            except KeyError:
-                logger.warning(
-                    "Unable to find 'geocent_time' in the posterior table for {}. "
-                    "The ligo.skymap fits file will therefore not store the "
-                    "DATE_OBS field in the header".format(label)
-                )
-                _time = None
-            with RedirectLogger("ligo.skymap", level="DEBUG") as redirector:
-                process = mp.Process(
-                    target=self._ligo_skymap_plot,
-                    args=[
-                        self.savedir, samples["ra"], samples["dec"],
-                        samples["luminosity_distance"], _time,
-                        label, self.nsamples_for_skymap, self.webdir,
-                        self.multi_threading_for_skymap
-                    ]
-                )
-                process.start()
-                PID = process.pid
-            self._ligo_skymap_PID[label] = PID
-        elif SKYMAP and not self.no_ligo_skymap:
-            self._ligo_skymap_array_plot(self.savedir, self.skymap[label], label)
+            msg += "This may cause problems when analysing posterior samples."
+        logger.warning(msg.format(sum(inds)))
+        return parameters, samples
 
     @staticmethod
-    @no_latex_plot
-    def _skymap_plot(savedir, ra, dec, label, weights, injection=None):
-        """Generate a skymap plot for a given set of samples
+    def check_for_weights(parameters, samples):
+        """Check to see if the samples are weighted
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        ra: pesummary.utils.utils.Array
-            array containing the samples for right ascension
-        dec: pesummary.utils.utils.Array
-            array containing the samples for declination
-        label: str
-            the label corresponding to the results file
-        weights: list
-            list of weights for the samples
-        injection: list, optional
-            list containing the injected value of ra and dec
+        parameters: list
+            list of parameters stored in the result file
+        samples: np.ndarray
+            array of samples for each parameter
         """
-        import math
+        likely_names = ["weights", "weight"]
+        if any(i in parameters for i in likely_names):
+            ind = (
+                parameters.index("weights") if "weights" in parameters else
+                parameters.index("weight")
+            )
+            return Array(np.array(samples).T[ind])
+        return None
 
-        if injection is not None and any(math.isnan(inj) for inj in injection):
-            injection = None
-        fig = gw._default_skymap_plot(ra, dec, weights, injection=injection)
-        _PlotGeneration.save(
-            fig, os.path.join(savedir, "{}_skymap".format(label))
-        )
+    @property
+    def pe_algorithm(self):
+        try:
+            return self.extra_kwargs["sampler"]["pe_algorithm"]
+        except KeyError:
+            return None
 
-    @staticmethod
-    @no_latex_plot
-    def _ligo_skymap_plot(savedir, ra, dec, dist, time, label, nsamples_for_skymap,
-                          webdir, multi_threading_for_skymap):
-        """Generate a skymap plot for a given set of samples using the
-        ligo.skymap package
+    def __repr__(self):
+        return self.summary()
+
+    def _parameter_summary(self, parameters, parameters_to_show=4):
+        """Return a summary of the parameter stored
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        ra: pesummary.utils.utils.Array
-            array containing the samples for right ascension
-        dec: pesummary.utils.utils.Array
-            array containing the samples for declination
-        dist: pesummary.utils.utils.Array
-            array containing the samples for luminosity distance
-        time: pesummary.utils.utils.Array
-            array containing the samples for the geocentric time of merger
-        label: str
-            the label corresponding to the results file
-        nsamples_for_skymap: int
-            the number of samples used to generate skymap
-        webdir: str
-            the directory to store the fits file
-        """
-        downsampled = False
-        if nsamples_for_skymap is not None:
-            ra, dec, dist = resample_posterior_distribution(
-                [ra, dec, dist], nsamples_for_skymap
-            )
-            downsampled = True
-        fig = gw._ligo_skymap_plot(
-            ra, dec, dist=dist, savedir=os.path.join(webdir, "samples"),
-            nprocess=multi_threading_for_skymap, downsampled=downsampled,
-            label=label, time=time
-        )
-        _PlotGeneration.save(
-            fig, os.path.join(savedir, "{}_skymap".format(label))
-        )
+        parameters: list
+            list of parameters to create a summary for
+        parameters_to_show: int, optional
+            number of parameters to show. Default 4.
+        """
+        params = parameters
+        if len(parameters) > parameters_to_show:
+            params = parameters[:2] + ["..."] + parameters[-2:]
+        return ", ".join(params)
 
-    @staticmethod
-    @no_latex_plot
-    def _ligo_skymap_array_plot(savedir, skymap, label):
-        """Generate a skymap based on skymap probability array already generated with
-        `ligo.skymap`
+    def summary(
+        self, parameters_to_show=4, show_parameters=True, show_nsamples=True
+    ):
+        """Return a summary of the contents of the file
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        skymap: np.ndarray
-            array of skymap probabilities
-        label: str
-            the label corresponding to the results file
-        """
-        fig = gw._ligo_skymap_plot_from_array(skymap)
-        _PlotGeneration.save(fig, os.path.join(savedir, "{}_skymap".format(label)))
+        parameters_to_show: int, optional
+            number of parameters to show. Default 4
+        show_parameters: Bool, optional
+            if True print a list of the parameters stored
+        show_nsamples: Bool, optional
+            if True print how many samples are stored in the file
+        """
+        string = ""
+        if self.path_to_results_file is not None:
+            string += "file: {}\n".format(self.path_to_results_file)
+        string += "cls: {}.{}\n".format(
+            self.__class__.__module__, self.__class__.__name__
+        )
+        if show_nsamples:
+            string += "nsamples: {}\n".format(len(self.samples))
+        if show_parameters:
+            string += "parameters: {}".format(
+                self._parameter_summary(
+                    self.parameters, parameters_to_show=parameters_to_show
+                )
+            )
+        return string
+
+    attrs = {
+        "input_version": "version", "extra_kwargs": "kwargs",
+        "priors": "prior", "analytic": "analytic", "labels": "labels",
+        "config": "config", "weights": "weights", "history": "history",
+        "description": "description"
+    }
 
-    def waveform_fd_plot(self, label):
-        """Generate a frequency domain waveform plot for a given result file
+    def _load(self, function, **kwargs):
+        """Extract the data from a file using a given function
 
         Parameters
         ----------
-        label: str
-            the label corresponding to the results file
+        function: func
+            function you wish to use to extract the data
+        **kwargs: dict, optional
+            optional kwargs to pass to the load function
         """
-        if self.approximant[label] == {}:
-            return
-        self._waveform_fd_plot(
-            self.savedir, self.detectors[label], self.maxL_samples[label], label
+        return self.load_from_function(
+            function, self.path_to_results_file, **kwargs
         )
 
-    @staticmethod
-    def _waveform_fd_plot(savedir, detectors, maxL_samples, label):
-        """Generate a frequency domain waveform plot for a given detector
-        network and set of samples
+    def load(self, function, _data=None, **kwargs):
+        """Load a results file according to a given function
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        detectors: list
-            list of detectors used in your analysis
-        maxL_samples: dict
-            dictionary of maximum likelihood values
-        label: str
-            the label corresponding to the results file
+        function: func
+            callable function that will load in your results file
         """
-        if detectors is None:
-            detectors = ["H1", "L1"]
+        self.data = _data
+        if _data is None:
+            self.data = self._load(function, **kwargs)
+        if isinstance(self.data["parameters"][0], list):
+            _cls = MultiAnalysisParameters
         else:
-            detectors = detectors.split("_")
-
-        fig = gw._waveform_plot(detectors, maxL_samples)
-        _PlotGeneration.save(
-            fig, os.path.join(savedir, "{}_waveform".format(label))
-        )
+            _cls = Parameters
+        self.parameters = _cls(self.data["parameters"])
+        self.converted_parameters = []
+        self.samples = self.data["samples"]
+        self.parameters, self.samples = self.check_for_nan_likelihoods(
+            self.parameters, self.samples,
+            remove=self.remove_nan_likelihood_samples
+        )
+        if "mcmc_samples" in self.data.keys():
+            self.mcmc_samples = self.data["mcmc_samples"]
+        if "injection" in self.data.keys():
+            if isinstance(self.data["injection"], dict):
+                self.injection_parameters = {
+                    key.decode("utf-8") if isinstance(key, bytes) else key: val
+                    for key, val in self.data["injection"].items()
+                }
+            elif isinstance(self.data["injection"], list):
+                self.injection_parameters = [
+                    {
+                        key.decode("utf-8") if isinstance(key, bytes) else
+                        key: val for key, val in i.items()
+                    } for i in self.data["injection"]
+                ]
+            else:
+                self.injection_parameters = self.data["injection"]
+        for new_attr, existing_attr in self.attrs.items():
+            if existing_attr in self.data.keys():
+                setattr(self, new_attr, self.data[existing_attr])
+            else:
+                setattr(self, new_attr, None)
+        if self.input_version is None:
+            self.input_version = self._default_version
+        if self.extra_kwargs is None:
+            self.extra_kwargs = self._default_kwargs
+        if self.description is None:
+            self.description = self._default_description
+        if self.weights is None:
+            self.weights = self.check_for_weights(self.parameters, self.samples)
 
-    def waveform_td_plot(self, label):
-        """Generate a time domain waveform plot for a given result file
+    @staticmethod
+    def extension_from_path(path):
+        """Return the extension of the file from the file path
 
         Parameters
         ----------
-        label: str
-            the label corresponding to the results file
+        path: str
+            path to the results file
         """
-        if self.approximant[label] == {}:
-            return
-        self._waveform_td_plot(
-            self.savedir, self.detectors[label], self.maxL_samples[label], label
-        )
+        extension = path.split(".")[-1]
+        return extension
 
     @staticmethod
-    def _waveform_td_plot(savedir, detectors, maxL_samples, label):
-        """Generate a time domain waveform plot for a given detector network
-        and set of samples
+    def guess_path_to_samples(path):
+        """Guess the path to the posterior samples stored in an hdf5 object
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        detectors: list
-            list of detectors used in your analysis
-        maxL_samples: dict
-            dictionary of maximum likelihood values
-        label: str
-            the label corresponding to the results file
+        path: str
+            path to the results file
         """
-        if detectors is None:
-            detectors = ["H1", "L1"]
+        def _find_name(name, item):
+            c1 = "posterior_samples" in name or "posterior" in name
+            c2 = isinstance(item, (h5py._hl.dataset.Dataset, np.ndarray))
+            try:
+                c3 = isinstance(item, h5py._hl.group.Group) and isinstance(
+                    item[0], (float, int, np.number)
+                )
+            except (TypeError, AttributeError):
+                c3 = False
+            c4 = (
+                isinstance(item, h5py._hl.group.Group) and "parameter_names" in
+                item.keys() and "samples" in item.keys()
+            )
+            if c1 and c3:
+                paths.append(name)
+            elif c1 and c4:
+                return paths.append(name)
+            elif c1 and c2:
+                return paths.append(name)
+
+        f = h5py.File(path, 'r')
+        paths = []
+        f.visititems(_find_name)
+        f.close()
+        if len(paths) == 1:
+            return paths[0]
+        elif len(paths) > 1:
+            raise ValueError(
+                "Found multiple posterior sample tables in '{}': {}. Not sure "
+                "which to load.".format(
+                    path, ", ".join(paths)
+                )
+            )
         else:
-            detectors = detectors.split("_")
-
-        fig = gw._time_domain_waveform(detectors, maxL_samples)
-        _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "{}_waveform_time_domain".format(label)
+            raise ValueError(
+                "Unable to find a posterior samples table in '{}'".format(path)
             )
-        )
 
-    def gwdata_plots(self, label):
-        """Generate all plots associated with the gwdata
+    def generate_all_posterior_samples(self, **kwargs):
+        """Empty function
+        """
+        pass
+
+    def add_fixed_parameters_from_config_file(self, config_file):
+        """Search the conifiguration file and add fixed parameters to the
+        list of parameters and samples
 
         Parameters
         ----------
-        label: str
-            the label corresponding to the results file
+        config_file: str
+            path to the configuration file
         """
-        from pesummary.utils.utils import determine_gps_time_and_window
-
-        base_error = "Failed to generate a %s because {}"
-        gps_time, window = determine_gps_time_and_window(
-            self.maxL_samples, self.labels
-        )
-        functions = [
-            self.strain_plot, self.spectrogram_plot, self.omegascan_plot
-        ]
-        args = [[label], [], [gps_time, window]]
-        func_names = ["strain_plot", "spectrogram plot", "omegascan plot"]
-
-        for func, args, name in zip(functions, args, func_names):
-            self._try_to_make_a_plot(args, func, base_error % (name))
-            continue
+        pass
 
-    def strain_plot(self, label):
-        """Generate a plot showing the comparison between the data and the
-        maxL waveform gfor a given result file
+    def add_injection_parameters_from_file(self, injection_file, **kwargs):
+        """Populate the 'injection_parameters' property with data from a file
 
         Parameters
         ----------
-        label: str
-            the label corresponding to the results file
+        injection_file: str
+            path to injection file
         """
-        from pesummary.utils.utils import RedirectLogger
-
-        logger.info("Launching subprocess to generate strain plot")
-        process = mp.Process(
-            target=self._strain_plot,
-            args=[self.savedir, self.gwdata, self.maxL_samples[label], label]
+        self.injection_parameters = self._grab_injection_parameters_from_file(
+            injection_file, **kwargs
         )
-        process.start()
 
-    @staticmethod
-    def _strain_plot(savedir, gwdata, maxL_samples, label):
-        """Generate a strain plot for a given set of samples
+    def _grab_injection_parameters_from_file(
+        self, path, cls=None, add_nans=True, **kwargs
+    ):
+        """Extract data from an injection file
 
         Parameters
         ----------
-        savedir: str
-            the directory to save the plot
-        gwdata: dict
-            dictionary of strain data for each detector
-        maxL_samples: dict
-            dictionary of maximum likelihood values
-        label: str
-            the label corresponding to the results file
-        """
-        fig = gw._strain_plot(gwdata, maxL_samples)
-        _PlotGeneration.save(
-            fig, os.path.join(savedir, "{}_straing".format(label))
-        )
-
-    def spectrogram_plot(self):
-        """Generate a plot showing the spectrogram for all detectors
-        """
-        figs = self._spectrogram_plot(self.savedir, self.gwdata)
-
-    @staticmethod
-    def _spectrogram_plot(savedir, strain):
-        """Generate a plot showing the spectrogram for all detectors
+        path: str
+            path to injection file
+        cls: class, optional
+            class to read in injection file. The class must have a read class
+            method and a samples_dict property. Default None which means that
+            the pesummary.core.file.injection.Injection class is used
+        """
+        if cls is None:
+            from pesummary.core.file.injection import Injection
+            cls = Injection
+        data = cls.read(path, **kwargs).samples_dict
+        for i in self.parameters:
+            if i not in data.keys():
+                data[i] = float("nan")
+        return data
+
+    def write(
+        self, package="core", file_format="dat", extra_kwargs=None,
+        file_versions=None, **kwargs
+    ):
+        """Save the data to file
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        strain: dict
-            dictionary of gwpy timeseries objects containing the strain data for
-            each IFO
+        package: str, optional
+            package you wish to use when writing the data
+        kwargs: dict, optional
+            all additional kwargs are passed to the pesummary.io.write function
         """
-        from pesummary.gw.plots import detchar
+        from pesummary.io import write
 
-        figs = detchar.spectrogram(strain)
-        for det, fig in figs.items():
-            _PlotGeneration.save(
-                fig, os.path.join(savedir, "spectrogram_{}".format(det))
+        if file_format == "pesummary" and np.array(self.parameters).ndim > 1:
+            args = [self.samples_dict]
+        else:
+            args = [self.parameters, self.samples]
+        if extra_kwargs is None:
+            extra_kwargs = self.extra_kwargs
+        if file_versions is None:
+            file_versions = self.input_version
+        if file_format == "ini":
+            kwargs["file_format"] = "ini"
+            return write(getattr(self, "config", None), **kwargs)
+        else:
+            return write(
+                *args, package=package, file_versions=file_versions,
+                file_kwargs=extra_kwargs, file_format=file_format, **kwargs
             )
 
-    def omegascan_plot(self, gps_time, window):
-        """Generate a plot showing the omegascan for all detectors
-
-        Parameters
-        ----------
-        gps_time: float
-            time around which to centre the omegascan
-        window: float
-            window around gps time to generate plot for
+    def downsample(self, number):
+        """Downsample the posterior samples stored in the result file
         """
-        figs = self._omegascan_plot(
-            self.savedir, self.gwdata, gps_time, window
+        self.samples, self.extra_kwargs = _downsample(
+            self.samples, number, extra_kwargs=self.extra_kwargs
         )
 
     @staticmethod
-    def _omegascan_plot(savedir, strain, gps, window):
-        """Generate a plot showing the spectrogram for all detectors
+    def latex_table(samples, parameter_dict=None, labels=None):
+        """Return a latex table displaying the passed data.
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        strain: dict
-            dictionary of gwpy timeseries objects containing the strain data for
-            each IFO
-        gps: float
-            time around which to centre the omegascan
-        window: float
-            window around gps time to generate plot for
-        """
-        from pesummary.gw.plots import detchar
-
-        figs = detchar.omegascan(strain, gps, window=window)
-        for det, fig in figs.items():
-            _PlotGeneration.save(
-                fig, os.path.join(savedir, "omegascan_{}".format(det))
-            )
-
-    def skymap_comparison_plot(self, label):
-        """Generate a plot to compare skymaps for all result files
+        samples_dict: list
+            list of pesummary.utils.utils.SamplesDict objects
+        parameter_dict: dict, optional
+            dictionary of parameters that you wish to include in the latex
+            table. The keys are the name of the parameters and the items are
+            the descriptive text. If None, all parameters are included
+        """
+        table = (
+            "\\begin{table}[hptb]\n\\begin{ruledtabular}\n\\begin{tabular}"
+            "{l %s}\n" % ("c " * len(samples))
+        )
+        if labels:
+            table += (
+                " & " + " & ".join(labels)
+            )
+            table += "\\\ \n\\hline \\\ \n"
+        data = {i: i for i in samples[0].keys()}
+        if parameter_dict is not None:
+            import copy
+
+            data = copy.deepcopy(parameter_dict)
+            for param in parameter_dict.keys():
+                if not all(param in samples_dict.keys() for samples_dict in samples):
+                    logger.warning(
+                        "{} not in list of parameters. Not adding to "
+                        "table".format(param)
+                    )
+                    data.pop(param)
 
-        Parameters
-        ----------
-        label: str
-            the label for the results file that you wish to plot
-        """
-        self._skymap_comparison_plot(
-            self.savedir, self.same_samples["ra"], self.same_samples["dec"],
-            self.labels, self.colors
+        for param, desc in data.items():
+            table += "{}".format(desc)
+            for samples_dict in samples:
+                median = samples_dict[param].average(type="median")
+                confidence = samples_dict[param].confidence_interval()
+                table += (
+                    " & $%s^{+%s}_{-%s}$" % (
+                        np.round(median, 2),
+                        np.round(confidence[1] - median, 2),
+                        np.round(median - confidence[0], 2)
+                    )
+                )
+            table += "\\\ \n"
+        table += (
+            "\\end{tabular}\n\\end{ruledtabular}\n\\caption{}\n\\end{table}"
         )
+        return table
 
     @staticmethod
-    def _skymap_comparison_plot(savedir, ra, dec, labels, colors):
-        """Generate a plot to compare skymaps for a given set of samples
+    def latex_macros(
+        samples, parameter_dict=None, labels=None, rounding="smart"
+    ):
+        """Return a latex table displaying the passed data.
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        ra: dict
-            dictionary of right ascension samples for each result file
-        dec: dict
-            dictionary of declination samples for each result file
-        labels: list
-            list of labels to distinguish each result file
-        colors: list
-            list of colors to be used to distinguish different result files
-        """
-        ra_list = [ra[key] for key in labels]
-        dec_list = [dec[key] for key in labels]
-        fig = gw._sky_map_comparison_plot(ra_list, dec_list, labels, colors)
-        _PlotGeneration.save(fig, os.path.join(savedir, "combined_skymap"))
+        samples_dict: list
+            list of pesummary.utils.utils.SamplesDict objects
+        parameter_dict: dict, optional
+            dictionary of parameters that you wish to generate macros for. The
+            keys are the name of the parameters and the items are the latex
+            macros name you wish to use. If None, all parameters are included.
+        rounding: int, optional
+            decimal place for rounding. Default uses the
+            `pesummary.utils.utils.smart_round` function to round according to
+            the uncertainty
+        """
+        macros = ""
+        data = {i: i for i in samples[0].keys()}
+        if parameter_dict is not None:
+            import copy
+
+            data = copy.deepcopy(parameter_dict)
+            for param in parameter_dict.keys():
+                if not all(param in samples_dict.keys() for samples_dict in samples):
+                    logger.warning(
+                        "{} not in list of parameters. Not generating "
+                        "macro".format(param)
+                    )
+                    data.pop(param)
+        for param, desc in data.items():
+            for num, samples_dict in enumerate(samples):
+                if labels:
+                    description = "{}{}".format(desc, labels[num])
+                else:
+                    description = desc
+
+                median = samples_dict[param].average(type="median")
+                confidence = samples_dict[param].confidence_interval()
+                if rounding == "smart":
+                    from pesummary.utils.utils import smart_round
+
+                    median, upper, low = smart_round([
+                        median, confidence[1] - median, median - confidence[0]
+                    ])
+                else:
+                    median = np.round(median, rounding)
+                    low = np.round(median - confidence[0], rounding)
+                    upper = np.round(confidence[1] - median, rounding)
+                macros += (
+                    "\\def\\%s{$%s_{-%s}^{+%s}$}\n" % (
+                        description, median, low, upper
+                    )
+                )
+                macros += (
+                    "\\def\\%smedian{$%s$}\n" % (description, median)
+                )
+                macros += (
+                    "\\def\\%supper{$%s$}\n" % (
+                        description, np.round(median + upper, 9)
+                    )
+                )
+                macros += (
+                    "\\def\\%slower{$%s$}\n" % (
+                        description, np.round(median - low, 9)
+                    )
+                )
+        return macros
 
-    def waveform_comparison_fd_plot(self, label):
-        """Generate a plot to compare the frequency domain waveform
 
-        Parameters
-        ----------
-        label: str
-            the label for the results file that you wish to plot
-        """
-        if any(self.approximant[i] == {} for i in self.labels):
-            return
+class SingleAnalysisRead(Read):
+    """Base class to read in a results file which contains a single analyses
 
-        self._waveform_comparison_fd_plot(
-            self.savedir, self.maxL_samples, self.labels, self.colors
-        )
+    Parameters
+    ----------
+    path_to_results_file: str
+        path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
+
+    Attributes
+    ----------
+    parameters: list
+        list of parameters stored in the file
+    samples: 2d list
+        list of samples stored in the result file
+    samples_dict: dict
+        dictionary of samples stored in the result file
+    input_version: str
+        version of the result file passed
+    extra_kwargs: dict
+        dictionary of kwargs that were extracted from the result file
+
+    Methods
+    -------
+    downsample:
+        downsample the posterior samples stored in the result file
+    to_dat:
+        save the posterior samples to a .dat file
+    to_latex_table:
+        convert the posterior samples to a latex table
+    generate_latex_macros:
+        generate a set of latex macros for the stored posterior samples
+    reweight_samples:
+        reweight the posterior and/or samples according to a new prior
+    """
+    def __init__(self, *args, **kwargs):
+        super(SingleAnalysisRead, self).__init__(*args, **kwargs)
 
-    @staticmethod
-    def _waveform_comparison_fd_plot(savedir, maxL_samples, labels, colors):
-        """Generate a plot to compare the frequency domain waveforms
+    @property
+    def samples_dict(self):
+        if self.mcmc_samples:
+            return MCMCSamplesDict(
+                self.parameters, [np.array(i).T for i in self.samples]
+            )
+        return SamplesDict(self.parameters, np.array(self.samples).T)
 
-        Parameters
-        ----------
-        savedir: str
-            the directory you wish to save the plot in
-        maxL_samples: dict
-            dictionary of maximum likelihood samples for each result file
-        labels: list
-            list of labels to distinguish each result file
-        colors: list
-            list of colors to be used to distinguish different result files
-        """
-        samples = [maxL_samples[i] for i in labels]
-        fig = gw._waveform_comparison_plot(samples, colors, labels)
-        _PlotGeneration.save(
-            fig, os.path.join(savedir, "compare_waveforms")
-        )
+    @property
+    def _default_version(self):
+        return "No version information found"
+
+    @property
+    def _default_kwargs(self):
+        _kwargs = {"sampler": {}, "meta_data": {}}
+        _kwargs["sampler"]["nsamples"] = len(self.data["samples"])
+        return _kwargs
 
-    def waveform_comparison_td_plot(self, label):
-        """Generate a plot to compare the time domain waveform
+    @property
+    def _default_description(self):
+        return "No description found"
+
+    def _add_fixed_parameters_from_config_file(self, config_file, function):
+        """Search the conifiguration file and add fixed parameters to the
+        list of parameters and samples
 
         Parameters
         ----------
-        label: str
-            the label for the results file that you wish to plot
+        config_file: str
+            path to the configuration file
+        function: func
+            function you wish to use to extract the information from the
+            configuration file
         """
-        if any(self.approximant[i] == {} for i in self.labels):
-            return
-
-        self._waveform_comparison_fd_plot(
-            self.savedir, self.maxL_samples, self.labels, self.colors
-        )
+        self.data[0], self.data[1] = function(self.parameters, self.samples, config_file)
 
-    @staticmethod
-    def _waveform_comparison_td_plot(savedir, maxL_samples, labels, colors):
-        """Generate a plot to compare the time domain waveforms
+    def _add_marginalized_parameters_from_config_file(self, config_file, function):
+        """Search the configuration file and add marginalized parameters to the
+        list of parameters and samples
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        maxL_samples: dict
-            dictionary of maximum likelihood samples for each result file
-        labels: list
-            list of labels to distinguish each result file
-        colors: list
-            list of colors to be used to distinguish different result files
-        """
-        samples = [maxL_samples[i] for i in labels]
-        fig = gw._time_domainwaveform_comparison_plot(samples, colors, labels)
-        _PlotGeneration.save(
-            fig, os.path.join(savedir, "compare_time_domain_waveforms")
-        )
+        config_file: str
+            path to the configuration file
+        function: func
+            function you wish to use to extract the information from the
+            configuration file
+        """
+        self.data[0], self.data[1] = function(self.parameters, self.samples, config_file)
 
-    def twod_comparison_contour_plot(self, label):
-        """Generate 2d comparison contour plots
+    def to_latex_table(self, parameter_dict=None, save_to_file=None):
+        """Make a latex table displaying the data in the result file.
 
         Parameters
         ----------
-        label: str
-            the label for the results file that you wish to plot
+        parameter_dict: dict, optional
+            dictionary of parameters that you wish to include in the latex
+            table. The keys are the name of the parameters and the items are
+            the descriptive text. If None, all parameters are included
+        save_to_file: str, optional
+            name of the file you wish to save the latex table to. If None, print
+            to stdout
         """
-        error_message = (
-            "Failed to generate a 2d contour plot for %s because {}"
-        )
-        twod_plots = [
-            ["mass_ratio", "chi_eff"], ["mass_1", "mass_2"],
-            ["luminosity_distance", "chirp_mass_source"],
-            ["mass_1_source", "mass_2_source"],
-            ["theta_jn", "luminosity_distance"],
-            ["network_optimal_snr", "chirp_mass_source"]
-        ]
-        gridsize = (
-            int(self.publication_kwargs["gridsize"]) if "gridsize" in
-            self.publication_kwargs.keys() else 100
-        )
-        for plot in twod_plots:
-            if not all(
-                all(
-                    i in self.samples[j].keys() for i in plot
-                ) for j in self.labels
-            ):
-                logger.warning(
-                    "Failed to generate 2d contour plots for {} because {} are not "
-                    "common in all result files".format(
-                        " and ".join(plot), " and ".join(plot)
-                    )
-                )
-                continue
-            samples = [[self.samples[i][j] for j in plot] for i in self.labels]
-            arguments = [
-                self.savedir, plot, samples, self.labels, latex_labels,
-                self.colors, self.linestyles, gridsize
-            ]
-            self._try_to_make_a_plot(
-                arguments, self._twod_comparison_contour_plot,
-                error_message % (" and ".join(plot))
+        import os
+
+        if save_to_file is not None and os.path.isfile("{}".format(save_to_file)):
+            raise FileExistsError(
+                "The file {} already exists.".format(save_to_file)
             )
 
-    @staticmethod
-    def _twod_comparison_contour_plot(
-        savedir, plot_parameters, samples, labels, latex_labels, colors,
-        linestyles, gridsize
+        table = self.latex_table([self.samples_dict], parameter_dict)
+        if save_to_file is None:
+            print(table)
+        elif os.path.isfile("{}".format(save_to_file)):
+            logger.warning(
+                "File {} already exists. Printing to stdout".format(save_to_file)
+            )
+            print(table)
+        else:
+            with open(save_to_file, "w") as f:
+                f.writelines([table])
+
+    def generate_latex_macros(
+        self, parameter_dict=None, save_to_file=None, rounding="smart"
     ):
-        """Generate a 2d comparison contour plot for a given set of samples
+        """Generate a list of latex macros for each parameter in the result
+        file
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        plot_parameters: list
-            list of parameters to use for the 2d contour plot
-        samples: list
-            list of samples for each parameter
-        labels: list
-            list of labels used to distinguish each result file
-        latex_labels: dict
-            dictionary containing the latex labels for each parameter
-        gridsize: int
-            the number of points to use when estimating the KDE
-        """
-        fig = publication.twod_contour_plots(
-            plot_parameters, samples, labels, latex_labels, colors=colors,
-            linestyles=linestyles, gridsize=gridsize
-        )
-        _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "publication", "2d_contour_plot_{}".format(
-                    "_and_".join(plot_parameters)
-                )
+        labels: list, optional
+            list of labels that you want to include in the table
+        parameter_dict: dict, optional
+            dictionary of parameters that you wish to generate macros for. The
+            keys are the name of the parameters and the items are the latex
+            macros name you wish to use. If None, all parameters are included.
+        save_to_file: str, optional
+            name of the file you wish to save the latex table to. If None, print
+            to stdout
+        rounding: int, optional
+            number of decimal points to round the latex macros
+        """
+        import os
+
+        if save_to_file is not None and os.path.isfile("{}".format(save_to_file)):
+            raise FileExistsError(
+                "The file {} already exists.".format(save_to_file)
             )
+
+        macros = self.latex_macros(
+            [self.samples_dict], parameter_dict, rounding=rounding
         )
+        if save_to_file is None:
+            print(macros)
+        else:
+            with open(save_to_file, "w") as f:
+                f.writelines([macros])
 
-    def violin_plot(self, label):
-        """Generate violin plot to compare certain parameters in all result
-        files
+    def to_dat(self, **kwargs):
+        """Save the PESummary results file object to a dat file
 
         Parameters
         ----------
-        label: str
-            the label for the results file that you wish to plot
+        kwargs: dict
+            all kwargs passed to the pesummary.core.file.formats.dat.write_dat
+            function
+        """
+        return self.write(file_format="dat", **kwargs)
+
+    def reweight_samples(self, function, **kwargs):
+        """Reweight the posterior and/or prior samples according to a new prior
         """
-        error_message = (
-            "Failed to generate a violin plot for %s because {}"
+        if self.mcmc_samples:
+            return ValueError("Cannot currently reweight MCMC chains")
+        _samples = self.samples_dict
+        new_samples = _samples.reweight(function, **kwargs)
+        self.parameters = Parameters(new_samples.parameters)
+        self.samples = np.array(new_samples.samples).T
+        self.extra_kwargs["sampler"].update(
+            {
+                "nsamples": new_samples.number_of_samples,
+                "nsamples_before_reweighting": _samples.number_of_samples
+            }
         )
-        violin_plots = ["mass_ratio", "chi_eff", "chi_p", "luminosity_distance"]
+        self.extra_kwargs["meta_data"]["reweighting"] = function
+        if not hasattr(self, "priors"):
+            return
+        if (self.priors is None) or ("samples" not in self.priors.keys()):
+            return
+        prior_samples = self.priors["samples"]
+        if not len(prior_samples):
+            return
+        new_prior_samples = prior_samples.reweight(function, **kwargs)
+        self.priors["samples"] = new_prior_samples
 
-        for plot in violin_plots:
-            if not all(plot in self.samples[j].keys() for j in self.labels):
-                logger.warning(
-                    "Failed to generate violin plots for {} because {} is not "
-                    "common in all result files".format(plot, plot)
-                )
-            samples = [self.samples[i][plot] for i in self.labels]
-            arguments = [
-                self.savedir, plot, samples, self.labels, latex_labels[plot]
-            ]
-            self._try_to_make_a_plot(
-                arguments, self._violin_plot, error_message % (plot)
-            )
 
-    @staticmethod
-    def _violin_plot(
-        savedir, plot_parameter, samples, labels, latex_label,
-        kde=Bounded_1d_kde, default_bounds=True
-    ):
-        """Generate a violin plot for a given set of samples
+class MultiAnalysisRead(Read):
+    """Base class to read in a results file which contains multiple analyses
 
-        Parameters
-        ----------
-        savedir: str
-            the directory you wish to save the plot in
-        plot_parameter: str
-            name of the parameter you wish to generate a violin plot for
-        samples: list
-            list of samples for each parameter
-        labels: list
-            list of labels used to distinguish each result file
-        latex_label: str
-             latex_label correspondig to parameter
-        """
-        xlow, xhigh = None, None
-        if default_bounds:
-            xlow, xhigh = gw._return_bounds(
-                plot_parameter, samples, comparison=True
-            )
-        fig = publication.violin_plots(
-            plot_parameter, samples, labels, latex_labels, kde=kde,
-            kde_kwargs={"xlow": xlow, "xhigh": xhigh}
-        )
-        _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "publication", "violin_plot_{}".format(
-                    plot_parameter
-                )
+    Parameters
+    ----------
+    path_to_results_file: str
+        path to the results file you wish to load
+    remove_nan_likelihood_samples: Bool, optional
+        if True, remove samples which have log_likelihood='nan'. Default True
+
+    Attributes
+    ----------
+    parameters: 2d list
+        list of parameters for each analysis
+    samples: 3d list
+        list of samples stored in the result file for each analysis
+    samples_dict: dict
+        dictionary of samples stored in the result file keyed by analysis label
+    input_version: str
+        version of the result files passed
+    extra_kwargs: dict
+        dictionary of kwargs that were extracted from the result file
+
+    Methods
+    -------
+    samples_dict_for_label: dict
+        dictionary of samples for a specific analysis
+    reduced_samples_dict: dict
+        dictionary of samples for one or more analyses
+    downsample:
+        downsample the posterior samples stored in the result file
+    to_dat:
+        save the posterior samples to a .dat file
+    to_latex_table:
+        convert the posterior samples to a latex table
+    generate_latex_macros:
+        generate a set of latex macros for the stored posterior samples
+    reweight_samples:
+        reweight the posterior and/or samples according to a new prior
+    """
+    def __init__(self, *args, **kwargs):
+        super(MultiAnalysisRead, self).__init__(*args, **kwargs)
+
+    @staticmethod
+    def check_for_nan_likelihoods(parameters, samples, remove=False):
+        import copy
+        _parameters = copy.deepcopy(parameters)
+        _samples = copy.deepcopy(samples)
+        for num, (params, samps) in enumerate(zip(_parameters, _samples)):
+            _parameters[num], _samples[num] = Read.check_for_nan_likelihoods(
+                params, samps, remove=remove
             )
-        )
+        return _parameters, _samples
 
-    def spin_dist_plot(self, label):
-        """Generate a spin disk plot to compare spins in all result
-        files
+    def samples_dict_for_label(self, label):
+        """Return the posterior samples for a specific label
 
         Parameters
         ----------
-        label: str
-            the label for the results file that you wish to plot
-        """
-        error_message = (
-            "Failed to generate a spin disk plot for %s because {}"
-        )
-        parameters = ["a_1", "a_2", "cos_tilt_1", "cos_tilt_2"]
-        for num, label in enumerate(self.labels):
-            if not all(i in self.samples[label].keys() for i in parameters):
-                logger.warning(
-                    "Failed to generate spin disk plots because {} are not "
-                    "common in all result files".format(
-                        " and ".join(parameters)
-                    )
-                )
-                continue
-            samples = [self.samples[label][i] for i in parameters]
-            arguments = [
-                self.savedir, parameters, samples, label, self.colors[num]
-            ]
+        labels: str
+            label you wish to get posterior samples for
 
-            self._try_to_make_a_plot(
-                arguments, self._spin_dist_plot, error_message % (label)
-            )
+        Returns
+        -------
+        outdict: SamplesDict
+            Returns a SamplesDict containing the requested posterior samples
+        """
+        if label not in self.labels:
+            raise ValueError("Unrecognised label: '{}'".format(label))
+        idx = self.labels.index(label)
+        return SamplesDict(self.parameters[idx], np.array(self.samples[idx]).T)
 
-    @staticmethod
-    def _spin_dist_plot(savedir, parameters, samples, label, color):
-        """Generate a spin disk plot for a given set of samples
+    def reduced_samples_dict(self, labels):
+        """Return the posterior samples for one or more labels
 
         Parameters
         ----------
+        labels: str, list
+            label(s) you wish to get posterior samples for
+
+        Returns
+        -------
+        outdict: MultiAnalysisSamplesDict
+            Returns a MultiAnalysisSamplesDict containing the requested
+            posterior samples
         """
-        fig = publication.spin_distribution_plots(
-            parameters, samples, label, color=color
-        )
-        _PlotGeneration.save(
-            fig, os.path.join(
-                savedir, "publication", "spin_disk_plot_{}".format(
-                    label
-                )
+        if not isinstance(labels, list):
+            labels = [labels]
+        not_allowed = [_label for _label in labels if _label not in self.labels]
+        if len(not_allowed):
+            raise ValueError(
+                "Unrecognised label(s) '{}'. The list of available labels are "
+                "{}.".format(", ".join(not_allowed), ", ".join(self.labels))
             )
+        return MultiAnalysisSamplesDict(
+            {
+                label: self.samples_dict_for_label(label) for label in labels
+            }
         )
 
-    def pepredicates_plot(self, label):
-        """Generate plots with the PEPredicates package
-
-        Parameters
-        ----------
-        label: str
-            the label for the results file that you wish to plot
-        """
+    @property
+    def samples_dict(self):
         if self.mcmc_samples:
-            samples = self.samples[label].combine
+            outdict = MCMCSamplesDict(
+                self.parameters[0], [np.array(i).T for i in self.samples[0]]
+            )
         else:
-            samples = self.samples[label]
-        self._pepredicates_plot(
-            self.savedir, samples, label,
-            self.pepredicates_probs[label]["default"], population_prior=False
-        )
-        self._pepredicates_plot(
-            self.savedir, samples, label,
-            self.pepredicates_probs[label]["population"], population_prior=True
-        )
+            outdict = self.reduced_samples_dict(self.labels)
+        return outdict
 
-    @staticmethod
-    @no_latex_plot
-    def _pepredicates_plot(
-        savedir, samples, label, probabilities, population_prior=False
-    ):
-        """Generate a plot with the PEPredicates package for a given set of
-        samples
+    @property
+    def _default_version(self):
+        return ["No version information found"] * len(self.parameters)
 
-        Parameters
-        ----------
-        savedir: str
-            the directory you wish to save the plot in
-        samples: dict
-            dictionary of samples for each parameter
-        label: str
-            the label corresponding to the result file
-        probabilities: dict
-            dictionary of classification probabilities
-        population_prior: Bool, optional
-            if True, the samples will be reweighted according to a population
-            prior
-        """
-        from pesummary.gw.pepredicates import PEPredicates
-
-        parameters = list(samples.keys())
-        samples = [
-            [samples[parameter][j] for parameter in parameters] for j in
-            range(len(samples[parameters[0]]))
-        ]
-        fig = PEPredicates.plot(
-            samples, parameters, population_prior=population_prior
-        )
-        if not population_prior:
-            _PlotGeneration.save(
-                fig, os.path.join(
-                    savedir, "{}_default_pepredicates".format(
-                        label
-                    )
-                )
-            )
-        else:
-            _PlotGeneration.save(
-                fig, os.path.join(
-                    savedir, "{}_population_pepredicates".format(
-                        label
-                    )
-                )
-            )
-        fig = gw._classification_plot(probabilities)
-        if not population_prior:
-            _PlotGeneration.save(
-                fig, os.path.join(
-                    savedir, "{}_default_pepredicates_bar".format(
-                        label
-                    )
-                )
-            )
-        else:
-            _PlotGeneration.save(
-                fig, os.path.join(
-                    savedir, "{}_population_pepredicates_bar".format(
-                        label
-                    )
-                )
-            )
+    @property
+    def _default_kwargs(self):
+        _kwargs = [{"sampler": {}, "meta_data": {}}] * len(self.parameters)
+        for num, ss in enumerate(self.data["samples"]):
+            _kwargs[num]["sampler"]["nsamples"] = len(ss)
+        return _kwargs
+
+    @property
+    def _default_description(self):
+        return {label: "No description found" for label in self.labels}
 
-    def psd_plot(self, label):
-        """Generate a psd plot for a given result file
+    def write(self, package="core", file_format="dat", **kwargs):
+        """Save the data to file
 
         Parameters
         ----------
-        label: str
-            the label corresponding to the result file
+        package: str, optional
+            package you wish to use when writing the data
+        kwargs: dict, optional
+            all additional kwargs are passed to the pesummary.io.write function
         """
-        error_message = (
-            "Failed to generate a PSD plot for %s because {}"
+        return super(MultiAnalysisRead, self).write(
+            package=package, file_format=file_format,
+            extra_kwargs=self.kwargs_dict, file_versions=self.version_dict,
+            **kwargs
         )
 
-        fmin = None
-
-        for num, label in enumerate(self.labels):
-            if list(self.psd[label].keys()) == [None]:
-                return
-            if list(self.psd[label].keys()) == []:
-                return
-            if "f_low" in list(self.file_kwargs[label]["sampler"].keys()):
-                fmin = self.file_kwargs[label]["sampler"]["f_low"]
-            labels = list(self.psd[label].keys())
-            frequencies = [np.array(self.psd[label][i]).T[0] for i in labels]
-            strains = [np.array(self.psd[label][i]).T[1] for i in labels]
-            arguments = [
-                self.savedir, frequencies, strains, fmin, labels, label
-            ]
+    @property
+    def kwargs_dict(self):
+        return {
+            label: kwarg for label, kwarg in zip(self.labels, self.extra_kwargs)
+        }
 
-            self._try_to_make_a_plot(
-                arguments, self._psd_plot, error_message % (label)
-            )
+    @property
+    def version_dict(self):
+        return {
+            label: version for label, version in zip(self.labels, self.input_version)
+        }
 
-    @staticmethod
-    def _psd_plot(savedir, frequencies, strains, fmin, psd_labels, label):
-        """Generate a psd plot for a given set of samples
+    def summary(self, *args, parameters_to_show=4, **kwargs):
+        """Return a summary of the contents of the file
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        frequencies: list
-            list of psd frequencies for each IFO
-        strains: list
-            list of psd strains for each IFO
-        fmin: float
-            frequency to start the psd plotting
-        psd_labels: list
-            list of IFOs used
-        label: str
-            the label used to distinguish the result file
+        parameters_to_show: int, optional
+            number of parameters to show. Default 4
         """
-        fig = gw._psd_plot(
-            frequencies, strains, labels=psd_labels, fmin=fmin
-        )
-        _PlotGeneration.save(
-            fig, os.path.join(savedir, "{}_psd_plot".format(label))
+        string = super(MultiAnalysisRead, self).summary(
+            show_parameters=False, show_nsamples=False
         )
+        string += "analyses: {}\n\n".format(", ".join(self.labels))
+        for num, label in enumerate(self.labels):
+            string += "{}\n".format(label)
+            string += "-" * len(label) + "\n"
+            string += "description: {}\n".format(self.description[label])
+            string += "nsamples: {}\n".format(len(self.samples[num]))
+            string += "parameters: {}\n\n".format(
+                self._parameter_summary(
+                    self.parameters[num], parameters_to_show=parameters_to_show
+                )
+            )
+        return string[:-2]
 
-    def calibration_plot(self, label):
-        """Generate a calibration plot for a given result file
+    def downsample(self, number):
+        """Downsample the posterior samples stored in the result file
+        """
+        for num, ss in enumerate(self.samples):
+            self.samples[num], self.extra_kwargs[num] = _downsample(
+                ss, number, extra_kwargs=self.extra_kwargs[num]
+            )
+
+    def to_latex_table(self, labels="all", parameter_dict=None, save_to_file=None):
+        """Make a latex table displaying the data in the result file.
 
         Parameters
         ----------
-        label: str
-            the label corresponding to the result file
-        """
-        import numpy as np
-
-        error_message = (
-            "Failed to generate calibration plot for %s because {}"
-        )
-        frequencies = np.arange(20., 1024., 1. / 4)
-
-        for num, label in enumerate(self.labels):
-            if list(self.calibration[label].keys()) == [None]:
-                return
-            if list(self.calibration[label].keys()) == []:
-                return
-
-            ifos = list(self.calibration[label].keys())
-            calibration_data = [
-                self.calibration[label][i] for i in ifos
-            ]
-            if "calibration" in self.priors.keys():
-                prior = [self.priors["calibration"][label][i] for i in ifos]
-            else:
-                prior = None
-            arguments = [
-                self.savedir, frequencies, calibration_data, ifos, prior,
-                label
-            ]
-            self._try_to_make_a_plot(
-                arguments, self._calibration_plot, error_message % (label)
+        labels: list, optional
+            list of labels that you want to include in the table
+        parameter_dict: dict, optional
+            dictionary of parameters that you wish to include in the latex
+            table. The keys are the name of the parameters and the items are
+            the descriptive text. If None, all parameters are included
+        save_to_file: str, optional
+            name of the file you wish to save the latex table to. If None, print
+            to stdout
+        """
+        import os
+
+        if save_to_file is not None and os.path.isfile("{}".format(save_to_file)):
+            raise FileExistsError(
+                "The file {} already exists.".format(save_to_file)
+            )
+        if labels != "all" and isinstance(labels, str) and labels not in self.labels:
+            raise ValueError("The label %s does not exist." % (labels))
+        elif labels == "all":
+            labels = list(self.labels)
+        elif isinstance(labels, str):
+            labels = [labels]
+        elif isinstance(labels, list):
+            for ll in labels:
+                if ll not in list(self.labels):
+                    raise ValueError("The label %s does not exist." % (ll))
+
+        table = self.latex_table(
+            [self.samples_dict[label] for label in labels], parameter_dict,
+            labels=labels
+        )
+        if save_to_file is None:
+            print(table)
+        elif os.path.isfile("{}".format(save_to_file)):
+            logger.warning(
+                "File {} already exists. Printing to stdout".format(save_to_file)
             )
+            print(table)
+        else:
+            with open(save_to_file, "w") as f:
+                f.writelines([table])
 
-    @staticmethod
-    def _calibration_plot(
-        savedir, frequencies, calibration_data, calibration_labels, prior, label
+    def generate_latex_macros(
+        self, labels="all", parameter_dict=None, save_to_file=None,
+        rounding=2
     ):
-        """Generate a calibration plot for a given set of samples
+        """Generate a list of latex macros for each parameter in the result
+        file
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        frequencies: list
-            list of frequencies used to interpolate the calibration data
-        calibration_data: list
-            list of calibration data for each IFO
-        calibration_labels: list
-            list of IFOs used
-        prior: list
-            list containing the priors used for each IFO
-        label: str
-            the label used to distinguish the result file
-        """
-        fig = gw._calibration_envelope_plot(
-            frequencies, calibration_data, calibration_labels, prior=prior
-        )
-        _PlotGeneration.save(
-            fig, os.path.join(savedir, "{}_calibration_plot".format(label))
+        labels: list, optional
+            list of labels that you want to include in the table
+        parameter_dict: dict, optional
+            dictionary of parameters that you wish to generate macros for. The
+            keys are the name of the parameters and the items are the latex
+            macros name you wish to use. If None, all parameters are included.
+        save_to_file: str, optional
+            name of the file you wish to save the latex table to. If None, print
+            to stdout
+        rounding: int, optional
+            number of decimal points to round the latex macros
+        """
+        import os
+
+        if save_to_file is not None and os.path.isfile("{}".format(save_to_file)):
+            raise FileExistsError(
+                "The file {} already exists.".format(save_to_file)
+            )
+        if labels != "all" and isinstance(labels, str) and labels not in self.labels:
+            raise ValueError("The label %s does not exist." % (labels))
+        elif labels == "all":
+            labels = list(self.labels)
+        elif isinstance(labels, str):
+            labels = [labels]
+        elif isinstance(labels, list):
+            for ll in labels:
+                if ll not in list(self.labels):
+                    raise ValueError("The label %s does not exist." % (ll))
+
+        macros = self.latex_macros(
+            [self.samples_dict[i] for i in labels], parameter_dict,
+            labels=labels, rounding=rounding
         )
+        if save_to_file is None:
+            print(macros)
+        else:
+            with open(save_to_file, "w") as f:
+                f.writelines([macros])
 
-    @staticmethod
-    def _interactive_corner_plot(savedir, label, samples, latex_labels):
-        """Generate an interactive corner plot for a given set of samples
+    def reweight_samples(self, function, labels=None, **kwargs):
+        """Reweight the posterior and/or prior samples according to a new prior
 
         Parameters
         ----------
-        savedir: str
-            the directory you wish to save the plot in
-        label: str
-            the label corresponding to the results file
-        samples: dict
-            dictionary containing PESummary.utils.utils.Array objects that
-            contain samples for each parameter
-        latex_labels: str
-            latex labels for each parameter in samples
-        """
-        source_parameters = [
-            "luminosity_distance", "mass_1_source", "mass_2_source",
-            "total_mass_source", "chirp_mass_source", "redshift"
-        ]
-        parameters = [i for i in samples.keys() if i in source_parameters]
-        data = [samples[parameter] for parameter in parameters]
-        labels = [latex_labels[parameter] for parameter in parameters]
-        _ = interactive.corner(
-            data, labels, write_to_html_file=os.path.join(
-                savedir, "corner", "{}_interactive_source.html".format(label)
-            ), dimensions={"width": 900, "height": 900}
-        )
-        extrinsic_parameters = ["luminosity_distance", "psi", "ra", "dec"]
-        parameters = [i for i in samples.keys() if i in extrinsic_parameters]
-        data = [samples[parameter] for parameter in parameters]
-        labels = [latex_labels[parameter] for parameter in parameters]
-        _ = interactive.corner(
-            data, labels, write_to_html_file=os.path.join(
-                savedir, "corner", "{}_interactive_extrinsic.html".format(label)
+        labels: list, optional
+            list of analyses you wish to reweight. Default reweight all
+            analyses
+        """
+        _samples_dict = self.samples_dict
+        for idx, label in enumerate(self.labels):
+            if labels is not None and label not in labels:
+                continue
+            new_samples = _samples_dict[label].reweight(function, **kwargs)
+            self.parameters[idx] = Parameters(new_samples.parameters)
+            self.samples[idx] = np.array(new_samples.samples).T
+            self.extra_kwargs[idx]["sampler"].update(
+                {
+                    "nsamples": new_samples.number_of_samples,
+                    "nsamples_before_reweighting": (
+                        _samples_dict[label].number_of_samples
+                    )
+                }
             )
-        )
+            self.extra_kwargs[idx]["meta_data"]["reweighting"] = function
+            if not hasattr(self, "priors"):
+                continue
+            if "samples" not in self.priors.keys():
+                continue
+            prior_samples = self.priors["samples"][label]
+            if not len(prior_samples):
+                continue
+            new_prior_samples = prior_samples.reweight(function, **kwargs)
+            self.priors["samples"][label] = new_prior_samples
```

### Comparing `pesummary-0.9.1/pesummary/gw/plots/bounds.py` & `pesummary-1.0.0/pesummary/gw/plots/bounds.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,25 +1,12 @@
-# Copyright (C) 2018  Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import numpy as np
 
-
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
 default_bounds = {"luminosity_distance": {"low": 0.},
                   "geocent_time": {"low": 0.},
                   "dec": {"low": -np.pi / 2, "high": np.pi / 2},
                   "ra": {"low": 0., "high": 2 * np.pi},
                   "a_1": {"low": 0., "high": 1.},
                   "a_2": {"low": 0., "high": 1.},
                   "phi_jl": {"low": 0., "high": 2 * np.pi},
@@ -33,19 +20,19 @@
                   "mass_1": {"low": 0},
                   "total_mass": {"low": 0.},
                   "chirp_mass": {"low": 0.},
                   "H1_time": {"low": 0.},
                   "L1_time": {"low": 0.},
                   "V1_time": {"low": 0.},
                   "E1_time": {"low": 0.},
-                  "spin_1x": {"low": 0., "high": 1.},
-                  "spin_1y": {"low": 0., "high": 1.},
+                  "spin_1x": {"low": -1., "high": 1.},
+                  "spin_1y": {"low": -1., "high": 1.},
                   "spin_1z": {"low": -1., "high": 1.},
-                  "spin_2x": {"low": 0., "high": 1.},
-                  "spin_2y": {"low": 0., "high": 1.},
+                  "spin_2x": {"low": -1., "high": 1.},
+                  "spin_2y": {"low": -1., "high": 1.},
                   "spin_2z": {"low": -1., "high": 1.},
                   "chi_p": {"low": 0., "high": 1.},
                   "chi_eff": {"low": -1., "high": 1.},
                   "mass_ratio": {"low": 0., "high": 1.},
                   "symmetric_mass_ratio": {"low": 0., "high": 0.25},
                   "inverted_mass_ratio": {"low": 1.},
                   "phi_1": {"low": 0., "high": 2 * np.pi},
```

### Comparing `pesummary-0.9.1/pesummary/gw/webpage/public.py` & `pesummary-1.0.0/pesummary/gw/webpage/public.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,26 +1,15 @@
-# Copyright (C) 2019 Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
 from .main import _WebpageGeneration as _GWWebpageGeneration
 from pesummary.core.webpage import webpage
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class _PublicWebpageGeneration(_GWWebpageGeneration):
     """
     """
     def __init__(
         self, webdir=None, samples=None, labels=None, publication=None,
         user=None, config=None, same_parameters=None, base_url=None,
@@ -29,15 +18,17 @@
         file_kwargs=None, existing_labels=None, existing_config=None,
         existing_file_version=None, existing_injection_data=None,
         existing_samples=None, existing_metafile=None, add_to_existing=False,
         existing_file_kwargs=None, existing_weights=None, result_files=None,
         notes=None, disable_comparison=False, pastro_probs=None, gwdata=None,
         disable_interactive=False, publication_kwargs={}, no_ligo_skymap=False,
         psd=None, priors=None, package_information={"packages": []},
-        mcmc_samples=False, external_hdf5_links=False
+        mcmc_samples=False, external_hdf5_links=False,
+        preliminary_pages=False, existing_plot=None, disable_expert=False,
+        analytic_priors=None
     ):
         super(_PublicWebpageGeneration, self).__init__(
             webdir=webdir, samples=samples, labels=labels,
             publication=publication, user=user, config=config,
             same_parameters=same_parameters, base_url=base_url,
             file_versions=file_versions, hdf5=hdf5, colors=colors,
             custom_plotting=custom_plotting,
@@ -53,15 +44,17 @@
             existing_weights=existing_weights, result_files=result_files,
             notes=notes, disable_comparison=disable_comparison,
             pastro_probs=pastro_probs, gwdata=gwdata,
             disable_interactive=disable_interactive,
             publication_kwargs=publication_kwargs,
             no_ligo_skymap=no_ligo_skymap, psd=psd, priors=priors,
             package_information=package_information,
-            mcmc_samples=mcmc_samples, external_hdf5_links=external_hdf5_links
+            mcmc_samples=mcmc_samples, external_hdf5_links=external_hdf5_links,
+            preliminary_pages=preliminary_pages, existing_plot=existing_plot,
+            disable_expert=disable_expert, analytic_priors=analytic_priors
         )
 
     def setup_page(
         self, html_page, links, label=None, title=None, approximant=None,
         background_colour=None, histogram_download=False
     ):
         """Set up each webpage with a header and navigation bar.
```

### Comparing `pesummary-0.9.1/pesummary/gw/webpage/main.py` & `pesummary-1.0.0/pesummary/gw/webpage/main.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,35 +1,20 @@
-# Copyright (C) 2019 Charlie Hoy <charlie.hoy@ligo.org>
-# This program is free software; you can redistribute it and/or modify it
-# under the terms of the GNU General Public License as published by the
-# Free Software Foundation; either version 3 of the License, or (at your
-# option) any later version.
-#
-# This program is distributed in the hope that it will be useful, but
-# WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General
-# Public License for more details.
-#
-# You should have received a copy of the GNU General Public License along
-# with this program; if not, write to the Free Software Foundation, Inc.,
-# 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+# Licensed under an MIT style license -- see LICENSE.md
 
 import os
-import uuid
-import math
 import numpy as np
 
-import pesummary
-from pesummary.core.webpage import webpage
 from pesummary.core.webpage.main import _WebpageGeneration as _CoreWebpageGeneration
 from pesummary.core.webpage.main import PlotCaption
 from pesummary.gw.file.standard_names import descriptive_names
-from pesummary.utils.utils import logger, jensen_shannon_divergence, safe_round
+from pesummary.utils.utils import logger
 from pesummary import conf
 
+__author__ = ["Charlie Hoy <charlie.hoy@ligo.org>"]
+
 
 class CommandLineCaption(object):
     """Class to handle generating the command line used to generate a plot and
     a caption to describe the plot
 
     Parameters
     ----------
@@ -116,21 +101,21 @@
         file_kwargs=None, existing_labels=None, existing_config=None,
         existing_file_version=None, existing_injection_data=None,
         existing_samples=None, existing_metafile=None, add_to_existing=False,
         existing_file_kwargs=None, existing_weights=None, result_files=None,
         notes=None, disable_comparison=False, pastro_probs=None, gwdata=None,
         disable_interactive=False, publication_kwargs={}, no_ligo_skymap=False,
         psd=None, priors=None, package_information={"packages": []},
-        mcmc_samples=False, external_hdf5_links=False
+        mcmc_samples=False, external_hdf5_links=False, preliminary_pages=False,
+        existing_plot=None, disable_expert=False, analytic_priors=None
     ):
         self.pepredicates_probs = pepredicates_probs
         self.pastro_probs = pastro_probs
         self.gracedb = gracedb
         self.approximant = approximant
-        self.key_data = key_data
         self.file_kwargs = file_kwargs
         self.publication = publication
         self.publication_kwargs = publication_kwargs
         self.result_files = result_files
         self.gwdata = gwdata
         self.no_ligo_skymap = no_ligo_skymap
         self.psd = psd
@@ -149,47 +134,80 @@
             existing_metafile=existing_metafile,
             existing_file_kwargs=existing_file_kwargs,
             existing_weights=existing_weights,
             add_to_existing=add_to_existing, notes=notes,
             disable_comparison=disable_comparison,
             disable_interactive=disable_interactive,
             package_information=package_information, mcmc_samples=mcmc_samples,
-            external_hdf5_links=external_hdf5_links
-        )
+            external_hdf5_links=external_hdf5_links, key_data=key_data,
+            existing_plot=existing_plot, disable_expert=disable_expert,
+            analytic_priors=analytic_priors
+        )
+        if self.file_kwargs is None:
+            self.file_kwargs = {
+                label: {"sampler": {}, "meta_data": {}} for label in self.labels
+            }
+        if self.approximant is None:
+            self.approximant = {label: None for label in self.labels}
+        if self.result_files is None:
+            self.result_files = [None] * len(self.labels)
         self.psd_path = {"other": os.path.join("..", "psds")}
         self.calibration_path = {"other": os.path.join("..", "calibration")}
+        self.preliminary_pages = preliminary_pages
+        if not isinstance(self.preliminary_pages, dict):
+            if self.preliminary_pages:
+                self.preliminary_pages = {
+                    label: True for label in self.labels
+                }
+            else:
+                self.preliminary_pages = {
+                    label: False for label in self.labels
+                }
+        if all(value for value in self.preliminary_pages.values()):
+            self.all_pages_preliminary = True
+        if len(self.labels) > 1:
+            if any(value for value in self.preliminary_pages.values()):
+                self.preliminary_pages["Comparison"] = True
 
-    def _jensen_shannon_divergence(self, param, samples):
-        """Return the Jensen Shannon divergence between two sets of samples
+    def categorize_parameters(self, parameters):
+        """Categorize the parameters into common headings
+
+        Parameters
+        ----------
+        parameters: list
+            list of parameters that you would like to sort
+        """
+        return super(_WebpageGeneration, self).categorize_parameters(
+            parameters, starting_letter=False
+        )
+
+    def _kde_from_same_samples(self, param, samples):
+        """Generate KDEs for a set of samples
 
         Parameters
         ----------
         param: str
             The parameter that the samples belong to
         samples: list
-            2d list containing the samples you wish to calculate the Jensen
-            Shannon divergence between
+            list of samples for each result file
         """
-        from pesummary.core.plots.bounded_1d_kde import Bounded_1d_kde
+        from pesummary.core.plots.bounded_1d_kde import ReflectionBoundedKDE
         from pesummary.gw.plots.plot import _return_bounds
 
         xlow, xhigh = _return_bounds(param, samples, comparison=True)
-        return jensen_shannon_divergence(
-            [samples[0], samples[1]], kde=Bounded_1d_kde, xlow=xlow,
-            xhigh=xhigh
+        return super(_WebpageGeneration, self)._kde_from_same_samples(
+            param, samples, kde=ReflectionBoundedKDE, xlow=xlow, xhigh=xhigh
         )
 
     def make_navbar_for_homepage(self):
         """Make a navbar for the homepage
         """
         links = super(_WebpageGeneration, self).make_navbar_for_homepage()
         if self.gwdata is not None:
             links.append(["Detchar", [i for i in self.gwdata.keys()]])
-        if self.notes is not None:
-            links.append("Notes")
         return links
 
     def make_navbar_for_result_page(self):
         """Make a navbar for the result page homepage
         """
         links = super(_WebpageGeneration, self).make_navbar_for_result_page()
         for num, label in enumerate(self.labels):
@@ -229,21 +247,16 @@
         if self.make_comparison:
             if not all(self.approximant[i] is not None for i in self.labels):
                 image_contents = []
                 _plot = os.path.join(path, "compare_time_domain_waveforms.png")
                 if os.path.isfile(_plot):
                     image_contents.append(_plot)
                     image_contents = [image_contents]
-                    unique_id = '{}'.format(uuid.uuid4().hex.upper()[:6])
-                    html_file.make_table_of_images(
-                        contents=image_contents, unique_id=unique_id
-                    )
-                    images = [y for x in image_contents for y in x]
-                    html_file.make_modal_carousel(
-                        images=images, unique_id=unique_id
+                    html_file = self.make_modal_carousel(
+                        html_file, image_contents, unique_id=True
                     )
 
         for i in self.labels:
             html_file.make_banner(approximant=i, key=i)
             image_contents, captions = [], []
             basic_string = os.path.join(self.webdir, "plots", "{}.png")
             relative_path = os.path.join(path, "{}.png")
@@ -264,21 +277,18 @@
                 basic_string.format("%s_calibration_plot" % (i))
             ):
                 image_contents.append(
                     relative_path.format("%s_calibration_plot" % (i))
                 )
                 captions.append(PlotCaption("calibration"))
             image_contents = [image_contents]
-            unique_id = '{}'.format(uuid.uuid4().hex.upper()[:6])
-            html_file.make_table_of_images(
-                contents=image_contents, unique_id=unique_id,
-                captions=[captions], extra_div=True
+            html_file = self.make_modal_carousel(
+                html_file, image_contents, unique_id=True, extra_div=True,
+                captions=[captions]
             )
-            images = [y for x in image_contents for y in x]
-            html_file.make_modal_carousel(images=images, unique_id=unique_id)
 
         for _key in ["sampler", "meta_data"]:
             if _key == "sampler":
                 html_file.make_banner(
                     approximant="Sampler kwargs", key="sampler_kwargs",
                     _style="font-size: 26px;"
                 )
@@ -314,97 +324,15 @@
             )
             html_file.end_div(4)
             html_file.end_container()
             html_file.export("{}.csv".format(_key))
 
         html_file.make_footer(user=self.user, rundir=self.webdir)
         html_file.close()
-
-        for num, i in enumerate(self.labels):
-            html_file = self.setup_page(
-                i, self.navbar["result_page"][i], i, approximant=i,
-                title="{} Summary page".format(i),
-                background_colour=self.colors[num]
-            )
-            html_file.make_banner(approximant=i, key=i)
-            images, cli, captions = self.default_images_for_result_page(i)
-            unique_id = '{}'.format(uuid.uuid4().hex.upper()[:6])
-            html_file.make_table_of_images(
-                contents=images, cli=cli, unique_id=unique_id,
-                captions=captions, autoscale=True
-            )
-            images = [y for x in images for y in x]
-            html_file.make_modal_carousel(images=images, unique_id=unique_id)
-
-            if self.custom_plotting:
-                from glob import glob
-
-                custom_plots = glob(
-                    "{}/plots/{}_custom_plotting_*".format(self.webdir, i)
-                )
-                path = self.image_path["other"]
-                for num, i in enumerate(custom_plots):
-                    custom_plots[num] = path + i.split("/")[-1]
-                image_contents = [
-                    custom_plots[i:4 + i] for i in range(
-                        0, len(custom_plots), 4
-                    )
-                ]
-                unique_id = '{}'.format(uuid.uuid4().hex.upper()[:6])
-                html_file.make_table_of_images(
-                    contents=image_contents, unique_id=unique_id
-                )
-                images = [y for x in image_contents for y in x]
-                html_file.make_modal_carousel(images=images, unique_id=unique_id)
-
-            html_file.make_banner(
-                approximant="Summary Table", key="summary_table",
-                _style="font-size: 26px;"
-            )
-            _style = "margin-top:3em; margin-bottom:5em; max-width:1400px"
-            _class = "row justify-content-center"
-            html_file.make_container(style=_style)
-            html_file.make_div(4, _class=_class, _style=None)
-
-            key_data = self.key_data
-            contents = []
-            headings = [
-                "posterior", "maxL", "maxP", "mean", "median",
-                "5th percentile", "95th percentile"
-            ]
-            _injection = not all(
-                math.isnan(_data["injected"]) for _data in self.key_data[i].values()
-            )
-            if _injection:
-                headings.append("injected")
-            for j in self.samples[i].keys():
-                row = []
-                row.append(j)
-                row.append(safe_round(self.key_data[i][j]["maxL"], 3))
-                row.append(safe_round(self.key_data[i][j]["maxP"], 3))
-                row.append(safe_round(self.key_data[i][j]["mean"], 3))
-                row.append(safe_round(self.key_data[i][j]["median"], 3))
-                row.append(safe_round(self.key_data[i][j]["5th percentile"], 3))
-                row.append(safe_round(self.key_data[i][j]["95th percentile"], 3))
-                if _injection:
-                    row.append(safe_round(self.key_data[i][j]["injected"], 3))
-                contents.append(row)
-
-            html_file.make_table(
-                headings=headings, contents=contents, heading_span=1,
-                accordian=False, format="table-hover header-fixed",
-                sticky_header=True
-            )
-            html_file.end_div(4)
-            html_file.end_container()
-            html_file.export(
-                "summary_information_{}.csv".format(i)
-            )
-            html_file.make_footer(user=self.user, rundir=self.webdir)
-            html_file.close()
+        super(_WebpageGeneration, self)._make_home_pages(pages, make_home=False)
 
     def make_publication_pages(self):
         """Wrapper function for _make_publication_pages()
         """
         pages = ["Publication"]
         self.create_blank_html_pages(pages)
         self._make_publication_pages(pages)
@@ -480,19 +408,17 @@
         image_contents = [
             pub_plots[i:3 + i] for i in range(0, len(pub_plots), 3)
         ]
         command_lines = [
             cli[i:3 + i] for i in range(0, len(cli), 3)
         ]
         captions = [cap[i:3 + i] for i in range(0, len(cap), 3)]
-        html_file.make_table_of_images(
-            contents=image_contents, cli=command_lines, captions=captions
+        html_file = self.make_modal_carousel(
+            html_file, image_contents, cli=command_lines, captions=captions
         )
-        images = [y for x in image_contents for y in x]
-        html_file.make_modal_carousel(images=images)
         html_file.make_footer(user=self.user, rundir=self.webdir)
         html_file.close()
 
     def make_detector_pages(self):
         """Wrapper function for _make_publication_pages()
         """
         pages = [i for i in self.gwdata.keys()]
@@ -503,51 +429,77 @@
         """Make the detector characterisation pages
 
         Parameters
         ----------
         pages: list
             list of pages that you wish to create
         """
-        from glob import glob
         from pesummary.utils.utils import (
             determine_gps_time_and_window, command_line_dict
         )
         from astropy.time import Time
 
         executable = self.get_executable("summarydetchar")
-        command_line = command_line_dict()
+        try:
+            command_line = command_line_dict()
+        except SystemExit:
+            command_line = {"gwdata": {}}
         if isinstance(command_line["gwdata"], dict):
             gwdata_command_line = [
                 "{}:{}".format(key, val) for key, val in
                 command_line["gwdata"].items()
             ]
         else:
             gwdata_command_line = command_line["gwdata"]
+            if gwdata_command_line is None:
+                gwdata_command_line = []
         general_cli = "%s --webdir %s --gwdata %s --plot {}{}" % (
             executable, os.path.join(self.webdir, "plots"),
             " ".join(gwdata_command_line)
         )
         path = self.image_path["other"]
         base = os.path.join(path, "{}_{}.png")
-        maxL_samples = {
-            i: {
-                "geocent_time": self.key_data[i]["geocent_time"]["maxL"]
-            } for i in self.labels
-        }
-        gps_time, window = determine_gps_time_and_window(maxL_samples, self.labels)
-        t = Time(gps_time, format='gps')
-        t = Time(t, format='datetime')
-        link = (
-            "https://ldas-jobs.ligo-wa.caltech.edu/~detchar/summary/day"
-            "/{}{}{}/".format(
-                t.value.year,
-                "0{}".format(t.value.month) if t.value.month < 10 else t.value.month,
-                "0{}".format(t.value.day) if t.value.day < 10 else t.value.day
+        ADD_DETCHAR_LINK = True
+        try:
+            maxL_samples = {
+                i: {
+                    "geocent_time": self.key_data[i]["geocent_time"]["maxL"]
+                } for i in self.labels
+            }
+        except KeyError:
+            # trying a different name for time
+            try:
+                maxL_samples = {
+                    i: {
+                        "geocent_time": self.key_data[i][
+                            "marginalized_geocent_time"
+                        ]["maxL"]
+                    } for i in self.labels
+                }
+            except KeyError:
+                logger.warning(
+                    "Failed to find a time parameter to link to detchar/"
+                    "summary pages. Not adding link to webpages."
+                )
+                ADD_DETCHAR_LINK = False
+        if ADD_DETCHAR_LINK:
+            gps_time, window = determine_gps_time_and_window(maxL_samples, self.labels)
+            t = Time(gps_time, format='gps')
+            t = Time(t, format='datetime')
+            link = (
+                "https://ldas-jobs.ligo-wa.caltech.edu/~detchar/summary/day"
+                "/{}{}{}/".format(
+                    t.value.year,
+                    "0{}".format(t.value.month) if t.value.month < 10 else t.value.month,
+                    "0{}".format(t.value.day) if t.value.day < 10 else t.value.day
+                )
             )
-        )
+        else:
+            link = None
+            gps_time, window = None, None
         for det in self.gwdata.keys():
             html_file = self.setup_page(
                 det, self.navbar["home"], title="{} Detchar".format(det)
             )
             html_file.make_banner(approximant=det, key="detchar", link=link)
             image_contents = [
                 [base.format("spectrogram", det), base.format("omegascan", det)]
@@ -558,19 +510,17 @@
                     general_cli.format(
                         "omegascan", "--gps %s --vmin 0 --vmax 25 --window %s" % (
                             gps_time, window
                         )
                     )
                 ]
             ]
-            html_file.make_table_of_images(
-                contents=image_contents, cli=command_lines, autoscale=True
+            html_file = self.make_modal_carousel(
+                html_file, image_contents, cli=command_lines, autoscale=True,
             )
-            images = [y for x in image_contents for y in x]
-            html_file.make_modal_carousel(images=images)
             html_file.make_footer(user=self.user, rundir=self.webdir)
             html_file.close()
 
     def make_classification_pages(self):
         """Wrapper function for _make_publication_pages()
         """
         pages = ["{}_{}_Classification".format(i, i) for i in self.labels]
@@ -667,20 +617,18 @@
                 [
                     PlotCaption("default_classification_mass_1_mass_2"),
                     PlotCaption("default_classification_bar"),
                     PlotCaption("population_classification_mass_1_mass_2"),
                     PlotCaption("population_classification_bar")
                 ]
             ]
-            html_file.make_table_of_images(
-                contents=image_contents, cli=command_lines, autoscale=True,
+            html_file = self.make_modal_carousel(
+                html_file, image_contents, cli=command_lines, autoscale=True,
                 captions=captions
             )
-            images = [y for x in image_contents for y in x]
-            html_file.make_modal_carousel(images=images)
             html_file.make_footer(user=self.user, rundir=self.webdir)
             html_file.close()
 
     def _make_entry_in_downloads_table(self, html_file, label, num, base_string):
         """Make a label specific entry into the downloads table
 
         Parameters
@@ -712,28 +660,29 @@
                                 os.path.join(
                                     self.psd_path["other"],
                                     "%s_%s_psd.dat" % (label, ifo)
                                 )
                             )
                         ]
                     )
-        if "calibration" in self.priors.keys():
+        if self.priors is not None and "calibration" in self.priors.keys():
             if label in self.priors["calibration"].keys():
                 for ifo in self.priors["calibration"][label].keys():
-                    table.append(
-                        [
-                            base_string.format(
-                                "%s calibration envelope file used for this "
-                                "analysis" % (ifo), os.path.join(
-                                    self.calibration_path["other"],
-                                    "%s_%s_cal.txt" % (label, ifo)
+                    if len(self.priors["calibration"][label][ifo]):
+                        table.append(
+                            [
+                                base_string.format(
+                                    "%s calibration envelope file used for "
+                                    "this analysis" % (ifo), os.path.join(
+                                        self.calibration_path["other"],
+                                        "%s_%s_cal.txt" % (label, ifo)
+                                    )
                                 )
-                            )
-                        ]
-                    )
+                            ]
+                        )
         return table
 
     def default_images_for_result_page(self, label):
         """Return the default images that will be displayed on the result page
         """
         path = self.image_path["other"]
         base_string = path + "%s_{}.png" % (label)
@@ -800,62 +749,63 @@
 
     def default_categories(self):
         """Return the default categories
         """
         categories = self.categories = {
             "masses": {
                 "accept": ["mass"],
-                "reject": ["source", "final"]
+                "reject": ["source", "final", "torus"]
             },
             "source": {
-                "accept": ["source"], "reject": ["final"]
+                "accept": ["source"], "reject": ["final", "torus"]
             },
             "remnant": {
-                "accept": ["final"], "reject": []
+                "accept": ["final", "torus"], "reject": []
             },
             "inclination": {
-                "accept": ["theta", "iota"], "reject": []
+                "accept": ["theta", "iota", "viewing"], "reject": []
             },
             "spins": {
-                "accept": ["spin", "chi_p", "chi_eff", "a_1", "a_2"],
-                "reject": ["lambda", "final", "gamma"]
+                "accept": ["spin", "chi_p", "chi_eff", "a_1", "a_2", "precession"],
+                "reject": ["lambda", "final", "gamma", "order"]
             },
             "spin_angles": {
-                "accept": ["phi", "tilt"], "reject": []
+                "accept": ["phi", "tilt", "beta"], "reject": []
             },
             "tidal": {
                 "accept": [
                     "lambda", "gamma_", "log_pressure",
-                    "spectral_decomposition_gamma_"
+                    "spectral_decomposition_gamma_", "compactness_",
+                    "tidal_disruption"
                 ],
                 "reject": []
             },
             "location": {
                 "accept": [
                     "ra", "dec", "psi", "luminosity_distance", "redshift",
                     "comoving_distance"
                 ],
                 "reject": ["mass_ratio", "radiated", "ram", "ran", "rat"]
             },
             "timings": {
                 "accept": ["time"], "reject": []
             },
             "SNR": {
-                "accept": ["snr"], "reject": []
+                "accept": ["snr", "subdominant_multipoles"], "reject": []
             },
             "calibration": {
                 "accept": ["spcal", "recalib", "frequency"],
-                "reject": ["minimum"]
+                "reject": ["minimum", "tidal_disruption", "quasinormal"]
             },
             "energy": {
                 "accept": ["peak_luminosity", "radiated"],
                 "reject": []
             },
             "others": {
-                "accept": ["phase", "likelihood", "prior"],
+                "accept": ["phase", "likelihood", "prior", "quasinormal"],
                 "reject": ["spcal", "recalib"]
             }
         }
         return categories
 
     def default_popular_options(self):
         """Return a list of popular options
@@ -877,7 +827,44 @@
         return contents
 
     def default_corner_params(self):
         """Return a list of default corner parameters used by the corner
         plotting function
         """
         return conf.gw_corner_parameters
+
+    def add_to_expert_pages(self, path, label):
+        """Additional expert plots to add beyond the default. This returns a
+        dictionary keyed by the parameter, with values providing the path
+        to the additional plots you wish to add. The plots are a 2d list
+        where each sublist represents a row in the table of images.
+
+        Parameters
+        ----------
+        path: str
+            path to the image directory
+        label: str
+            label of the plot you wish to add
+        """
+        mydict = super(_WebpageGeneration, self).add_to_expert_pages(
+            path, label
+        )
+        contour_base = path + "{}_2d_contour_{}_log_likelihood.png"
+        mydict.update({
+            "network_precessing_snr": [
+                [
+                    contour_base.format(label, "_b_bar"),
+                    contour_base.format(label, "_precessing_harmonics_overlap"),
+                ]
+            ]
+        })
+        return mydict
+
+    @property
+    def additional_1d_pages(self):
+        """Additional 1d histogram pages beyond one for each parameter. You may,
+        for instance, want a 1d histogram page which combines multiple
+        parameters. This returns a dictionary, keyed by the new 1d histogram
+        page, with values indicating the parameters you wish to include on this
+        page. Only the 1d marginalized histograms are shown.
+        """
+        return conf.additional_1d_pages
```

### Comparing `pesummary-0.9.1/pesummary/conf/caption.py` & `pesummary-1.0.0/pesummary/conf/caption.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,28 +1,45 @@
 # Caption for 1d histogram
 caption_1d_histogram = (
     "Plot showing the marginalized posterior distribution for {}. The vertical "
     "dashed lines show the 90% credible interval. The title gives the median "
     "and 90% confidence interval."
 )
 
+# Caption for bootstrapped 1d histogram
+caption_1d_histogram_bootstrap = (
+    "Plot showing {} posterior realizations for {}. {} samples were randomly "
+    "drawn for each test."
+)
+
+# Caption for 2d contour plot
+caption_2d_contour = (
+    "Plot showing the 2d posterior distribution for {} and {}. The contours "
+    "show the 50% and 90% credible intervals."
+)
+
 # Caption for autocorrelation plot
 caption_autocorrelation = (
     "Plot showing the autocorrelation function for {}. This plot is commonly "
     "used to check for randomness in the data. An autocorrelation of 1/-1 "
     "means that the samples are correlated and 0 means no correlation. The "
     "autocorrelation function at a lag of N gives the correlation between "
     "the 0th sample and the Nth sample."
 )
 
 # Caption for sample evolution plot
 caption_sample_evolution = (
     "Scatter plot showing the evolution of the collected samples for {}."
 )
 
+# Caption for sample evolution plot colored by second variable
+caption_sample_evolution_colored = (
+    caption_sample_evolution[:-1] + " colored by {}."
+)
+
 # Caption for preliminary skymap
 caption_skymap_preliminary = (
     "Plot showing the most likely position of the source. This map has been "
     "produced by creating a 2d histogram from the samples of 'ra' and 'dec'. "
     "This is a valid approximation near the equator but breaks down near the "
     "poles. The 50% and 90% credible intervals are approximate. For true "
     "50% and 90% credible intervals a 'ligo.skymap' skymap should be "
```

### Comparing `pesummary-0.9.1/README.md` & `pesummary-1.0.0/README.md`

 * *Files 14% similar despite different names*

```diff
@@ -1,19 +1,19 @@
 # PESummary
 
 ## Release status
 
 [![PyPI version](https://badge.fury.io/py/pesummary.svg)](http://badge.fury.io/py/pesummary)
 [![Conda version](https://img.shields.io/conda/vn/conda-forge/pesummary.svg)](https://anaconda.org/conda-forge/pesummary/)
 [![PyPI - Downloads](https://img.shields.io/pypi/dm/pesummary)](https://img.shields.io/pypi/dm/pesummary)
-[![Total Downloads](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)](https://anaconda.org/conda-forge/pycbc/badges/downloads.svg)
+[![Total Downloads](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)](https://anaconda.org/conda-forge/pesummary/badges/downloads.svg)
 
 [![License](https://img.shields.io/pypi/l/pesummary.svg)](https://choosealicense.com/licenses/mit/)
 [![Python versions](https://img.shields.io/pypi/pyversions/pesummary.svg)](https://img.shields.io/pypi/pyversions/pesummary.svg)
-[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3937807.svg)](https://doi.org/10.5281/zenodo.3937807)
+[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4771187.svg)](https://doi.org/10.5281/zenodo.4771187)
 
 ## Development status
 
 [![Pipeline Status](https://git.ligo.org/lscsoft/pesummary/badges/master/pipeline.svg)](https://git.ligo.org/lscsoft/pesummary/commits/master)
 [![Coverage report](https://docs.ligo.org/lscsoft/pesummary/coverage_badge.svg)](https://docs.ligo.org/lscsoft/pesummary/htmlcov/index.html)
 
 This package helps the user to generate summary webpages to visualise the output from any sample generating code.
```

### Comparing `pesummary-0.9.1/CONTRIBUTING.md` & `pesummary-1.0.0/CONTRIBUTING.md`

 * *Files 4% similar despite different names*

```diff
@@ -11,15 +11,15 @@
 ```
 
 replacing the SSH url to that of your fork. You can then install `PESummary` by
 running,
 
 ```bash
 $ cd pesummary
-$ python setup.py install
+$ pip install .
 ```
 
 which will install `PESummary`. For further instructions on how to install
 PESummary please visit the [docs](https://docs.ligo.org/lscsoft/pesummary/installation.html).
 
 ## Update your fork
 If you already have a fork of `PESummary`, and are starting work on a new
@@ -123,17 +123,17 @@
 continuous integration (CI) for every branch and for every merge request. New
 code contributions must have 100% test coverage. Modifications to existing code
 must not decrease test coverage. In order to run unit tests, run the following
 commands,
 
 ```bash
 $ cd pesummary
-$ pip install optional_requirements.txt
+$ pip install .[tests]
 $ cd pesummary
-$ covarage run -m pytest tests/
+$ coverage run -m pytest tests/
 $ coverage html
 ```
 
 This will save a coverage report that you can view in a web browser by opening
 the file,
 
 ```bash
@@ -142,14 +142,14 @@
 
 ## Code style
 Code should be written in the [PEP8](https://www.python.org/dev/peps/pep-0008/)
 style. To check code style, run the following commands,
 
 ```bash
 $ cd pesummary
-$ pip install optional_requirements.txt
+$ pip install .[lint]
 $ flake8 .
 ```
 
 ## Documentation
 Documentation strings should be written in the
 [NumpyDoc style](https://numpydoc.readthedocs.io/en/latest/).
```

